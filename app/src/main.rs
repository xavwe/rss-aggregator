// main.rs
use feed_rs::parser;
use reqwest;
use rss::{Channel, ChannelBuilder, Item, ItemBuilder};
use std::error::Error;
use std::fs;
use chrono::{DateTime, FixedOffset, Utc};
use tokio;
use serde::Deserialize;
use regex::Regex;

// Config struct for deserializing config.toml
#[derive(Debug, Deserialize)]
struct Config {
    max_items: Option<usize>,
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn Error + Send + Sync>> {
    // Read configuration from config.toml
    let config: Config = fs::read_to_string("config.toml")
        .ok()
        .and_then(|contents| toml::from_str(&contents).ok())
        .unwrap_or(Config { max_items: None });
    let max_items = config.max_items.unwrap_or(300);
    println!("Using max_items = {}", max_items);

    // Read feed URLs from "feeds.txt" (one URL per line)
    let feeds_content = fs::read_to_string("feeds.txt")?;
    let feed_urls: Vec<String> = feeds_content
        .lines()
        .map(|line| line.trim())
        .filter(|line| !line.is_empty())
        .map(|s| s.to_string())
        .collect();

    if feed_urls.is_empty() {
        eprintln!("No feed URLs found in feeds.txt");
        return Ok(());
    }

    // Concurrently fetch and parse feeds
    let mut all_items = Vec::new();
    let mut feed_data_list = Vec::new();
    let mut handles = Vec::new();
    for url in feed_urls {
        let url_owned = url.to_string();
        let handle = tokio::spawn(async move { fetch_feed_data(url_owned).await });
        handles.push(handle);
    }

    // Collect results from tasks
    for handle in handles {
        match handle.await? {
            Ok(feed_data) => {
                all_items.extend(feed_data.items.clone());
                feed_data_list.push(feed_data);
            },
            Err(e) => eprintln!("Error fetching feed: {}", e),
        }
    }

    // Sort items by publication date (newest first)
    all_items.sort_by(|a, b| b.pub_date.cmp(&a.pub_date));

    // Limit the list to the maximum number of items specified (0 means unlimited)
    if max_items > 0 && all_items.len() > max_items {
        all_items.truncate(max_items);
    }

    // Build a master RSS feed from the aggregated items
    let channel = build_master_feed(&all_items);

    // Write the generated RSS feed to a file
    fs::write("feeds/master.xml", channel.to_string())?;
    println!("Master feed generated with {} items", all_items.len());

    // Generate individual feed files
    for feed_data in feed_data_list {
        let filename = to_kebab_case(&feed_data.title);
        let filepath = format!("feeds/{}.xml", filename);
        
        let individual_channel = build_individual_feed(&feed_data);
        fs::write(&filepath, individual_channel.to_string())?;
        println!("Generated individual feed: {} ({} items)", filepath, feed_data.items.len());
    }

    Ok(())
}

// A simple struct to hold the feed item data
#[derive(Debug, Clone)]
struct FeedItem {
    title: String,
    link: String,
    description: Option<String>,
    pub_date: DateTime<FixedOffset>,
}

// Struct to hold both feed metadata and items
#[derive(Debug)]
struct FeedData {
    title: String,
    url: String,
    items: Vec<FeedItem>,
}

/// Fetches a feed from the given URL and parses its items and metadata.
async fn fetch_feed_data(url: String) -> Result<FeedData, Box<dyn Error + Send + Sync>> {
    let response = reqwest::get(&url).await?;
    let bytes = response.bytes().await?;
    let feed = parser::parse(bytes.as_ref())?;

    // Extract feed title
    let feed_title = feed.title
        .map(|t| t.content)
        .unwrap_or_else(|| url.clone());

    // Create a FixedOffset with zero offset.
    let offset = FixedOffset::east_opt(0).unwrap();

    let mut items = Vec::new();
    for entry in feed.entries {
        // Convert published/updated dates to DateTime<FixedOffset>
        let pub_date = entry
            .published
            .map(|d| d.with_timezone(&offset))
            .or(entry.updated.map(|d| d.with_timezone(&offset)))
            .unwrap_or_else(|| Utc::now().with_timezone::<FixedOffset>(&offset));

        // Use the first available link (if any)
        let link = if !entry.links.is_empty() {
            entry.links[0].href.clone()
        } else {
            String::new()
        };

        let title = entry
            .title
            .map(|t| t.content)
            .unwrap_or_else(|| String::from("No title"));

        let description = entry.summary.map(|s| s.content);

        items.push(FeedItem {
            title,
            link,
            description,
            pub_date,
        });
    }

    Ok(FeedData {
        title: feed_title,
        url,
        items,
    })
}

/// Builds an RSS channel (master feed) from the provided items.
fn build_master_feed(items: &[FeedItem]) -> Channel {
    let rss_items: Vec<Item> = items
        .iter()
        .map(|fi| {
            let mut builder = ItemBuilder::default();
            builder.title(fi.title.clone());
            builder.link(fi.link.clone());
            if let Some(desc) = &fi.description {
                builder.description(desc.clone());
            }
            // Format the publication date as RFC 2822 for RSS
            builder.pub_date(fi.pub_date.to_rfc2822());
            builder.build()
        })
        .collect();

    ChannelBuilder::default()
        .title("Master RSS Feed")
        .link("https://raw.githubusercontent.com/xavwe/rss-aggregator/refs/heads/main/feeds/master.xml")
        .description("Aggregated RSS feed")
        .items(rss_items)
        .build()
}

/// Builds an RSS channel for an individual feed.
fn build_individual_feed(feed_data: &FeedData) -> Channel {
    let rss_items: Vec<Item> = feed_data.items
        .iter()
        .map(|fi| {
            let mut builder = ItemBuilder::default();
            builder.title(fi.title.clone());
            builder.link(fi.link.clone());
            if let Some(desc) = &fi.description {
                builder.description(desc.clone());
            }
            // Format the publication date as RFC 2822 for RSS
            builder.pub_date(fi.pub_date.to_rfc2822());
            builder.build()
        })
        .collect();

    let filename = to_kebab_case(&feed_data.title);
    let github_link = format!("https://raw.githubusercontent.com/xavwe/rss-aggregator/refs/heads/main/feeds/{}.xml", filename);

    ChannelBuilder::default()
        .title(feed_data.title.clone())
        .link(github_link)
        .description(format!("Archived feed from {}", feed_data.url))
        .items(rss_items)
        .build()
}

/// Converts a string to kebab-case for use as a filename.
fn to_kebab_case(input: &str) -> String {
    let re = Regex::new(r"[^a-zA-Z0-9]+").unwrap();
    re.replace_all(input.to_lowercase().as_str(), "-")
        .trim_matches('-')
        .to_string()
}
