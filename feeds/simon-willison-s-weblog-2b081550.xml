<?xml version="1.0" encoding="utf-8"?><rss version="2.0"><channel><title>Simon Willison&apos;s Weblog</title><link>https://raw.githubusercontent.com/xavwe/rss-aggregator/refs/heads/main/feeds/simon-willison-s-weblog-2b081550.xml</link><description>Archived feed from https://simonwillison.net/atom/everything</description><item><title>My review of Claude&apos;s new Code Interpreter, released under a very confusing name</title><link>https://simonwillison.net/2025/Sep/9/claude-code-interpreter/#atom-everything</link><description><![CDATA[<p>Today on the Anthropic blog: <strong><a href="https://www.anthropic.com/news/create-files">Claude can now create and edit files</a></strong>:</p>
<blockquote>
<p>Claude can now create and edit Excel spreadsheets, documents, PowerPoint slide decks, and PDFs directly in <a href="https://claude.ai/">Claude.ai</a> and the desktop app. [...]</p>
<p>File creation is now available as a preview for Max, Team, and Enterprise plan users. Pro users will get access in the coming weeks.</p>
</blockquote>
<p>Then right at the <em>very end</em> of their post:</p>
<blockquote>
<p>This feature gives Claude internet access to create and analyze files, which may put your data at risk. Monitor chats closely when using this feature. <a href="https://support.anthropic.com/en/articles/12111783-create-and-edit-files-with-claude">Learn more</a>.</p>
</blockquote>
<p>And tucked away half way down their <a href="https://support.anthropic.com/en/articles/12111783-create-and-edit-files-with-claude">Create and edit files with Claude</a> help article:</p>
<blockquote>
<p>With this feature, Claude can also do more advanced data analysis and data science work. Claude can create Python scripts for data analysis. Claude can create data visualizations in image files like PNG. You can also upload CSV, TSV, and other files for data analysis and visualization.</p>
</blockquote>
<p>Talk about <a href="https://www.merriam-webster.com/wordplay/bury-the-lede-versus-lead">burying the lede</a>... this is their version of <a href="https://simonwillison.net/tags/code-interpreter/">ChatGPT Code Interpreter</a>, my all-time favorite feature of ChatGPT!</p>

<p>Claude can now write and execute custom Python (and Node.js) code in a server-side sandbox and use it to process and analyze data.</p>
<p>In a particularly egregious example of AI companies being terrible at naming features, the official name for this one really does appear to be <strong>Upgraded file creation and analysis</strong>. Sigh.</p>
<p>This is quite a confusing release, because Claude <em>already</em> had a variant of this feature, <a href="https://www.anthropic.com/news/analysis-tool">released in October 2024</a> with the weak but more sensible name <strong>Analysis tool</strong>. Here are <a href="https://simonwillison.net/2024/Oct/24/claude-analysis-tool/">my notes from when that came out</a>. That tool worked by generating and executing JavaScript in the user's own browser.</p>
<p>The new tool works entirely differently. It's much closer in implementation to OpenAI's Code Interpreter: Claude now has access to a server-side container environment in which it can run shell commands and execute Python and Node.js code to manipulate data and both read and generate files.</p>
<ul>
  <li><a href="https://simonwillison.net/2025/Sep/9/claude-code-interpreter/#switching-it-on-in-settings-features">Switching it on in settings/features</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/9/claude-code-interpreter/#exploring-the-environment">Exploring the environment</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/9/claude-code-interpreter/#starting-with-something-easy">Starting with something easy</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/9/claude-code-interpreter/#something-much-harder-recreating-the-ai-adoption-chart">Something much harder: recreating the AI adoption chart</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/9/claude-code-interpreter/#prompt-injection-risks">Prompt injection risks</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/9/claude-code-interpreter/#my-verdict-on-claude-code-interpreter-so-far">My verdict on Claude Code Interpreter so far</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/9/claude-code-interpreter/#ai-labs-find-explaining-this-feature-incredibly-difficult">AI labs find explaining this feature incredibly difficult</a></li>
</ul>
<h4 id="switching-it-on-in-settings-features">Switching it on in settings/features</h4>
<p>I have a Pro Plan but found the setting to enable it on the <a href="https://claude.ai/settings/features">claude.ai/settings/features</a>. It's possible my account was granted early access without me realizing, since the Pro plan isn't supposed to have it yet:</p>
<p><img src="https://static.simonwillison.net/static/2025/claude-analysis-toggle.jpg" alt="Experimental. Preview and provide feedback on upcoming enhancements to our platform. Please note: experimental features might influence Claude’s behavior and some interactions may differ from the standard experience. Analysis tool:  Claude can write and run code to process data, run analysis, and produce data visualizations in real time. Upgraded file creation and analysis: Allow Claude to create and edit docs, spreadsheets, presentations, PDFs, and data reports on web and desktop. Does not support versioning or remixing of Artifacts. This feature gives Claude network access to create and analyze files, which has security risks. Monitor chats closely when using this feature. The two features each have a toggle - the toggle for the file creation tool is turned on." style="max-width: 100%;" /></p>
<p>Interestingly, toggling on the "Upgraded file creation and analysis" option toggles off the "Analysis tool" option, and vice versa. You can't have both at the same time.</p>
<p>I guess this is to help reduce confusion? It surprised me though as Claude can sometimes use the existing analysis tool to help it in the creation of artifacts, as I <a href="https://simonwillison.net/2025/Jun/25/ai-powered-apps-with-claude/">noted here</a> when I wrote about the Claudeception feature they added in June.</p>
<h4 id="exploring-the-environment">Exploring the environment</h4>
<p>I started by using the tool to explore the tool. Here's my <a href="https://claude.ai/share/d796a596-380c-4db2-847d-2eeedc620bc0">first transcript</a> where I simply prompted the following to see what it could figure out about itself:</p>
<blockquote>
<p><code>Tell me everything you can about your shell and Python environments</code></p>
</blockquote>
<p>Here's part of the output:</p>
<p><img src="https://static.simonwillison.net/static/2025/claude-files-1.jpg" alt="My prompt, then Claude shows: I'll explore my shell and Python environments to give you a comprehensive overview. • Getting system information and OS details • Checking shell type and version • Getting shell info and bash version • Checking Python version and locations - this one is expandedd to show: bash python3 -version &amp;&amp; which python3 &amp;&amp; which python Output Python 3.12.3 /usr/bin/python3 /usr/bin/python" style="max-width: 100%;" /></p>
<p>And the key details it returned, each of which I've verified as true by checking the displayed output of the relevant commands:</p>
<blockquote>
<ul>
<li>
<strong>OS</strong>: Ubuntu 24.04.2 LTS (Noble) running on Linux kernel 4.4.0-</li>
<li>
<strong>Architecture</strong>: x86_64</li>
<li>
<strong>Shell</strong>: GNU Bash 5.2.21(1)-release</li>
<li>
<strong>Current User</strong>: root (with access to /home/claude directory)</li>
<li>
<strong>Working Directory</strong>: Currently in root (/), but /home/claude is the main workspace</li>
<li>
<strong>Python Version</strong>: Python 3.12.3</li>
<li>
<strong>Python Executable</strong>: /usr/bin/python3</li>
<li>
<strong>Package Manager</strong>: pip 24.0</li>
<li>
<strong>Python Path</strong>: Standard system locations including:
<ul>
<li>/usr/lib/python3.12 (standard library)</li>
<li>/usr/local/lib/python3.12/dist-packages (local packages)</li>
</ul>
</li>
<li>
<strong>Disk Space</strong>: 4.9GB total, with 4.6GB available</li>
<li>
<strong>Memory</strong>: 9.0GB RAM available, minimal usage currently</li>
<li>
<strong>Node.js</strong>: v18.19.1 available</li>
<li>
<strong>npm</strong>: Configured with global packages in <code>/home/claude/.npm-global</code>
</li>
</ul>
</blockquote>
<p>So we have an Ubuntu 24.04.2 container with 9GB of RAM, ~5GB of disk, Python 3.12.3, and Node.js v18.19.1.</p>
<p>How about network access? I tried the following:</p>
<blockquote>
<p><code>Can you access the Internet? Can you run curl? How about using Python http to fetch https://latest.datasette.io/fixtures.json ?</code></p>
</blockquote>
<p>Again, you can see the <a href="https://claude.ai/share/d796a596-380c-4db2-847d-2eeedc620bc0">detailed output in the transcript</a>. Claude tried <code>https://latest.datasette.io/fixtures.json</code> and then <code>https://httpbin.org/json</code> and got a 403 forbidden error for both, then <code>https://google.com</code> and got this curious result:</p>
<div class="highlight highlight-source-shell"><pre>curl -s -I https://google.com</pre></div>
<p>Output:</p>
<pre><code>HTTP/1.1 200 OK
date: Tue, 09 Sep 2025 16:02:17 GMT
server: envoy

HTTP/2 403 
content-length: 13
content-type: text/plain
date: Tue, 09 Sep 2025 16:02:17 GMT
</code></pre>
<p>Claude did note that it can still use the <code>web_fetch</code> and <code>web_search</code> containers independently of that container environment, so it should be able to fetch web content using tools running outside of the container and then write it to a file there.</p>
<p>On a hunch I tried this:</p>
<blockquote>
<p><code>Run pip install sqlite-utils</code></p>
</blockquote>
<p>... and it worked! Claude can <code>pip install</code> additional packages from <a href="https://pypi.org/">PyPI</a>.</p>

<p>A little more poking around revealed the following relevant environment variables:</p>
<pre><code>HTTPS_PROXY=http://21.0.0.167:15001
no_proxy=localhost,127.0.0.1,169.254.169.254,metadata.google.internal,*.svc.cluster.local,*.local,*.googleapis.com,*.google.com
NO_PROXY=localhost,127.0.0.1,169.254.169.254,metadata.google.internal,*.svc.cluster.local,*.local,*.googleapis.com,*.google.com
https_proxy=http://21.0.0.167:15001
http_proxy=http://21.0.0.167:15001
</code></pre>
<p>So based on an earlier HTTP header there's an <a href="https://www.envoyproxy.io/">Envoy proxy</a> running at an accessible port which apparently implements a strict allowlist.</p>
<p>I later noticed that <a href="https://support.anthropic.com/en/articles/12111783-create-and-edit-files-with-claude#h_0ee9d698a1">the help page</a> includes a full description of what's on that allowlist:</p>
<blockquote>
<p><strong>Anthropic Services (Explicit)</strong></p>
<p>api.anthropic.com, statsig.anthropic.com</p>
<p><strong>Version Control</strong></p>
<p>github.com</p>
<p><strong>Package Managers - JavaScript/Node</strong></p>
<p><strong>NPM:</strong> registry.npmjs.org, npmjs.com, npmjs.org<br />
<strong>Yarn:</strong> yarnpkg.com, registry.yarnpkg.com</p>
<p><strong>Package Managers - Python</strong></p>
<p>pypi.org, files.pythonhosted.org, pythonhosted.org</p>
</blockquote>
<p>So it looks like we have a <em>very</em> similar system to ChatGPT Code Interpreter. The key differences are that Claude's system can install additional Python packages and has Node.js pre-installed.</p>
<p>One important limitation from the docs:</p>
<blockquote>
<p>The maximum file size is 30MB per file for both uploads and downloads.</p>
</blockquote>
<p>The ChatGPT <a href="https://help.openai.com/en/articles/8555545-file-uploads-faq">limit here</a> is 512MB. I've often uploaded 100MB+ SQLite database files to ChatGPT, so I'm a little disappointed by this lower limit for Claude.</p>
<h4 id="starting-with-something-easy">Starting with something easy</h4>
<p>I grabbed a copy of the SQLite database behind <a href="https://til.simonwillison.net/">my TILs website</a> (21.9MB <a href="https://s3.amazonaws.com/til.simonwillison.net/tils.db">from here</a>) and uploaded it to Claude, then prompted:</p>
<blockquote>
<p><code>Use your Python environment to explore this SQLite database and generate a PDF file containing a join diagram of all the tables</code></p>
</blockquote>
<p>Here's <a href="https://claude.ai/share/f91a95be-0fb0-4e14-b46c-792b47117a3d">that conversation</a>. It did an OK job, producing both <a href="https://static.simonwillison.net/static/2025/til_database_join_diagram.pdf">the PDF</a> I asked for and a PNG equivalent which looks like this (since created files are not available in shared chats):</p>
<p><img src="https://static.simonwillison.net/static/2025/til_database_join_diagram.jpg" alt="Each table gets a box with a name and columns. A set of lines is overlaid which doesn't quite seem to represent the joins in a useful fashion." style="max-width: 100%;" /></p>
<p>This isn't an ideal result - those join lines are difficult to follow - but I'm confident I could get from here to something I liked with only a little more prompting. The important thing is that the system clearly works, and can analyze data in uploaded SQLite files and use them to produce images and PDFs.</p>
<h4 id="something-much-harder-recreating-the-ai-adoption-chart">Something much harder: recreating the AI adoption chart</h4>
<p>Thankfully I have a fresh example of a really challenging ChatGPT Code Interpreter task from just last night, which I described in great detail in <a href="https://simonwillison.net/2025/Sep/9/apollo-ai-adoption/">Recreating the Apollo AI adoption rate chart with GPT-5, Python and Pyodide</a>.</p>
<p>Short version: I took <a href="https://www.apolloacademy.com/ai-adoption-rate-trending-down-for-large-companies/">this chart</a> from Apollo Global and asked ChatGPT to recreate it based on a screenshot and an uploaded XLSX file.</p>
<p><img src="https://static.simonwillison.net/static/2025/apollo-ai-chart.jpg" alt="AI adoption rates starting to decline for larger firms. A chart of AI adoption rate by firm size. Includes lines for 250+, 100-249, 50-99, 20-49, 10-19, 5-8 and 1-4 sized organizations. Chart starts in November 2023 with percentages ranging from 3 to 5, then all groups grow through August 2025 albeit with the 250+ group having a higher score than the others. That 25+ group peaks in Jul5 2025 at around 14% and then appears to slope slightly downwards to 12% by August. Some of the other lines also start to tip down, though not as much." style="max-width: 100%;" /></p>
<p>This time I skipped the bit where I had ChatGPT hunt down the original data and jumped straight to the "recreate this chart" step. I used the exact same prompt as I provided to ChatGPT:</p>
<blockquote>
<p><code>Use this data to recreate this chart using python</code></p>
</blockquote>
<p>And uploaded the same two files - <a href="https://static.simonwillison.net/static/cors-allow/2025/Employment-Size-Class-Sep-2025.xlsx">this XLSX file</a> and the <a href="https://static.simonwillison.net/static/2025/apollo-ai-chart.jpg">screenshot of the original chart</a>.</p>
<p><img src="https://static.simonwillison.net/static/2025/claude-files-2.jpg" alt="Claude UI - prompt is &quot;Use this data to recreate this chart using python&quot; - I've selected Sonnet 4 and given it both the XLSX and the screenshot as attachments." style="max-width: 100%;" /></p>
<p>Claude wrote and ran a bunch of Python code and produced this:</p>
<p><img src="https://static.simonwillison.net/static/2025/claude-attempt-1.jpg" alt="The chart has jagged lines, not smooth ones, but they do appear to show the correct numbers. The title and axis are positioned in ways that overlap other text a bit." style="max-width: 100%;" /></p>
<p>That's not a bad first attempt - we'll sort out the jagged lines in a moment. With ChatGPT I later told it to plot a six-survey rolling average (since that's what Apollo had done with theirs), so I ran the exact same prompt against Claude:</p>
<blockquote>
<p><code>Plot it as a six survey rolling average</code></p>
</blockquote>
<p><img src="https://static.simonwillison.net/static/2025/claude-attempt-2.jpg" alt="The lines are more even now, and appear to match the numbers reported by Apollo. They still have a jagged look to them though." style="max-width: 100%;" /></p>
<p>So I told Claude:</p>
<blockquote>
<p><code>Make those lines less jagged</code></p>
</blockquote>
<p>This did not do what I wanted:</p>
<p><img src="https://static.simonwillison.net/static/2025/claude-attempt-3.jpg" alt="The chart looks mostly the same. The lines are a tiny bit thicker." style="max-width: 100%;" /></p>
<p>So I gave it a bit of a hint:</p>
<blockquote>
<p><code>No not like that, I wanted the lines to be a smooth curve like in the original screenshot - you shouldn't have to use scipy for this or manipulate the data, I'm sure matplotlib can do this already</code></p>
</blockquote>
<p>A little bit of sycophancy:</p>
<blockquote>
<p>You're absolutely right! Matplotlib has built-in curve smoothing. I was overcomplicating it. Let me create a version that uses matplotlib's natural curve smoothing capabilities. [...]</p>
</blockquote>
<p>This time it used the <code>antialiased=True</code> and <code>solid_capstyle='round'</code> options in matplotlib, which again wasn't what I was after. I tried one more thing:</p>
<blockquote>
<p>I did not mean smooth the rendering of the lines. I meant that I wanted curved and not straight lines between points.</p>
</blockquote>
<p>(Normally if I have to argue this much with a model I'll start a fresh session and try a different prompting strategy from the start.)</p>
<p>But this did give me what I was looking for:</p>
<p><img src="https://static.simonwillison.net/static/2025/claude-attempt-4.jpg" alt="This time the chart finally has smooth curves between points." style="max-width: 100%;" /></p>
<p>Oddly enough my ChatGPT chart suffered from that same overlapping title glitch, so I gave Claude the same prompt I had given ChatGPT:</p>
<blockquote>
<p><code>fix the chart title, it looks like this:</code> (with a screenshot)</p>
</blockquote>
<p><img src="https://static.simonwillison.net/static/2025/claude-attempt-5.jpg" alt="Now the chart shows a clear title at the top saying AI adoption rates starting to decline for larger firms" style="max-width: 100%;" /></p>
<p>We got there in the end! I've shared <a href="https://claude.ai/share/cc32d405-cb53-4e52-a1a0-9b4df4e528ac">the full transcript of the chat</a>, although frustratingly the images and some of the code may not be visible. I <a href="https://gist.github.com/simonw/806e1aa0e6c29ad64834037f779e0dc0">created this Gist</a> with copies of the files that it let me download.</p>
<h4 id="prompt-injection-risks">Prompt injection risks</h4>
<p>ChatGPT Code Interpreter has no access to the internet at all, which limits how much damage an attacker can do if they manage to sneak their own malicious instructions into the model's context.</p>
<p>Since Claude Code Interpreter (I'm <em>not</em> going to be calling it "Upgraded file creation and analysis"!) has a limited form of internet access, we need to worry about <a href="https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/">lethal trifecta</a> and other prompt injection attacks.</p>
<p>The <a href="https://support.anthropic.com/en/articles/12111783-create-and-edit-files-with-claude#h_0ee9d698a1">help article</a> actually covers this in some detail:</p>
<blockquote>
<p>It is possible for a bad actor to inconspicuously add instructions via external files or websites that trick Claude into:</p>
<ol>
<li>Downloading and running untrusted code in the sandbox environment for malicious purposes.</li>
<li>Reading sensitive data from a <a href="http://claude.ai">claude.ai</a> connected knowledge source (e.g., Remote MCP, projects) and using the sandbox environment to make an external network request to leak the data.</li>
</ol>
<p>This means Claude can be tricked into sending information from its context (e.g., prompts, projects, data via MCP, Google integrations) to malicious third parties. To mitigate these risks, we recommend you monitor Claude while using the feature and stop it if you see it using or accessing data unexpectedly.</p>
</blockquote>
<p>"We recommend you monitor Claude while using the feature" smells me to me like unfairly outsourcing the problem to Anthropic's users, but I'm not sure what more they can do!</p>
<p>It's interesting that they still describe the external communication risk even though they've locked down a lot of network access. My best guess is that they know that allowlisting <code>github.com</code> opens an <em>enormous</em> array of potential exfiltration vectors.</p>
<p>Anthropic also note:</p>
<blockquote>
<p>We have performed red-teaming and security testing on the feature. We have a continuous process for ongoing security testing and red-teaming of this feature.</p>
</blockquote>
<p>I plan to be cautious using this feature with any data that I very much don't want to be leaked to a third party, if there's even the slightest chance that a malicious instructions might sneak its way in.</p>
<h4 id="my-verdict-on-claude-code-interpreter-so-far">My verdict on Claude Code Interpreter so far</h4>
<p>I'm generally very excited about this. Code Interpreter has been my most-valued LLM feature since it launched in early 2023, and the Claude version includes some upgrades on the original - package installation, Node.js support - that I expect will be very useful.</p>
<p>I don't particularly mark it down for taking a little more prompting to recreate the Apollo chart than ChatGPT did. For one thing I was using Claude Sonnet 4 - I expect Claude Opus 4.1 would have done better. I also have a much stronger intuition for Code Interpreter prompts that work with GPT-5.</p>
<p>I don't think my chart recreation exercise here should be taken as showing any meaningful differences between the two.</p>
<h4 id="ai-labs-find-explaining-this-feature-incredibly-difficult">AI labs find explaining this feature incredibly difficult</h4>
<p>I find it <em>fascinating</em> how difficult the AI labs find describing this feature to people! OpenAI went from "Code Interpreter" to "Advanced Data Analysis" and maybe back again? It's hard to even find their official landing page for that feature now. (I <a href="https://chatgpt.com/share/68c070ff-fe9c-8006-91b5-cff799253836">got GPT-5 to look for it</a> and it hunted for 37 seconds and settled on the help page for <a href="https://help.openai.com/en/articles/8437071-data-analysis-with-chatgpt">Data analysis with ChatGPT</a>).</p>
<p>Anthropic already used the bad name "Analysis tool" for a different implementation, and now have the somehow-worse name "Upgraded file creation and analysis". Their launch announcement avoids even talking about code execution, focusing exclusively on the tool's ability to generate spreadsheets and PDFs!</p>
<p>I wonder if any of the AI labs will crack the code on how to name and explain this thing? I feel like it's still a very under-appreciated feature of LLMs, despite having been around for more than two years now.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/node">node</a>, <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/visualization">visualization</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/prompt-injection">prompt-injection</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/chatgpt">chatgpt</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/code-interpreter">code-interpreter</a>, <a href="https://simonwillison.net/tags/llm-tool-use">llm-tool-use</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a></p>]]></description><pubDate>Tue, 9 Sep 2025 18:11:32 +0000</pubDate></item><item><title>The 2025 PSF Board Election is Open!</title><link>https://simonwillison.net/2025/Sep/9/psf-board-election/#atom-everything</link><description><![CDATA[<p><strong><a href="https://pyfound.blogspot.com/2025/09/the-2025-psf-board-election-is-open.html">The 2025 PSF Board Election is Open!</a></strong></p>
The Python Software Foundation's annual board member election is taking place right now, with votes (from previously affirmed voting members) accepted from September 2nd, 2:00 pm UTC through Tuesday, September 16th, 2:00 pm UTC.</p>
<p>I've served on the board since 2022 and I'm running for a second term. Here's the opening section of my <a href="https://www.python.org/nominations/elections/2025-python-software-foundation-board/nominees/">nomination statement</a>.</p>
<blockquote>
<p>Hi, I'm Simon Willison. I've been a board member of the Python Software Foundation since 2022 and I'm running for re-election in 2025.</p>
<p>Last year I wrote a detailed article about <a href="https://simonwillison.net/2024/Sep/18/board-of-the-python-software-foundation/">Things I’ve learned serving on the board of the Python Software Foundation</a>. I hope to continue learning and sharing what I've learned for a second three-year term.</p>
<p>One of my goals for a second term is to help deepen the relationship between the AI research world and the Python Software Foundation. There is an enormous amount of value being created in the AI space using Python and I would like to see more of that value flow back into the rest of the Python ecosystem.</p>
<p>I see the Python Package Index (PyPI) as one of the most impactful projects of the Python Software Foundation and plan to continue to advocate for further investment in the PyPI team and infrastructure.</p>
<p>As a California resident I'm excited to see PyCon return to the West Coast, and I'm looking forward to getting involved in helping make PyCon 2026 and 2027 in Long Beach, California as successful as possible.</p>
</blockquote>
<p>I'm delighted to have been endorsed this year by <a href="https://inventwithpython.com/blog/psf-candidate-endorsements-2025.html">Al Sweigart</a>, <a href="https://fosstodon.org/@lorenipsum/115170249309856873">Loren Crary</a> and <a href="https://social.coop/@chrisjrn/115135449245231588">Christopher Neugebauer</a>. If you are a voting member I hope I have earned your vote this year.</p>
<p>You can watch video introductions from several of the other nominees <a href="https://www.youtube.com/watch?v=MM9lLXH-GjA">in this six minute YouTube video</a> and <a href="https://www.youtube.com/playlist?list=PLFIcqSiijithlBSVBvZzrlGwhGfuT8uzp">this playlist</a>.


    <p>Tags: <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/psf">psf</a></p>]]></description><pubDate>Tue, 9 Sep 2025 10:13:33 +0000</pubDate></item><item><title>Geoffrey Huntley is cursed</title><link>https://simonwillison.net/2025/Sep/9/cursed/#atom-everything</link><description><![CDATA[<p><strong><a href="https://ghuntley.com/cursed/">I ran Claude in a loop for three months, and it created a genz programming language called cursed</a></strong></p>
Geoffrey Huntley vibe-coded an entirely new programming language using Claude:</p>
<blockquote>
<p>The programming language is called "cursed". It's cursed in its lexical structure, it's cursed in how it was built, it's cursed that this is possible, it's cursed in how cheap this was, and it's cursed through how many times I've sworn at Claude.</p>
</blockquote>
<p>Geoffrey's initial prompt:</p>
<blockquote>
<p><code>Hey, can you make me a programming language like Golang but all the lexical keywords are swapped so they're Gen Z slang?</code></p>
</blockquote>
<p>Then he pushed it to keep on iterating over a three month period.</p>
<p>Here's Hello World:</p>
<pre><code>vibe main
yeet "vibez"

slay main() {
    vibez.spill("Hello, World!")
}
</code></pre>
<p>And here's <a href="https://github.com/ghuntley/cursed/blob/ecda33d496e1562e0e02efb25b6936ad94e79b72/test_suite/leetcode_comprehensive_suite/binary_search/704_binary_search_backup.%F0%9F%92%80">binary search</a>, part of <a href="https://github.com/ghuntley/cursed/tree/zig/test_suite/leetcode_comprehensive_suite">17+ LeetCode problems</a> that run as part of the test suite:</p>
<pre><code>slay binary_search(nums normie[], target normie) normie {
    sus left normie = 0
    sus right normie = len(nums) - 1    
    bestie (left &lt;= right) {
        sus mid normie = left + (right - left) / 2
        ready (nums[mid] == target) {
            damn mid
        }
        ready (nums[mid] &lt; target) {
            left = mid + 1
        } otherwise {
            right = mid - 1
        }
    }
    damn -1
}
</code></pre>
<p>This is a <em>substantial</em> project. The repository currently has <a href="https://github.com/ghuntley/cursed/commits/zig/">1,198 commits</a>. It has both an interpreter mode and a compiler mode, and can compile programs to native binaries (via LLVM) for macOS, Linux and Windows.</p>
<p>It looks like it was mostly built using Claude running via <a href="https://ampcode.com/">Sourcegraph's Amp</a>, which produces <a href="https://github.com/ghuntley/cursed/commit/ec5be8a4c4f6e82f6b93774a9b3b3f88308680dd">detailed commit messages</a>. The commits include links to archived Amp sessions but sadly those don't appear to be publicly visible.</p>
<p>The first version was written in C, then Geoffrey had Claude port it to Rust and then Zig. <a href="https://twitter.com/GeoffreyHuntley/status/1965295152962097550">His cost estimate</a>:</p>
<blockquote>
<p>Technically it costs about 5k usd to build your own compiler now because cursed was implemented first in c, then rust, now zig. So yeah, it’s not one compiler it’s three editions of it. For a total of $14k USD.</p>
</blockquote>

    <p><small></small>Via <a href="https://x.com/GeoffreyHuntley/status/1965258228314636524">@GeoffreyHuntley</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/c">c</a>, <a href="https://simonwillison.net/tags/programming-languages">programming-languages</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/rust">rust</a>, <a href="https://simonwillison.net/tags/zig">zig</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a>, <a href="https://simonwillison.net/tags/geoffrey-huntley">geoffrey-huntley</a></p>]]></description><pubDate>Tue, 9 Sep 2025 09:31:21 +0000</pubDate></item><item><title>Recreating the Apollo AI adoption rate chart with GPT-5, Python and Pyodide</title><link>https://simonwillison.net/2025/Sep/9/apollo-ai-adoption/#atom-everything</link><description><![CDATA[<p>Apollo Global Management's "Chief Economist" Dr. Torsten Sløk released <a href="https://www.apolloacademy.com/ai-adoption-rate-trending-down-for-large-companies/">this interesting chart</a> which appears to show a slowdown in AI adoption rates among large (&gt;250 employees) companies:</p>
<p><img src="https://static.simonwillison.net/static/2025/apollo-ai-chart.jpg" alt="AI adoption rates starting to decline for larger firms. A chart of AI adoption rate by firm size. Includes lines for 250+, 100-249, 50-99, 20-49, 10-19, 5-8 and 1-4 sized organizations. Chart starts in November 2023 with percentages ranging from 3 to 5, then all groups grow through August 2025 albeit with the 250+ group having a higher score than the others. That 25+ group peaks in Jul5 2025 at around 14% and then appears to slope slightly downwards to 12% by August. Some of the other lines also start to tip down, though not as much." style="max-width: 100%;" /></p>
<p>Here's the full description that accompanied the chart:</p>
<blockquote>
<p>The US Census Bureau conducts a biweekly survey of 1.2 million firms, and one question is whether a business has used AI tools such as machine learning, natural language processing, virtual agents or voice recognition to help produce goods or services in the past two weeks. Recent data by firm size shows that AI adoption has been declining among companies with more than 250 employees, see chart below.</p>
</blockquote>
<p>(My first thought on seeing that chart is that I hope it represents the <em>peak of inflated expectations</em> leading into the <em>trough of dissillusionment</em> in the <a href="https://en.wikipedia.org/wiki/Gartner_hype_cycle">Gartner Hype Cycle</a> (which Wikipedia calls "largely disputed, with studies pointing to it being inconsistently true at best"), since that means we might be reaching the end of the initial hype phase and heading towards the <em>slope of enlightenment</em>.)</p>
<h4 id="finding-the-us-census-data-with-gpt-5-search">Finding the US Census data with GPT-5 search</h4>
<p>This is the first I'd heard of the US Census Bureau running a biweekly (that's once every two weeks) survey about AI!</p>
<p>I decided to track down the numbers and see if I could recreate the chart myself. And since <a href="https://simonwillison.net/2025/Sep/6/research-goblin/">GPT-5 is really good at search now</a> I fed it the following prompt to see how well it could do:</p>
<blockquote>
<p><code>&gt; The US Census Bureau conducts a biweekly survey of 1.2 million firms, and one question is whether a business has used AI tools such as machine learning, natural language processing, virtual agents or voice recognition to help produce goods or services in the past two weeks. Recent data by firm size shows that AI adoption has been declining among companies with more than 250 employees, see chart below.</code></p>
<p><code>Find me that original data</code></p>
</blockquote>
<p>It took 1m19s to <a href="https://chatgpt.com/share/68bf48cf-0e70-8006-a045-96fa8e7ddfc1">find exactly what I needed</a>:</p>
<blockquote>
<p>Here’s the original U.S. Census Bureau source for that series:</p>
<ul>
<li>
<strong>BTOS “Employment size class” time series (XLSX)</strong> — biweekly shares of firms answering <strong>Yes</strong> to “In the last two weeks, did this business use Artificial Intelligence (AI) in producing goods or services?”, broken out by firm-size buckets (incl. <strong>250+ employees</strong>). Coverage: <strong>Sep 11, 2023 → Aug 24, 2025</strong>. (<a href="https://www.census.gov/hfp/btos/data_downloads">Census.gov</a>) [...]</li>
</ul>
</blockquote>
<p>That <a href="https://www.census.gov/hfp/btos/data_downloads">Census page</a> was not <em>at all</em> obvious. Thankfully GPT-5 had tipped me off to the "Employment size class" file, this link here:</p>
<p><img src="https://static.simonwillison.net/static/2025/census-page.jpg" alt="US Census website. Business Trends and Outlook Survey, Updated August 28, 2025. Current Data has 6 visible XLSX files with names like WFH Supplement, WFH Questions 27-29, National, Sectur, Subsector and Emplomyent size class. A red arrow highlights that last one." style="max-width: 100%;" /></p>
<p>So I downloaded that file, and confirmed that it was indeed a spreadsheet containing the data I wanted (in among all sorts of other survey questions). Here's <a href="https://static.simonwillison.net/static/cors-allow/2025/Employment-Size-Class-Sep-2025.xlsx">a 374KB XLSX copy</a> of the file I downloaded.</p>
<h4 id="recreating-the-chart-with-gpt-5-code-interpreter">Recreating the chart with GPT-5 code interpreter</h4>
<p>So what should I do with it now? I decided to see if GPT-5 could turn the spreadsheet back into that original chart, using Python running in its <a href="https://simonwillison.net/tags/code-interpreter/">code interpreter</a> tool.</p>
<p>So I uploaded the XLSX file back to ChatGPT, dropped in a screenshot of the Apollo chart and prompted:</p>
<blockquote>
<p><code>Use this data to recreate this chart using python</code></p>
</blockquote>
<p><img src="https://static.simonwillison.net/static/2025/chart-prompt.jpg" alt="ChatGPT. I dropped in a screenshot of the chart, uploaded the spreadsheet which turned into an inline table browser UI and prompted it to recreate the chart using python." style="max-width: 100%;" /></p>
<p>I thought this was a pretty tall order, but it's always worth throwing big challenges at an LLM to learn from how well it does.</p>
<p>It <em>really worked hard on this</em>. I didn't time it exactly but it spent at least 7 minutes "reasoning" across 5 different thinking blocks, interspersed with over a dozen Python analysis sessions. It used <code>pandas</code> and <code>numpy</code> to explore the uploaded spreadsheet and find the right figures, then tried several attempts at plotting with <code>matplotlib</code>.</p>
<p>As far as I can tell GPT-5 in ChatGPT can now feed charts it creates back into its own vision model, because it appeared to render a broken (empty) chart and then keep on trying to get it working.</p>
<p>It found a data dictionary in the last tab of the spreadsheet and used that to build a lookup table matching the letters <code>A</code> through <code>G</code> to the actual employee size buckets.</p>
<p>At the end of the process it spat out this chart:</p>
<p><img src="https://static.simonwillison.net/static/2025/recreated-chart-1.jpg" alt="matplotlib chart. The title is AI adoption rates starting to decline for larger firms, though there's a typography glitch in that title. It has a neat legend for the different size ranges, then a set of lines that look about right compared to the above graph - but they are more spiky and the numbers appear to trend up again at the end of the chart." style="max-width: 100%;" /></p>
<p>At first glance I thought it had nailed it... but then I compared the chart more closely with the Apollo original and spotted some definite discrepancies. GPT-5's chart peaked at 14.5% but the highest value in Apollo's was more like 13.5%. The GPT-5 chart was spikier - and most interestingly it included a clear uptick in the last data point where Apollo's had trended downwards.</p>
<p>I decided it was time to look at the actual data. I opened up the spreadsheet in Numbers, found the AI question columns and manually reviewed them. They seemed to match the GPT-5 chart results - so why the difference to Apollo's?</p>
<p>Then I noticed a crucial detail in the Apollo chart that I had cropped out of my original screenshot!</p>
<blockquote>
<p>Note: Data is six-survey moving average.</p>
</blockquote>
<p>So I told ChatGPT:</p>
<blockquote>
<p><code>Do the first question, plot it as a six survey rolling average</code></p>
</blockquote>
<p>I asked for the first question because it turned out there were two that were relevant in the survey spreadsheet.</p>
<ol>
<li>In the last two weeks, did this business use Artificial Intelligence (AI) in producing goods or services? (Examples of AI: machine learning, natural language processing, virtual agents, voice recognition, etc.)</li>
<li>During the next six months, do you think this business will be using Artificial Intelligence (AI) in producing goods or services? (Examples of AI: machine learning, natural language processing, virtual agents, voice recognition, etc.)</li>
</ol>
<p>It churned away for a little longer, added this code to the script:</p>
<pre><span class="pl-c"># Compute 6-survey rolling average (biweekly cadence → ~12 weeks)</span>
<span class="pl-s1">rolled</span> <span class="pl-c1">=</span> <span class="pl-s1">wide</span>.<span class="pl-c1">rolling</span>(<span class="pl-s1">window</span><span class="pl-c1">=</span><span class="pl-c1">6</span>, <span class="pl-s1">min_periods</span><span class="pl-c1">=</span><span class="pl-c1">6</span>).<span class="pl-c1">mean</span>()</pre>
<p>And popped out this chart (after I told it to fix the glitch in the title):</p>
<p><img src="https://static.simonwillison.net/static/2025/recreated-chart-2.jpg" alt="Second chart. This time the lines are basically an exact match for the Apollo one." style="max-width: 100%;" /></p>
<p>I think it's done it! This is a very solid match for the Apollo original, recreated using <code>matplotlib</code> and <code>pandas</code> from the same underlying source data from the US Census.</p>
<p>Here's the full Python code it wrote, which I think is quite readable (in as much as Pandas code can be):</p>
<pre><span class="pl-k">import</span> <span class="pl-s1">pandas</span> <span class="pl-k">as</span> <span class="pl-s1">pd</span>
<span class="pl-k">import</span> <span class="pl-s1">matplotlib</span>.<span class="pl-s1">pyplot</span> <span class="pl-k">as</span> <span class="pl-s1">plt</span>
<span class="pl-k">from</span> <span class="pl-s1">matplotlib</span>.<span class="pl-s1">ticker</span> <span class="pl-k">import</span> <span class="pl-v">PercentFormatter</span>

<span class="pl-s1">path</span> <span class="pl-c1">=</span> <span class="pl-s">"/mnt/data/Employment Size Class.xlsx"</span>

<span class="pl-s1">resp</span> <span class="pl-c1">=</span> <span class="pl-s1">pd</span>.<span class="pl-c1">read_excel</span>(<span class="pl-s1">path</span>, <span class="pl-s1">sheet_name</span><span class="pl-c1">=</span><span class="pl-s">"Response Estimates"</span>)
<span class="pl-s1">dates</span> <span class="pl-c1">=</span> <span class="pl-s1">pd</span>.<span class="pl-c1">read_excel</span>(<span class="pl-s1">path</span>, <span class="pl-s1">sheet_name</span><span class="pl-c1">=</span><span class="pl-s">"Collection and Reference Dates"</span>)

<span class="pl-s1">is_current</span> <span class="pl-c1">=</span> <span class="pl-s1">resp</span>[<span class="pl-s">"Question"</span>].<span class="pl-c1">astype</span>(<span class="pl-s1">str</span>).<span class="pl-c1">str</span>.<span class="pl-c1">strip</span>().<span class="pl-c1">str</span>.<span class="pl-c1">startswith</span>(<span class="pl-s">"In the last two weeks"</span>)
<span class="pl-s1">ai_yes</span> <span class="pl-c1">=</span> <span class="pl-s1">resp</span>[<span class="pl-s1">is_current</span> <span class="pl-c1">&amp;</span> <span class="pl-s1">resp</span>[<span class="pl-s">"Answer"</span>].<span class="pl-c1">astype</span>(<span class="pl-s1">str</span>).<span class="pl-c1">str</span>.<span class="pl-c1">strip</span>().<span class="pl-c1">str</span>.<span class="pl-c1">lower</span>().<span class="pl-c1">eq</span>(<span class="pl-s">"yes"</span>)].<span class="pl-c1">copy</span>()

<span class="pl-s1">code_to_bucket</span> <span class="pl-c1">=</span> {<span class="pl-s">"A"</span>:<span class="pl-s">"1-4"</span>,<span class="pl-s">"B"</span>:<span class="pl-s">"5-9"</span>,<span class="pl-s">"C"</span>:<span class="pl-s">"10-19"</span>,<span class="pl-s">"D"</span>:<span class="pl-s">"20-49"</span>,<span class="pl-s">"E"</span>:<span class="pl-s">"50-99"</span>,<span class="pl-s">"F"</span>:<span class="pl-s">"100-249"</span>,<span class="pl-s">"G"</span>:<span class="pl-s">"250 or more employees"</span>}
<span class="pl-s1">ai_yes</span>[<span class="pl-s">"Bucket"</span>] <span class="pl-c1">=</span> <span class="pl-s1">ai_yes</span>[<span class="pl-s">"Empsize"</span>].<span class="pl-c1">map</span>(<span class="pl-s1">code_to_bucket</span>)

<span class="pl-s1">period_cols</span> <span class="pl-c1">=</span> [<span class="pl-s1">c</span> <span class="pl-k">for</span> <span class="pl-s1">c</span> <span class="pl-c1">in</span> <span class="pl-s1">ai_yes</span>.<span class="pl-c1">columns</span> <span class="pl-k">if</span> <span class="pl-en">str</span>(<span class="pl-s1">c</span>).<span class="pl-c1">isdigit</span>() <span class="pl-c1">and</span> <span class="pl-en">len</span>(<span class="pl-en">str</span>(<span class="pl-s1">c</span>))<span class="pl-c1">==</span><span class="pl-c1">6</span>]
<span class="pl-s1">long</span> <span class="pl-c1">=</span> <span class="pl-s1">ai_yes</span>.<span class="pl-c1">melt</span>(<span class="pl-s1">id_vars</span><span class="pl-c1">=</span>[<span class="pl-s">"Bucket"</span>], <span class="pl-s1">value_vars</span><span class="pl-c1">=</span><span class="pl-s1">period_cols</span>, <span class="pl-s1">var_name</span><span class="pl-c1">=</span><span class="pl-s">"Smpdt"</span>, <span class="pl-s1">value_name</span><span class="pl-c1">=</span><span class="pl-s">"value"</span>)

<span class="pl-s1">dates</span>[<span class="pl-s">"Smpdt"</span>] <span class="pl-c1">=</span> <span class="pl-s1">dates</span>[<span class="pl-s">"Smpdt"</span>].<span class="pl-c1">astype</span>(<span class="pl-s1">str</span>)
<span class="pl-s1">long</span>[<span class="pl-s">"Smpdt"</span>] <span class="pl-c1">=</span> <span class="pl-s1">long</span>[<span class="pl-s">"Smpdt"</span>].<span class="pl-c1">astype</span>(<span class="pl-s1">str</span>)
<span class="pl-s1">merged</span> <span class="pl-c1">=</span> <span class="pl-s1">long</span>.<span class="pl-c1">merge</span>(<span class="pl-s1">dates</span>[[<span class="pl-s">"Smpdt"</span>,<span class="pl-s">"Ref End"</span>]], <span class="pl-s1">on</span><span class="pl-c1">=</span><span class="pl-s">"Smpdt"</span>, <span class="pl-s1">how</span><span class="pl-c1">=</span><span class="pl-s">"left"</span>)
<span class="pl-s1">merged</span>[<span class="pl-s">"date"</span>] <span class="pl-c1">=</span> <span class="pl-s1">pd</span>.<span class="pl-c1">to_datetime</span>(<span class="pl-s1">merged</span>[<span class="pl-s">"Ref End"</span>], <span class="pl-s1">errors</span><span class="pl-c1">=</span><span class="pl-s">"coerce"</span>)

<span class="pl-s1">merged</span>[<span class="pl-s">"value"</span>] <span class="pl-c1">=</span> <span class="pl-s1">pd</span>.<span class="pl-c1">to_numeric</span>(<span class="pl-s1">long</span>[<span class="pl-s">"value"</span>].<span class="pl-c1">astype</span>(<span class="pl-s1">str</span>).<span class="pl-c1">str</span>.<span class="pl-c1">replace</span>(<span class="pl-s">"%"</span>,<span class="pl-s">""</span>,<span class="pl-s1">regex</span><span class="pl-c1">=</span><span class="pl-c1">False</span>).<span class="pl-c1">str</span>.<span class="pl-c1">strip</span>(), <span class="pl-s1">errors</span><span class="pl-c1">=</span><span class="pl-s">"coerce"</span>)

<span class="pl-s1">order</span> <span class="pl-c1">=</span> [<span class="pl-s">"250 or more employees"</span>,<span class="pl-s">"100-249"</span>,<span class="pl-s">"50-99"</span>,<span class="pl-s">"20-49"</span>,<span class="pl-s">"10-19"</span>,<span class="pl-s">"5-9"</span>,<span class="pl-s">"1-4"</span>]
<span class="pl-s1">wide</span> <span class="pl-c1">=</span> <span class="pl-s1">merged</span>.<span class="pl-c1">pivot_table</span>(<span class="pl-s1">index</span><span class="pl-c1">=</span><span class="pl-s">"date"</span>, <span class="pl-s1">columns</span><span class="pl-c1">=</span><span class="pl-s">"Bucket"</span>, <span class="pl-s1">values</span><span class="pl-c1">=</span><span class="pl-s">"value"</span>, <span class="pl-s1">aggfunc</span><span class="pl-c1">=</span><span class="pl-s">"mean"</span>).<span class="pl-c1">sort_index</span>()
<span class="pl-s1">wide</span> <span class="pl-c1">=</span> <span class="pl-s1">wide</span>[[<span class="pl-s1">c</span> <span class="pl-k">for</span> <span class="pl-s1">c</span> <span class="pl-c1">in</span> <span class="pl-s1">order</span> <span class="pl-k">if</span> <span class="pl-s1">c</span> <span class="pl-c1">in</span> <span class="pl-s1">wide</span>.<span class="pl-c1">columns</span>]]
<span class="pl-s1">rolled</span> <span class="pl-c1">=</span> <span class="pl-s1">wide</span>.<span class="pl-c1">rolling</span>(<span class="pl-s1">window</span><span class="pl-c1">=</span><span class="pl-c1">6</span>, <span class="pl-s1">min_periods</span><span class="pl-c1">=</span><span class="pl-c1">6</span>).<span class="pl-c1">mean</span>()

<span class="pl-s1">start</span>, <span class="pl-s1">end</span> <span class="pl-c1">=</span> <span class="pl-s1">pd</span>.<span class="pl-c1">Timestamp</span>(<span class="pl-s">"2023-11-01"</span>), <span class="pl-s1">pd</span>.<span class="pl-c1">Timestamp</span>(<span class="pl-s">"2025-08-31"</span>)
<span class="pl-s1">rolled_win</span> <span class="pl-c1">=</span> <span class="pl-s1">rolled</span>.<span class="pl-c1">loc</span>[(<span class="pl-s1">rolled</span>.<span class="pl-c1">index</span> <span class="pl-c1">&gt;=</span> <span class="pl-s1">start</span>) <span class="pl-c1">&amp;</span> (<span class="pl-s1">rolled</span>.<span class="pl-c1">index</span> <span class="pl-c1">&lt;=</span> <span class="pl-s1">end</span>)]

<span class="pl-s1">fig</span>, <span class="pl-s1">ax</span> <span class="pl-c1">=</span> <span class="pl-s1">plt</span>.<span class="pl-c1">subplots</span>(<span class="pl-s1">figsize</span><span class="pl-c1">=</span>(<span class="pl-c1">12</span>, <span class="pl-c1">6</span>))
<span class="pl-k">for</span> <span class="pl-s1">col</span> <span class="pl-c1">in</span> <span class="pl-s1">order</span>:
    <span class="pl-k">if</span> <span class="pl-s1">col</span> <span class="pl-c1">in</span> <span class="pl-s1">rolled_win</span>.<span class="pl-c1">columns</span>:
        <span class="pl-s1">ax</span>.<span class="pl-c1">plot</span>(<span class="pl-s1">rolled_win</span>.<span class="pl-c1">index</span>, <span class="pl-s1">rolled_win</span>[<span class="pl-s1">col</span>], <span class="pl-s1">label</span><span class="pl-c1">=</span><span class="pl-s1">col</span>, <span class="pl-s1">linewidth</span><span class="pl-c1">=</span><span class="pl-c1">2</span>)

<span class="pl-s1">ax</span>.<span class="pl-c1">set_title</span>(<span class="pl-s">"AI adoption (last two weeks) — 6‑survey rolling average"</span>, <span class="pl-s1">pad</span><span class="pl-c1">=</span><span class="pl-c1">16</span>)
<span class="pl-s1">ax</span>.<span class="pl-c1">yaxis</span>.<span class="pl-c1">set_major_formatter</span>(<span class="pl-en">PercentFormatter</span>(<span class="pl-c1">100</span>))
<span class="pl-s1">ax</span>.<span class="pl-c1">set_ylabel</span>(<span class="pl-s">"%"</span>)
<span class="pl-s1">ax</span>.<span class="pl-c1">set_xlabel</span>(<span class="pl-s">""</span>)
<span class="pl-s1">ax</span>.<span class="pl-c1">grid</span>(<span class="pl-c1">True</span>, <span class="pl-s1">alpha</span><span class="pl-c1">=</span><span class="pl-c1">0.25</span>, <span class="pl-s1">linestyle</span><span class="pl-c1">=</span><span class="pl-s">"--"</span>)
<span class="pl-s1">ax</span>.<span class="pl-c1">legend</span>(<span class="pl-s1">title</span><span class="pl-c1">=</span><span class="pl-c1">None</span>, <span class="pl-s1">loc</span><span class="pl-c1">=</span><span class="pl-s">"upper left"</span>, <span class="pl-s1">ncols</span><span class="pl-c1">=</span><span class="pl-c1">2</span>, <span class="pl-s1">frameon</span><span class="pl-c1">=</span><span class="pl-c1">False</span>)
<span class="pl-s1">plt</span>.<span class="pl-c1">tight_layout</span>()

<span class="pl-s1">png_path</span> <span class="pl-c1">=</span> <span class="pl-s">"/mnt/data/ai_adoption_rolling6_by_firm_size.png"</span>
<span class="pl-s1">svg_path</span> <span class="pl-c1">=</span> <span class="pl-s">"/mnt/data/ai_adoption_rolling6_by_firm_size.svg"</span>
<span class="pl-s1">plt</span>.<span class="pl-c1">savefig</span>(<span class="pl-s1">png_path</span>, <span class="pl-s1">dpi</span><span class="pl-c1">=</span><span class="pl-c1">200</span>, <span class="pl-s1">bbox_inches</span><span class="pl-c1">=</span><span class="pl-s">"tight"</span>)
<span class="pl-s1">plt</span>.<span class="pl-c1">savefig</span>(<span class="pl-s1">svg_path</span>, <span class="pl-s1">bbox_inches</span><span class="pl-c1">=</span><span class="pl-s">"tight"</span>)</pre>
<p>I like how it generated <a href="https://static.simonwillison.net/static/2025/ai_adoption_rolling6_by_firm_size.svg">an SVG version</a> of the chart without me even asking for it.</p>
<p>You can access <a href="https://chatgpt.com/share/68bf48cf-0e70-8006-a045-96fa8e7ddfc1">the ChatGPT transcript</a> to see full details of everything it did.</p>
<h4 id="rendering-that-chart-client-side-using-pyodide">Rendering that chart client-side using Pyodide</h4>
<p>I had one more challenge to try out. Could I render that same chart entirely in the browser using <a href="https://pyodide.org/en/stable/">Pyodide</a>, which can execute both Pandas and Matplotlib?</p>
<p>I fired up a new ChatGPT GPT-5 session and prompted:</p>
<blockquote>
<p><code>Build a canvas that loads Pyodide and uses it to render an example bar chart with pandas and matplotlib and then displays that on the page</code></p>
</blockquote>
<p>My goal here was simply to see if I could get a proof of concept of a chart rendered, ideally using the Canvas feature of ChatGPT. Canvas is OpenAI's version of Claude Artifacts, which lets the model write and then execute HTML and JavaScript directly in the ChatGPT interface.</p>
<p>It worked! Here's <a href="https://chatgpt.com/c/68bf2993-ca94-832a-a95e-fb225911c0a6">the transcript</a> and here's <a href="https://tools.simonwillison.net/pyodide-bar-chart">what it built me</a>, exported  to my <a href="https://tools.simonwillison.net/">tools.simonwillison.net</a> GitHub Pages site (<a href="https://github.com/simonw/tools/blob/main/pyodide-bar-chart.html">source code here</a>).</p>
<p><img src="https://static.simonwillison.net/static/2025/pyodide-matplotlib.jpg" alt="Screenshot of a web application demonstrating Pyodide integration. Header reads &quot;Pyodide + pandas + matplotlib — Bar Chart&quot; with subtitle &quot;This page loads Pyodide in the browser, uses pandas to prep some data, renders a bar chart with matplotlib, and displays it below — all client-side.&quot; Left panel shows terminal output: &quot;Ready&quot;, &quot;# Python environment ready&quot;, &quot;• pandas 2.2.0&quot;, &quot;• numpy 1.26.4&quot;, &quot;• matplotlib 3.5.2&quot;, &quot;Running chart code...&quot;, &quot;Done. Chart updated.&quot; with &quot;Re-run demo&quot; and &quot;Show Python&quot; buttons. Footer note: &quot;CDN: pyodide, pandas, numpy, matplotlib are fetched on demand. First run may take a few seconds.&quot; Right panel displays a bar chart titled &quot;Example Bar Chart (pandas + matplotlib in Pyodide)&quot; showing blue bars for months Jan through Jun with values approximately: Jan(125), Feb(130), Mar(80), Apr(85), May(85), Jun(120). Y-axis labeled &quot;Streams&quot; ranges 0-120, X-axis labeled &quot;Month&quot;." style="max-width: 100%;" /></p>
<p>I've now proven to myself that I can render those Python charts directly in the browser. Next step: recreate the Apollo chart.</p>
<p>I knew it would need a way to load the spreadsheet that was CORS-enabled. I uploaded my copy to my <code>/static/cors-allow/2025/...</code> directory (configured in S3 to serve CORS headers), pasted in the finished plotting code from earlier and told ChatGPT:</p>
<blockquote>
<p><code>Now update it to have less explanatory text and a less exciting design (black on white is fine) and run the equivalent of this:</code></p>
<p>(... pasted in Python code from earlier ...)</p>
<p><code>Load the XLSX sheet from https://static.simonwillison.net/static/cors-allow/2025/Employment-Size-Class-Sep-2025.xlsx</code></p>
</blockquote>
<p>It didn't quite work - I got an error about <code>openpyxl</code> which I manually researched the fix for and prompted:</p>
<blockquote>
<p><code>Use await micropip.install("openpyxl") to install openpyxl - instead of using loadPackage</code></p>
</blockquote>
<p>I had to paste in another error message:</p>
<blockquote>
<p><code>zipfile.BadZipFile: File is not a zip file</code></p>
</blockquote>
<p>Then one about a <code>SyntaxError: unmatched ')'</code> and a <code>TypeError: Legend.__init__() got an unexpected keyword argument 'ncols'</code> - copying and pasting error messages remains a frustrating but necessary part of the vibe-coding loop.</p>
<p>... but with those fixes in place, the resulting code worked! Visit <a href="https://tools.simonwillison.net/ai-adoption">tools.simonwillison.net/ai-adoption</a> to see the final result:</p>
<p><img src="https://static.simonwillison.net/static/2025/recreated-chart-pyodide.jpg" alt="Web page. Title is AI adoption - 6-survey rolling average. Has a Run, Downlaed PNG, Downlaod SVG button. Panel on the left says Loading Python... Fetcing packages numpy, pandas, matplotlib. Installing openpyxl via micropop... ready. Running. Done. Right hand panel shows the rendered chart." style="max-width: 100%;" /></p>
<p>Here's the code for that page, <a href="https://github.com/simonw/tools/blob/main/ai-adoption.html">170 lines</a> all-in of HTML, CSS, JavaScript and Python.</p>
<h4 id="what-i-ve-learned-from-this">What I've learned from this</h4>
<p>This was another of those curiosity-inspired investigations that turned into a whole set of useful lessons.</p>
<ul>
<li>GPT-5 is great at tracking down US Census data, no matter how difficult their site is to understand if you don't work with their data often</li>
<li>It can do a very good job of turning data + a screenshot of a chart into a recreation of that chart using code interpreter, Pandas and matplotlib</li>
<li>Running Python + matplotlib in a browser via Pyodide is very easy and only takes a few dozen lines of code</li>
<li>Fetching an XLSX sheet into Pyodide is only a small extra step using <code>pyfetch</code> and <code>openpyxl</code>:
<pre style="margin-top: 0.5em"><span class="pl-k">import</span> <span class="pl-s1">micropip</span>
<span class="pl-k">await</span> <span class="pl-s1">micropip</span>.<span class="pl-c1">install</span>(<span class="pl-s">"openpyxl"</span>)
<span class="pl-k">from</span> <span class="pl-s1">pyodide</span>.<span class="pl-s1">http</span> <span class="pl-k">import</span> <span class="pl-s1">pyfetch</span>
<span class="pl-s1">resp_fetch</span> <span class="pl-c1">=</span> <span class="pl-k">await</span> <span class="pl-en">pyfetch</span>(<span class="pl-c1">URL</span>)
<span class="pl-s1">wb_bytes</span> <span class="pl-c1">=</span> <span class="pl-k">await</span> <span class="pl-s1">resp_fetch</span>.<span class="pl-c1">bytes</span>()
<span class="pl-s1">xf</span> <span class="pl-c1">=</span> <span class="pl-s1">pd</span>.<span class="pl-c1">ExcelFile</span>(<span class="pl-s1">io</span>.<span class="pl-c1">BytesIO</span>(<span class="pl-s1">wb_bytes</span>), <span class="pl-s1">engine</span><span class="pl-c1">=</span><span class="pl-s">'openpyxl'</span>)</pre>
</li>
<li>Another new-to-me pattern: you can render an image to the DOM from Pyodide code <a href="https://github.com/simonw/tools/blob/cf26ed8a6f243159bdc90a3d88f818261732103f/ai-adoption.html#L124">like this</a>:
<pre style="margin-top: 0.5em"><span class="pl-k">from</span> <span class="pl-s1">js</span> <span class="pl-k">import</span> <span class="pl-s1">document</span>
<span class="pl-s1">document</span>.<span class="pl-c1">getElementById</span>(<span class="pl-s">'plot'</span>).<span class="pl-c1">src</span> <span class="pl-c1">=</span> <span class="pl-s">'data:image/png;base64,'</span> <span class="pl-c1">+</span> <span class="pl-s1">img_b64</span></pre>
</li>
</ul>
<p>I will most definitely be using these techniques again in future.</p>

<p><strong>Update</strong>: Coincidentally Claude released their own upgraded equivalent to ChatGPT Code Interpreter later on the day that I published this story, so I <a href="https://simonwillison.net/2025/Sep/9/claude-code-interpreter/#something-much-harder-recreating-the-ai-adoption-chart">ran the same chart recreation experiment</a> against Claude Sonnet 4 to see how it compared.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/census">census</a>, <a href="https://simonwillison.net/tags/data-journalism">data-journalism</a>, <a href="https://simonwillison.net/tags/javascript">javascript</a>, <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/tools">tools</a>, <a href="https://simonwillison.net/tags/visualization">visualization</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/pyodide">pyodide</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/chatgpt">chatgpt</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/code-interpreter">code-interpreter</a>, <a href="https://simonwillison.net/tags/llm-reasoning">llm-reasoning</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a>, <a href="https://simonwillison.net/tags/ai-assisted-search">ai-assisted-search</a>, <a href="https://simonwillison.net/tags/gpt-5">gpt-5</a></p>]]></description><pubDate>Tue, 9 Sep 2025 06:47:49 +0000</pubDate></item><item><title>Anthropic status: Model output quality</title><link>https://simonwillison.net/2025/Sep/9/anthropic-model-output-quality/#atom-everything</link><description><![CDATA[<p><strong><a href="https://status.anthropic.com/incidents/72f99lh1cj2c">Anthropic status: Model output quality</a></strong></p>
Anthropic <a href="https://simonwillison.net/2025/Aug/30/claude-degraded-quality/">previously reported</a> model serving bugs that affected Claude Opus 4 and 4.1 for 56.5 hours. They've now fixed additional bugs affecting "a small percentage" of Sonnet 4 requests for almost a month, plus a less long-lived Haiku 3.5 issue:</p>
<blockquote>
<p>Resolved issue 1 - A small percentage of Claude Sonnet 4 requests experienced degraded output quality due to a bug from Aug 5-Sep 4, with the impact increasing from Aug 29-Sep 4. A fix has been rolled out and this incident has been resolved. </p>
<p>Resolved issue 2 - A separate bug affected output quality for some Claude Haiku 3.5 and Claude Sonnet 4 requests from Aug 26-Sep 5. A fix has been rolled out and this incident has been resolved.</p>
</blockquote>
<p>They directly address accusations that these stem from deliberate attempts to save money on serving models:</p>
<blockquote>
<p>Importantly, we never intentionally degrade model quality as a result of demand or other factors, and the issues mentioned above stem from unrelated bugs.</p>
</blockquote>
<p>The timing of these issues is really unfortunate, corresponding with the rollout of GPT-5 which I see as the non-Anthropic model to feel truly competitive with Claude for writing code since their release of Claude 3.5 back in  June last year.

    <p><small></small>Via <a href="https://twitter.com/theo/status/1965216210729259485">@theo</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/claude-4">claude-4</a>, <a href="https://simonwillison.net/tags/gpt-5">gpt-5</a></p>]]></description><pubDate>Tue, 9 Sep 2025 06:28:21 +0000</pubDate></item><item><title>Quoting TheSoftwareGuy</title><link>https://simonwillison.net/2025/Sep/8/thesoftwareguy/#atom-everything</link><description><![CDATA[<blockquote cite="https://news.ycombinator.com/item?id=45169624#45172524"><p>Having worked inside AWS I can tell you one big reason [that they don't describe their internals] is the attitude/fear that anything we put in out public docs may end up getting relied on by customers. If customers rely on the implementation to work in a specific way, then changing that detail requires a LOT more work to prevent breaking customer's workloads. If it is even possible at that point.</p></blockquote>
<p class="cite">&mdash; <a href="https://news.ycombinator.com/item?id=45169624#45172524">TheSoftwareGuy</a>, comment on Hacker News</p>

    <p>Tags: <a href="https://simonwillison.net/tags/aws">aws</a></p>]]></description><pubDate>Mon, 8 Sep 2025 23:23:43 +0000</pubDate></item><item><title>Load Llama-3.2 WebGPU in your browser from a local folder</title><link>https://simonwillison.net/2025/Sep/8/webgpu-local-folder/#atom-everything</link><description><![CDATA[<p><strong><a href="https://static.simonwillison.net/static/2025/llama-3.2-webgpu/">Load Llama-3.2 WebGPU in your browser from a local folder</a></strong></p>
Inspired by <a href="https://news.ycombinator.com/item?id=45168953#45169054">a comment</a> on Hacker News I decided to see if it was possible to modify the <a href="https://github.com/huggingface/transformers.js-examples/tree/main/llama-3.2-webgpu">transformers.js-examples/tree/main/llama-3.2-webgpu</a> Llama 3.2 chat demo (<a href="https://huggingface.co/spaces/webml-community/llama-3.2-webgpu">online here</a>, I <a href="https://simonwillison.net/2024/Sep/30/llama-32-webgpu/">wrote about it last November</a>) to add an option to open a local model file directly from a folder on disk, rather than waiting for it to download over the network.</p>
<p>I posed the problem to OpenAI's GPT-5-enabled Codex CLI like this:</p>
<pre><code>git clone https://github.com/huggingface/transformers.js-examples
cd transformers.js-examples/llama-3.2-webgpu
codex
</code></pre>
<p>Then this prompt:</p>
<blockquote>
<p><code>Modify this application such that it offers the user a file browse button for selecting their own local copy of the model file instead of loading it over the network. Provide a "download model" option too.</code></p>
</blockquote>
<p>Codex churned away for several minutes, even running commands like <code>curl -sL https://raw.githubusercontent.com/huggingface/transformers.js/main/src/models.js | sed -n '1,200p'</code> to inspect the source code of the underlying Transformers.js library.</p>
<p>After four prompts total (<a href="https://gist.github.com/simonw/3c46c9e609f6ee77367a760b5ca01bd2?permalink_comment_id=5751814#gistcomment-5751814">shown here</a>) it built something which worked!</p>
<p>To try it out you'll need your own local copy of the Llama 3.2 ONNX model. You can get that (a ~1.2GB) download) like so:</p>
<pre><code>git lfs install
git clone https://huggingface.co/onnx-community/Llama-3.2-1B-Instruct-q4f16
</code></pre>
<p>Then visit my <a href="https://static.simonwillison.net/static/2025/llama-3.2-webgpu/">llama-3.2-webgpu</a> page in Chrome or Firefox Nightly (since WebGPU is required), click "Browse folder", select that folder you just cloned, agree to the "Upload" confirmation (confusing since nothing is uploaded from your browser, the model file is opened locally on your machine) and click "Load local model".</p>
<p>Here's an animated demo (recorded in real-time, I didn't speed this up):</p>
<p><img alt="GIF. I follow the setup instructions, clicking to load a local model and browsing to the correct folder. Once loaded the model shows a chat interface, I run the example about time management which returns tokens at about 10/second." src="https://static.simonwillison.net/static/2025/webgpu-llama-demo-small.gif" /></p>
<p>I pushed <a href="https://github.com/simonw/transformers.js-examples/commit/cdebf4128c6e30414d437affd4b13b6c9c79421d">a branch with those changes here</a>. The next step would be to modify this to support other models in addition to the Llama 3.2 demo, but I'm pleased to have got to this proof of concept with so little work beyond throwing some prompts at Codex to see if it could figure it out.</p>
<p>According to the Codex <code>/status</code> command <a href="https://gist.github.com/simonw/3c46c9e609f6ee77367a760b5ca01bd2?permalink_comment_id=5751807#gistcomment-5751807">this used</a> 169,818 input tokens, 17,112 output tokens and 1,176,320 cached input tokens. At current GPT-5 token pricing ($1.25/million input, $0.125/million cached input, $10/million output) that would cost 53.942 cents, but Codex CLI hooks into my existing $20/month ChatGPT Plus plan so this was bundled into that.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=45168953#45173297">My Hacker News comment</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/javascript">javascript</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llama">llama</a>, <a href="https://simonwillison.net/tags/local-llms">local-llms</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/transformers-js">transformers-js</a>, <a href="https://simonwillison.net/tags/webgpu">webgpu</a>, <a href="https://simonwillison.net/tags/llm-pricing">llm-pricing</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a>, <a href="https://simonwillison.net/tags/gpt-5">gpt-5</a>, <a href="https://simonwillison.net/tags/codex-cli">codex-cli</a></p>]]></description><pubDate>Mon, 8 Sep 2025 20:53:52 +0000</pubDate></item><item><title>Quoting James Luan</title><link>https://simonwillison.net/2025/Sep/8/james-luan/#atom-everything</link><description><![CDATA[<blockquote cite="https://zilliz.com/blog/will-amazon-s3-vectors-kill-vector-databases-or-save-them"><p>I recently spoke with the CTO of a popular AI note-taking app who told me something surprising: they spend <strong><em>twice</em></strong> <em>as much</em> on vector search as they do on OpenAI API calls. Think about that for a second. Running the retrieval layer costs them more than paying for the LLM itself.</p></blockquote>
<p class="cite">&mdash; <a href="https://zilliz.com/blog/will-amazon-s3-vectors-kill-vector-databases-or-save-them">James Luan</a>, Engineering architect of Milvus</p>

    <p>Tags: <a href="https://simonwillison.net/tags/vector-search">vector-search</a>, <a href="https://simonwillison.net/tags/embeddings">embeddings</a></p>]]></description><pubDate>Mon, 8 Sep 2025 16:24:24 +0000</pubDate></item><item><title>Is the LLM response wrong, or have you just failed to iterate it?</title><link>https://simonwillison.net/2025/Sep/7/is-the-llm-response-wrong-or-have-you-just-failed-to-iterate-it/#atom-everything</link><description><![CDATA[<p><strong><a href="https://mikecaulfield.substack.com/p/is-the-llm-response-wrong-or-have">Is the LLM response wrong, or have you just failed to iterate it?</a></strong></p>
More from Mike Caulfield (see also <a href="https://simonwillison.net/2025/Sep/7/the-sift-method/">the SIFT method</a>). He starts with a <em>fantastic</em> example of Google's <a href="https://simonwillison.net/2025/Sep/7/ai-mode/">AI mode</a> usually correctly handling a common piece of misinformation but occasionally falling for it (the curse of non-deterministic systems), then shows an example if what he calls a "sorting prompt" as a follow-up:</p>
<blockquote>
<p>What is the evidence for and against this being a real photo of Shirley Slade?</p>
</blockquote>
<p>The response starts with a non-committal "there is compelling evidence for and against...", then by the end has firmly convinced itself that the photo is indeed a fake. It reads like a fact-checking variant of "think step by step".</p>
<p>Mike neatly describes a problem I've also observed recently where "hallucination" is frequently mis-applied as meaning any time a model makes a mistake:</p>
<blockquote>
<p>The term hallucination has become nearly worthless in the LLM discourse. It initially described a very weird, mostly non-humanlike behavior where LLMs would make up things out of whole cloth that did not seem to exist as claims referenced any known source material or claims inferable from any known source material. Hallucinations as stuff made up out of nothing. Subsequently people began calling any error or imperfect summary a hallucination, rendering the term worthless.</p>
</blockquote>
<p>In this example is the initial incorrect answers were not hallucinations: they correctly summarized online content that contained misinformation. The trick then is to encourage the model to look further, using "sorting prompts" like these:</p>
<blockquote>
<ul>
<li>Facts and misconceptions and hype about what I posted</li>
<li>What is the evidence for and against the claim I posted</li>
<li>Look at the most recent information on this issue, summarize how it shifts the analysis (if at all), and provide link to the latest info</li>
</ul>
</blockquote>
<p>I appreciated this closing footnote:</p>
<blockquote>
<p>Should platforms have more features to nudge users to this sort of iteration? Yes. They should. Getting people to iterate investigation rather than argue with LLMs would be a good first step out of this mess that the chatbot model has created.</p>
</blockquote>

    <p><small></small>Via <a href="https://bsky.app/profile/mikecaulfield.bsky.social/post/3lya2nv7xi226">@mikecaulfield.bsky.social</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/ai-assisted-search">ai-assisted-search</a>, <a href="https://simonwillison.net/tags/hallucinations">hallucinations</a>, <a href="https://simonwillison.net/tags/digital-literacy">digital-literacy</a></p>]]></description><pubDate>Sun, 7 Sep 2025 21:45:04 +0000</pubDate></item><item><title>Quoting Anil Dash</title><link>https://simonwillison.net/2025/Sep/7/anil-dash/#atom-everything</link><description><![CDATA[<blockquote cite="https://bsky.app/profile/anildash.com/post/3lybkmj7ast2c"><p>I agree with the intellectual substance of virtually every common critique of AI. And it's very clear that turning those critiques into a competition about who can frame them in the most scathing way online has done <em>zero</em> to slow down adoption, even if much of that is due to default bundling.</p>
<p>At what point are folks going to try literally any other tactic than condescending rants? Does it matter that LLM apps are at the top of virtually every app store nearly every day because individual people are choosing to download them, and the criticism hasn't been effective in slowing that?</p></blockquote>
<p class="cite">&mdash; <a href="https://bsky.app/profile/anildash.com/post/3lybkmj7ast2c">Anil Dash</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/anil-dash">anil-dash</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Sun, 7 Sep 2025 21:32:09 +0000</pubDate></item><item><title>The SIFT method</title><link>https://simonwillison.net/2025/Sep/7/the-sift-method/#atom-everything</link><description><![CDATA[<p><strong><a href="https://guides.lib.uchicago.edu/c.php?g=1241077&amp;p=9082322">The SIFT method</a></strong></p>
The SIFT method is "an evaluation strategy developed by digital literacy expert, Mike Caulfield, to help determine whether online content can be trusted for credible or reliable sources of information."</p>
<p>This looks <em>extremely</em> useful as a framework for helping people more effectively consume information online (increasingly gathered with <a href="https://simonwillison.net/tags/ai-assisted-search/">the help of LLMs</a>).</p>
<ul>
<li><strong>Stop</strong>. "Be aware of your emotional response to the headline or information in the article" to protect against clickbait, and don't read further or share until you've applied the other three steps.</li>
<li><strong>Investigate the Source</strong>. Apply <a href="https://pressbooks.pub/webliteracy/chapter/what-reading-laterally-means/">lateral reading</a>, checking what others say about the source rather than just trusting their "about" page.</li>
<li><strong>Find Better Coverage</strong>. "Use lateral reading to see if you can find other sources corroborating the same information or disputing it" and consult trusted fact checkers if necessary.</li>
<li><strong>Trace Claims, Quotes, and Media to their Original Context</strong>. Try to find the original report or referenced material to learn more and check it isn't being represented out of context.</li>
</ul>
<p>This framework really resonates with me: it formally captures and improves on a bunch of informal techniques I've tried to apply in my own work.

    <p><small></small>Via <a href="https://bsky.app/profile/anildash.com/post/3lyavuu6ku22r">@anildash.com</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/blogging">blogging</a>, <a href="https://simonwillison.net/tags/research">research</a>, <a href="https://simonwillison.net/tags/ai-assisted-search">ai-assisted-search</a>, <a href="https://simonwillison.net/tags/digital-literacy">digital-literacy</a></p>]]></description><pubDate>Sun, 7 Sep 2025 20:51:31 +0000</pubDate></item><item><title>AI mode is good, actually</title><link>https://simonwillison.net/2025/Sep/7/ai-mode/#atom-everything</link><description><![CDATA[<p>When I wrote about <a href="https://simonwillison.net/2025/Sep/6/research-goblin/">how good ChatGPT with GPT-5</a> is at search yesterday I nearly added a note about how comparatively disappointing Google's efforts around this are.</p>
<p>I'm glad I left that out, because it turns out Google's new "AI mode" is genuinely really good! It feels  very similar to GPT-5 search but returns results <em>much</em> faster.</p>
<p><strong><a href="https://www.google.com/ai">www.google.com/ai</a></strong> (not available in the EU, as I found out this morning since I'm staying in France for a few days.)</p>
<p>Here's <a href="https://share.google/aimode/SZNHD7Owi0Wv9d3Ci">what I got</a> for the following question:</p>
<blockquote>
<p>Anthropic but lots of physical books and cut them up and scan them for training data. Do any other AI labs do the same thing?</p>
</blockquote>
<p><img alt="Screenshot of search results page with navigation tabs &quot;AI Mode&quot; and more.  Main search query reads &quot;Anthropic but lots of physical books and cut them up and scan them for training data. Do any other AI labs do the same thing?&quot; Results show &quot;11 sites&quot; and include text: &quot;In 2024, Anthropic paid millions of dollars to buy physical books in bulk, which they then cut up, scanned, and discarded to train their AI models. This method was a major component of a copyright lawsuit settlement that Anthropic paid in September 2025. While Anthropic took this destructive, large-scale approach, other major AI labs have taken different routes to acquire book data.&quot; Followed by &quot;Here's how other AI companies have sourced book data:&quot; with bullet points for &quot;OpenAI and Microsoft: In June 2025, the companies announced a partnership with Harvard University libraries to train AI models on digitized, public-domain books. This non-destructive method provides high-quality text while preserving the original physical copies.&quot; &quot;Meta: Meta has been accused of relying on large-scale piracy to build its datasets for models like LLaMA. In 2025, a search tool revealed that Meta had copied more than 7.5 million books from the pirate site LibGen for its AI systems.&quot; &quot;Google: Decades before the current AI boom, Google pioneered non-destructive scanning methods for its Google Books project. This method became a successful legal precedent for digitizing copyrighted works for internal purposes. In 2024, Anthropic hired the former head of this project, Tom Turvey, to help with its own book-acquisition strategy." src="https://static.simonwillison.net/static/2025/ai-mode-books.jpg" /></p>
<p>I'll be honest: I hadn't spent much time with AI mode for a couple of reasons:</p>
<ul>
<li>My expectations of "AI mode" were extremely low based on my terrible experience of "AI overviews"</li>
<li>The name "AI mode" is so generic!</li>
</ul>
<p>Based on some initial experiments I'm impressed - Google finally seem to be taking full advantage of their search infrastructure for building out truly great AI-assisted search.</p>
<p>I do have one disappointment: AI mode will tell you that it's "running 5 searches" but it won't tell you what those searches are! Seeing the searches that were run is really important for me in evaluating the likely quality of the end results. I've had the same problem with Google's Gemini app in the past - the lack of transparency as to what it's doing really damages my trust.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/gemini">gemini</a>, <a href="https://simonwillison.net/tags/google">google</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/search">search</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-search">ai-assisted-search</a></p>]]></description><pubDate>Sun, 7 Sep 2025 10:08:31 +0000</pubDate></item><item><title>GPT-5 Thinking in ChatGPT (aka Research Goblin) is shockingly good at search</title><link>https://simonwillison.net/2025/Sep/6/research-goblin/#atom-everything</link><description><![CDATA[<p>"Don't use chatbots as search engines" was great advice for several years... until it wasn't.</p>
<p>I wrote about how good OpenAI's o3 was at using its Bing-backed search tool <a href="https://simonwillison.net/2025/Apr/21/ai-assisted-search/">back in April</a>. GPT-5 feels even better.</p>
<p>I've started calling it my <strong>Research Goblin</strong>. I can assign a task to it, no matter how trivial or complex, and it will do an often unreasonable amount of work to search the internet and figure out an answer.</p>
<p>This is excellent for satisfying curiosity, and occasionally useful for more important endeavors as well.</p>
<p>I always run my searches by selecting the "GPT-5 Thinking" model from the model picker - in my experience this leads to far more comprehensive (albeit much slower) results.</p>
<p>Here are some examples from just the last couple of days. Every single one of them was run on my phone, usually while I was doing something else. Most of them were dictated using the iPhone voice keyboard, which I find faster than typing. Plus, it's fun to talk to my Research Goblin.</p>

<ul>
  <li><a href="https://simonwillison.net/2025/Sep/6/research-goblin/#bouncy-travelators">Bouncy travelators</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/6/research-goblin/#identify-this-building">Identify this building</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/6/research-goblin/#starbucks-uk-cake-pops">Starbucks UK cake pops</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/6/research-goblin/#britannica-to-seed-wikipedia">Britannica to seed Wikipedia</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/6/research-goblin/#official-name-for-the-university-of-cambridge">Official name for the University of Cambridge</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/6/research-goblin/#history-of-the-caverns-in-exeter-quay">History of the caverns in Exeter quay</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/6/research-goblin/#aldi-vs-lidl">Aldi vs Lidl</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/6/research-goblin/#ai-labs-scanning-books-for-training-data">AI labs scanning books for training data</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/6/research-goblin/#gpt-5-for-search-feels-competent">GPT-5 for search feels competent</a></li>
  <li><a href="https://simonwillison.net/2025/Sep/6/research-goblin/#tips-for-using-search-in-chatgpt">Tips for using search in ChatGPT</a></li>
</ul>

<h4 id="bouncy-travelators">Bouncy travelators</h4>
<blockquote>
<p>They used to be rubber bouncy travelators at Heathrow and they were really fun, have all been replaced by metal ones now and if so, when did that happen?</p>
</blockquote>
<p>I was traveling through Heathrow airport pondering what had happened to the fun bouncy rubber travelators.</p>
<p><a href="https://chatgpt.com/share/68bc2d98-9aac-8006-98b9-1424d98290f8">Here's what I got</a>. Research Goblin narrowed it down to some time between 2014-2018 but, more importantly, found me this <a href="https://www.sfchronicle.com/totalsf/article/sfo-bouncy-moving-walkway-airport-19845449.php">delightful 2024 article</a> by Peter Hartlaub in the San Francisco Chronicle with a history of the SFO bouncy walkways, now also sadly retired.</p>
<h4 id="identify-this-building">Identify this building</h4>
<blockquote>
<p><img src="https://static.simonwillison.net/static/2025/reading-building.jpg" alt="not a great photo of a building with a distinctive shaped roof" style="max-width: 100%;" /></p>
<p>Identify this building in reading</p>
</blockquote>
<p>This is a photo I snapped out of the window on the train. It <a href="https://chatgpt.com/share/68bc2e21-1d24-8006-b083-00b3233e1c67">thought for 1m4s</a> and correctly identified it as <a href="https://en.wikipedia.org/wiki/The_Blade,_Reading">The Blade</a>.</p>
<h2 id="starbucks-uk-cake-pops">Starbucks UK cake pops</h2>
<blockquote>
<p>Starbucks in the UK don't sell cake pops! Do a deep investigative dive</p>
</blockquote>
<p>The Starbucks in Exeter railway station didn't have cake pops, and the lady I asked didn't know what they were.</p>
<p><a href="https://chatgpt.com/share/68bc71b4-68f4-8006-b462-cf32f61e7ec3">Here's the result</a>. It turns out Starbucks did launch cake pops in the UK <a href="https://www.nationalworld.com/lifestyle/starbucks-cake-pops-launched-in-uk-on-new-autumn-menu-full-list-of-items-4284537">in September 2023</a> but they aren't available at all outlets, in particular the licensed travel locations such as the one at Exeter St Davids station.</p>
<p>I particularly enjoyed how it established definitive proof by consulting <a href="https://www.starbucks.co.uk/sites/starbucks-uk-pwa/files/2024-11/HOL24_UK_AllergenBook_CORE_FOOD_v02.LR_.pdf">the nutrition and allergen guide PDF</a> on starbucks.co.uk, which does indeed list both the Birthday Cake Pop (my favourite) and the Cookies and Cream one (apparently discontinued in the USA, at least <a href="https://www.reddit.com/r/starbucks/comments/1lp5chq/just_learned_today_the_cookies_cream_cake_pop_has/">according to r/starbucks</a>).</p>
<h4 id="britannica-to-seed-wikipedia">Britannica to seed Wikipedia</h4>
<blockquote>
<p>Someone on hacker News said:</p>
<p>&gt; I was looking at another thread about how Wikipedia was the best thing on the internet. But they only got the head start by taking copy of Encyclopedia Britannica and everything else</p>
<p>Find what they meant by that</p>
</blockquote>
<p><a href="https://chatgpt.com/share/68bc3062-5a68-8006-a12b-cf7196a130ae">The result</a>. It turns out Wikipedia did seed itself with content from the out-of-copyright 1911 Encyclopædia Britannica... but that project took place in 2006, five years after Wikipedia first launched in 2001.</p>
<p>I asked:</p>
<blockquote>
<p>What is the single best article I can link somebody to that explains the 1911 Britannica thing</p>
</blockquote>
<p>And it pointed me to <a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Encyclopaedia_Britannica">Wikipedia:WikiProject Encyclopaedia Britannica</a> which includes a detailed explanation and a link to <a href="https://en.m.wikipedia.org/wiki/Template:EB1911">the 13,000 pages</a> still tagged with the template from that project. I posted what I found <a href="https://news.ycombinator.com/item?id=45135061#45137109">in a comment</a>.</p>
<p>Notably (for me anyway) I didn't feel the need to disclose my use of ChatGPT in finding that information - at this point that feels a little like disclosing that I ran a Google search.</p>
<h4 id="official-name-for-the-university-of-cambridge">Official name for the University of Cambridge</h4>
<blockquote>
<p>What is the official legal name of the university of Cambridge?</p>
</blockquote>
<p><a href="https://simonwillison.net/2025/Sep/6/iancal/">Here's the context for that one</a>. It thought for 19 seconds - the thinking trace reveals it knew the answer but wanted to confirm it. <a href="https://chatgpt.com/share/68bc723d-a344-8006-9514-fb3ea2c2b43b">It answered</a>:</p>
<blockquote>
<p>“The Chancellor, Masters, and Scholars of the University of Cambridge.” <a href="https://www.cam.ac.uk/about-the-university/how-the-university-and-colleges-work/the-university-as-a-charity">University of Cambridge</a>, <a href="https://www.cambridge.org/legal">Cambridge University Press &amp; Assessment</a></p>
</blockquote>
<p>That first link gave me the citation I needed in order to be sure this was right.</p>
<p>Since this is my shortest example, here's a screenshot of the expanded "Thought for 19s" panel. I <em>always</em> expand the thoughts - seeing how it pulled together its answer is crucial for evaluating if the answer is likely to be useful or not.</p>
<p><img src="https://static.simonwillison.net/static/2025/cambridge-name.jpg" alt="Screenshot of an AI assistant's thought process and research. Header shows &quot;Thought for 19s&quot; followed by bullet points: &quot;The user asked for the official legal name of the University of Cambridge, and I think they mean the formal corporate name. It's probably 'The Chancellor, Masters, and Scholars of the University of Cambridge,' similar to Oxford's. I'm going to verify by checking the web for precise information.&quot; &quot;The official name of Cambridge might include the serial comma, like 'Masters, and Scholars.' There's a similar structure with Oxford, which uses a comma after 'Masters' too. The University is an exempt charity, so you can cross-check with legal documents like statutes or charity records. I'll double-check.&quot; Shows search results for &quot;official legal name of University of Cambridge&quot; from en.wikipedia.org, www.cam.ac.uk, projects.propublica.org and &quot;10 more&quot; sources. Continues with &quot;The University of Cambridge's official legal name, based on the University website and Cambridge University Press, is 'The Chancellor, Masters, and Scholars of the University of Cambridge,' with the serial comma after 'Masters.' It's clear from these authoritative sources, so I'll provide that straightforward answer, citing the University site.&quot; &quot;The official legal name of the University of Cambridge is 'The Chancellor, Masters, and Scholars of the University of Cambridge,' with the comma after Masters and the Oxford comma before 'and.' I'll answer concisely in one line and cite official sources to confirm. No need for extra UI elements here.&quot; Shows &quot;Done&quot; status. Final answer displays: &quot;The Chancellor, Masters, and Scholars of the University of Cambridge.&quot; attributed to University of Cambridge, Cambridge University Press &amp; Assessment, with &quot;Sources&quot; section at bottom." style="max-width: 100%;" /></p>
<h4 id="history-of-the-caverns-in-exeter-quay">History of the caverns in Exeter quay</h4>
<blockquote>
<p>Research On the waterfront restaurant in Exeter, is it dug into the cliffs somehow? History of the building, who built it, why and how</p>
</blockquote>
<p>We were out to dinner <a href="https://maps.app.goo.gl/xxvaPQiNWACtbq3H8">here</a> and noticed that the interior of the restaurant appeared to be a space dug into the cliff, which piqued my interest.</p>
<p>This was <a href="https://chatgpt.com/share/68bc32fb-d52c-8006-9259-0b984dc832b2">the ChatGPT session</a> that inspired the Research Goblin nickname. It just kept on digging!</p>
<p>The first reply took 2m40s and confirmed that yes, these quay buildings were carved into the red sandstone cliff <a href="https://www.exploredevon.info/activities/walk/exeter-quay/">in the 1820s-1830s</a>.</p>
<p>ChatGPT with GPT-5 really likes to suggest additional steps it can take. In this case:</p>
<blockquote>
<p>If you’d like, I can dig up the exact Historic England entry that covers the “Southern Warehouse” address and overlay it on a map of the vaults.</p>
</blockquote>
<p>I often say "yes" purely out of curiosity to see what it will do next, and the offer to "overlay it on a map" was irresistible, like how would it even do that?</p>
<p>It did a <em>ton</em> of extra searches, found latitude and longitude coordinates for the restaurant (from Wikimedia Commons) and the warehouse buildings (from National Heritage List for England via Wikipedia), showed me that data in a table and then used Python to render this image:</p>
<p><img src="https://static.simonwillison.net/static/2025/bad-chart.png" alt="Scatter plot titled &quot;On The Waterfront vs. Warehouse Vaults (Exeter Quay)&quot; with scientific notation &quot;+5.071e1&quot; in top left. Y-axis shows &quot;Latitude&quot; ranging from 0.0065 to 0.0090. X-axis shows &quot;Longitude&quot; ranging from -3.5310 to -3.5280. Three orange X markers plotted: &quot;Warehouse Vaults (6-11, The Quay) — HE 1223046&quot; at top center, &quot;On The Waterfront (4-9 Southern Warehouse)&quot; at middle left, and &quot;Warehouse Vaults (1-4, The Quay) — HE 1223045&quot; at bottom right" style="max-width: 100%;" /></p>
<p>Yeah, so that one is a bit of a miss!</p>
<p>But... the underlying research chain, as always, provided fascinating. I noticed that at one point it had searched for "Exeter Quay vaults map PDF", which inspired my next question:</p>
<blockquote>
<p>Did you find any PDF diagrams, showing the tunnels into the cliff</p>
</blockquote>
<p>2m20s later it had found <a href="https://planningdocs.exeter.gov.uk/servlets/direct/KgQ8hlUut3Q2r5wVMItggobi9P/2/844706/1/1/AS_PDF_FILE">a planning document</a> and <a href="https://www.turner-locker.co.uk/wp-content/uploads/2018/08/Cellar-1-The-Quay-TO-LET-2018-1.pdf">a letting brochure</a>, both PDFs, but noted that neither of them had the diagram I wanted. Evidently it actually "reads" the PDFs it finds as part of its activities, since it appears to have knowledge of the images contained within them as well as the text.</p>
<p>It concluded with this intriguing note:</p>
<blockquote>
<p>There’s strong evidence that a detailed plan/section <strong>does</strong> exist in the Exeter Archaeology report often cited by planners—<strong>“Archaeological, Historical and Conservation Study of the Exeter Canal Basin” (Exeter Archaeology Report 00.18, 2000)</strong>—but it isn’t online; it’s just cited in planning documents. You’d likely need to request it via the Devon Heritage Centre/Historic Environment Record.</p>
<p>If you want, I can keep hunting specialist archives for a digitized copy or a different report that includes the vault layouts.</p>
</blockquote>
<p>Obviously I said "Yes do that" and 2m24s later it chucked out a whole host of extra links, an apology that it still hadn't found a detailed plan diagram and, under the section heading "What I recommend next (actionable + exact ask)":</p>
<blockquote>
<p>If you’re happy, I can draft an email you can paste to the right archivists. [...]</p>
<blockquote>
<p>Subject: Exeter Quayside cliff vaults – request for plan/section drawings
Hello,</p>
<p>I’m researching the cliff-cut vaults behind The Quay (On The Waterfront / Kings Wharf).
Could you supply scans or copies of any <strong>plans/sections/diagrams</strong> of these vaults—especially those in <strong>Exeter Archaeology Report 00.18 (2000)</strong> (Archaeological, Historical and Conservation Study of the Exeter Canal Basin) and any <strong>EMAFU/Exeter Archaeology drawings</strong> for “The Quay” showing the <strong>vault/tunnel layouts cut into the red sandstone cliff</strong>?</p>
<p>For reference: NHLE list entries <strong>1223045 (1–4 Warehouse Vaults), 1223046 (6–11 Warehouse Vaults)</strong>, and <strong>1223114 (14–25 Vaults)</strong>.</p>
<p>Many thanks!</p>
</blockquote>
</blockquote>
<p>It's a good thing my Research Goblin doesn't have the ability to send email (which we already know is <a href="https://simonwillison.net/2025/May/31/snitchbench-with-llm/">very risky business</a>) or at this point I wouldn't be surprised if it started autonomously bothering archivists on my behalf!</p>
<h4 id="aldi-vs-lidl">Aldi vs Lidl</h4>
<blockquote>
<p>In UK supermarket terms compare lidl and aldi</p>
</blockquote>
<p>I saw an Aldi and a Lidl next to each other in Exeter and was curious as to how they compare. In this case my Research Goblin went <em>way</em> deeper than I had intended, thinking for 1m25s about market share and price surveys and loyalty schemes and <a href="https://chatgpt.com/share/68bc7913-7ed8-8006-bd3f-63e540470cd7">answering</a> with ten paragraphs of detailed analysis.</p>
<p>I replied:</p>
<blockquote>
<p>Yeah, I just wanted the basic vibes like is one fancier than the other?</p>
</blockquote>
<p>It thought for 6 seconds, didn't run any additional searches and replied:</p>
<blockquote>
<p>Short version: neither is “fancy” — both are no-frills discounters — but <strong>Lidl often feels a touch more polished</strong>, while <strong>Aldi feels a touch more bare-bones</strong>. [...]</p>
</blockquote>
<p>Since I was now enjoying having my Goblin express judgement over British supermarket chains I said:</p>
<blockquote>
<p>How do they rate in fancinews compared to all of the other major UK supermarket chains?</p>
</blockquote>
<p>(That was a typo for "fanciness" which it figured out.)</p>
<p>It thought for another 1m1s, ran 7 searches, consulted 27 URLs and gave me back a ranking that looked about right to me.</p>
<p><img src="https://static.simonwillison.net/static/2025/fancy-supermarkets.jpg" alt="ChatGPT. Prompt: How do they rate in fancinews compared to all of the other major UK supermarket chains? Thought for 1m 1s › Gotcha-assuming you mean &quot;fanciness.&quot; Here's the UK supermarket vibe-ladder (posh → no-frills): Super-posh: M&amp;S Food and Waitrose - premium feel, strong store appearance/quality scores; Waitrose is literally described as &quot;upmarket.&quot; Which? The Guardian Upper-mainstream: Sainsbury's, Ocado (online, but it sells M&amp;s Food since 2020, so the range skews premium). Which? The Guardian Mainstream: Tesco, Morrisons, Co-op - broad ranges, middle-of-the-road on experience/ appearance. Which? Value-forward: Asda, Iceland — price-led; Iceland's frozen focus, Asda's value reputation (not &quot;fancy,&quot; but popular). Time Out Worldwide Discounters (least fancy by design): Lidl (a touch more polished in-store) and Aldi (most stripped-back, no-frills). Both sit lower on &quot;store appearance/ service&quot; in consumer surveys, even while smashing value rankings. Which? +1 Net: on a &quot;posh&quot; scale, think M&amp;S/Waitrose at the top, Sainsbury's/Tesco middle, Asda/Iceland lower, and Lidl/Aldi the least fancy— by intent, not as a knock on quality or value." style="max-width: 100%;" /></p>
<h4 id="ai-labs-scanning-books-for-training-data">AI labs scanning books for training data</h4>
<blockquote>
<p>Anthropic bought lots of physical books and cut them up and scan them for training data. Do any other AI labs do the same thing?</p>
</blockquote>
<p>Relevant to <a href="https://simonwillison.net/2025/Sep/6/anthropic-settlement/">today's big story</a>. Research Goblin was <a href="https://chatgpt.com/share/68bc771c-c188-8006-a8e5-4b1624f5bdf0">unable to find</a> any news stories or other evidence that any labs other than Anthropic are engaged in large scale book scanning for training data. That's not to say it isn't happening, but it's happening very quietly if that's the case.</p>
<h4 id="gpt-5-for-search-feels-competent">GPT-5 for search feels competent</h4>
<p>The word that best describes how I feel about GPT-5 search is that it feels <strong>competent</strong>.</p>
<p>I've thrown all sorts of things at it over the last few weeks and it rarely disappoints me. It almost always does better than if I were to dedicate the same amount of time to manually searching myself, mainly because it's much faster at running searches and evaluating the results than I am.</p>
<p>I particularly love that it works so well on mobile. I used to reserve my deeper research sessions to a laptop where I could open up dozens of tabs. I'll still do that for higher stakes activities but I'm finding the scope of curiosity satisfaction I can perform on the go with just my phone has increased quite dramatically.</p>
<p>I've mostly stopped using OpenAI's Deep Research feature, because ChatGPT search now gives me the results I'm interested in far more quickly for most queries.</p>
<p>As a developer who builds software on LLMs I see ChatGPT search as the gold standard for what can be achieved using tool calling combined with chain-of-thought. Techniques like RAG are <em>massively</em> more effective if you can reframe them as several levels of tool calling with a carefully selected set of powerful search tools.</p>
<p>The way that search tool integrates with reasoning is key, because it allows GPT-5 to execute a search, reason about the results and then execute follow-up searches - all as part of that initial "thinking" process.</p>
<p>Anthropic call this ability <a href="https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#interleaved-thinking">interleaved thinking</a> and it's also <a href="https://platform.openai.com/docs/guides/reasoning#keeping-reasoning-items-in-context">supported by the OpenAI Responses API</a>.</p>
<h4 id="tips-for-using-search-in-chatgpt">Tips for using search in ChatGPT</h4>
<p>As with all things AI, GPT-5 search rewards intuition gathered through experience. Any time a curious thought pops into my head I try to catch it and throw it at my Research Goblin. If it's something I'm certain it won't be able to handle then even better! I can learn from watching it fail.</p>
<p>I've been trying out hints like "go deep" which seem to trigger a more thorough research job. I enjoy throwing those at shallow and unimportant questions like the UK Starbucks cake pops one just to see what happens!</p>
<p>You can throw questions at it which have a single, unambiguous answer - but I think questions which are broader and don't have a "correct" answer can be a lot more fun. The UK supermarket rankings above are a great example of that.</p>
<p>Since I love a questionable analogy for LLMs Research Goblin is... well, it's a goblin. It's very industrious, not quite human and not entirely trustworthy. You have to be able to outwit it if you want to keep it gainfully employed.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/bing">bing</a>, <a href="https://simonwillison.net/tags/search">search</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/llm-tool-use">llm-tool-use</a>, <a href="https://simonwillison.net/tags/llm-reasoning">llm-reasoning</a>, <a href="https://simonwillison.net/tags/ai-assisted-search">ai-assisted-search</a>, <a href="https://simonwillison.net/tags/gpt-5">gpt-5</a></p>]]></description><pubDate>Sat, 6 Sep 2025 19:31:57 +0000</pubDate></item><item><title>Quoting Jason Liu</title><link>https://simonwillison.net/2025/Sep/6/jason-liu/#atom-everything</link><description><![CDATA[<blockquote cite="https://twitter.com/jxnlco/status/1964050092312211636"><p>I am once again shocked at how much better image retrieval performance you can get if you embed highly opinionated summaries of an image, a summary that came out of a visual language model, than using CLIP embeddings themselves. If you tell the LLM that the summary is going to be embedded and used to do search downstream. I had one system go from 28% recall at 5 using CLIP to 75% recall at 5 using an LLM summary.</p></blockquote>
<p class="cite">&mdash; <a href="https://twitter.com/jxnlco/status/1964050092312211636">Jason Liu</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/vision-llms">vision-llms</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/embeddings">embeddings</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/jason-liu">jason-liu</a></p>]]></description><pubDate>Sat, 6 Sep 2025 17:20:27 +0000</pubDate></item><item><title>Kimi-K2-Instruct-0905</title><link>https://simonwillison.net/2025/Sep/6/kimi-k2-instruct-0905/#atom-everything</link><description><![CDATA[<p><strong><a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905">Kimi-K2-Instruct-0905</a></strong></p>
New not-quite-MIT licensed model from Chinese Moonshot AI, a follow-up to the highly regarded Kimi-K2 model they <a href="https://simonwillison.net/2025/Jul/11/kimi-k2/">released in July</a>.</p>
<p>This one is an incremental improvement - I've seen it referred to online as "Kimi K-2.1". It scores a little higher on a bunch of popular coding benchmarks, reflecting Moonshot's claim that it "demonstrates significant improvements in performance on public benchmarks and real-world coding agent tasks".</p>
<p>More importantly the context window size has been increased from 128,000 to 256,000 tokens.</p>
<p>Like its predecessor this is a <em>big</em> model - 1 trillion parameters in a mixture-of-experts configuration with 384 experts, 32B activated parameters and 8 selected experts per token.</p>
<p>I used <a href="https://console.groq.com/playground?model=moonshotai/kimi-k2-instruct-0905">Groq's playground tool</a> to try "Generate an SVG of a pelican riding a bicycle" and got <a href="https://gist.github.com/simonw/80f9fc8f888edc43e1f2a5170c95de3d">this result</a>, at a very healthy 445 tokens/second taking just under 2 seconds total:</p>
<p><img alt="This one is pretty good. Bicycle is mostly the right shape, the pelican is perched quite awkwardly on the saddle, it has quite a good attempt at the chain mechanism." src="https://static.simonwillison.net/static/2025/kimi-0905.png" />


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/groq">groq</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a>, <a href="https://simonwillison.net/tags/ai-in-china">ai-in-china</a>, <a href="https://simonwillison.net/tags/moonshot">moonshot</a></p>]]></description><pubDate>Sat, 6 Sep 2025 16:59:25 +0000</pubDate></item><item><title>Quoting IanCal</title><link>https://simonwillison.net/2025/Sep/6/iancal/#atom-everything</link><description><![CDATA[<blockquote cite="https://news.ycombinator.com/item?id=45135302#45135852"><p>RDF has the same problems as the SQL schemas with information scattered. What fields mean requires documentation.</p>
<p>There - they have a name on a person. What name? Given? Legal? Chosen? Preferred for this use case?</p>
<p>You only have one ID for Apple eh? Companies are complex to model, do you mean Apple just as someone would talk about it? The legal structure of entities that underpins all major companies, what part of it is referred to?</p>
<p>I spent a long time building identifiers for universities and companies (which was taken for <a href="https://ror.org/">ROR</a> later) and it was a nightmare to say what a university even was. What’s the name of Cambridge? It’s not “Cambridge University” or “The university of Cambridge” legally. But it also is the actual name as people use it. <em>[It's <a href="https://www.cam.ac.uk/about-the-university/how-the-university-and-colleges-work/the-university-as-a-charity">The Chancellor, Masters, and Scholars of the University of Cambridge</a>]</em></p>
<p>The university of Paris went from something like 13 institutes to maybe one to then a bunch more. Are companies locations at their headquarters? Which headquarters?</p>
<p>Someone will suggest modelling to solve this but here lies the biggest problem:</p>
<p>The correct modelling depends on <em>the questions you want to answer</em>.</p></blockquote>
<p class="cite">&mdash; <a href="https://news.ycombinator.com/item?id=45135302#45135852">IanCal</a>, on Hacker News, discussing RDF</p>

    <p>Tags: <a href="https://simonwillison.net/tags/metadata">metadata</a>, <a href="https://simonwillison.net/tags/sql">sql</a>, <a href="https://simonwillison.net/tags/hacker-news">hacker-news</a>, <a href="https://simonwillison.net/tags/rdf">rdf</a></p>]]></description><pubDate>Sat, 6 Sep 2025 06:41:49 +0000</pubDate></item><item><title>Why I think the $1.5 billion Anthropic class action settlement may count as a win for Anthropic</title><link>https://simonwillison.net/2025/Sep/6/anthropic-settlement/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.theverge.com/anthropic/773087/anthropic-to-pay-1-5-billion-to-authors-in-landmark-ai-settlement">Anthropic to pay $1.5 billion to authors in landmark AI settlement</a></strong></p>
I wrote about <a href="https://simonwillison.net/2025/Jun/24/anthropic-training/">the details of this case</a> when it was found that Anthropic's training on book content was fair use, but they needed to have purchased individual copies of the books first... and they had seeded their collection with pirated ebooks from Books3, PiLiMi and LibGen.</p>
<p>The remaining open question from that case was the penalty for pirating those 500,000 books. That question has now been resolved in a settlement:</p>
<blockquote>
<p>Anthropic has reached an agreement to pay “at least” a staggering $1.5 billion, plus interest, to authors to settle its class-action lawsuit. The amount breaks down to smaller payouts expected to be approximately $3,000 per book or work.</p>
</blockquote>
<p>It's wild to me that a $1.5 billion settlement can feel like a win for Anthropic, but given that it's undisputed that they downloaded pirated books  (as did Meta and likely many other research teams) the maximum allowed penalty was $150,000 per book, so $3,000 per book is actually a significant discount.</p>
<p>As far as I can tell this case sets a precedent for Anthropic's <a href="https://simonwillison.net/2025/Jun/24/anthropic-training/#purchase-and-scan">more recent approach</a> of buying millions of (mostly used) physical books and destructively scanning them for training as covered by "fair use". I'm not sure if other in-flight legal cases will find differently.</p>
<p>To be clear: it appears it is legal, at least in the USA, to buy a used copy of a physical book (used = the author gets nothing), chop the spine off, scan the pages, discard the paper copy and then train on the scanned content. The transformation from paper to scan is "fair use".</p>
<p>If this <em>does</em> hold it's going to be a great time to be a bulk retailer of used books!</p>
<p><strong>Update</strong>: The official website for the class action lawsuit is <a href="https://www.anthropiccopyrightsettlement.com">www.anthropiccopyrightsettlement.com</a>:</p>
<blockquote>
<p>In the coming weeks, and if the court preliminarily approves the settlement, the website will provide to find a full and easily searchable listing of all works covered by the settlement.</p>
</blockquote>
<p>In the meantime the Atlantic have <a href="https://www.theatlantic.com/technology/archive/2025/03/search-libgen-data-set/682094/">a search engine</a> to see if your work was included in LibGen, one of the pirated book sources involved in this case.</p>
<p>I had a look and it turns out the book I co-authored with 6 other people back in 2007 <a href="https://www.oreilly.com/library/view/the-art/9780980285840/">The Art &amp; Science of JavaScript</a> is in there, so maybe I'm due for 1/7th of one of those $3,000 settlements!</p>
<p><strong>Update 2</strong>: Here's an interesting detail from the <a href="https://www.washingtonpost.com/technology/2025/09/05/anthropic-book-authors-copyright-settlement/">Washington Post story</a> about the settlement:</p>
<blockquote>
<p>Anthropic said in the settlement that the specific digital copies of books covered by the agreement were not used in the training of its commercially released AI models.</p>
</blockquote>
<p><strong>Update 3</strong>: I'm not confident that destroying the scanned books is a hard requirement here - I got that impression from <a href="https://www.documentcloud.org/documents/25982181-authors-v-anthropic-ruling/#document/p16">this section</a> of the summary judgment in June:</p>
<blockquote>
<p>Here, every purchased print copy was copied in order to save storage space and to enable searchability as a digital copy. The print original was destroyed. One replaced the other. And, there is no evidence that the new, digital copy was shown, shared, or sold outside the company. This use was even more clearly transformative than those in <em>Texaco</em>, <em>Google</em>, and <em>Sony Betamax</em> (where the number of copies went up by at least one), and, of course, more transformative than those uses rejected in <em>Napster</em> (where the number went up by “millions” of copies shared for free with others).</p>
</blockquote>


    <p>Tags: <a href="https://simonwillison.net/tags/law">law</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/training-data">training-data</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a></p>]]></description><pubDate>Sat, 6 Sep 2025 05:51:27 +0000</pubDate></item><item><title>Quoting Kenton Varda</title><link>https://simonwillison.net/2025/Sep/5/kenton-varda/#atom-everything</link><description><![CDATA[<blockquote cite="https://twitter.com/KentonVarda/status/1963966469148180839"><p>After struggling for years trying to figure out why people think [Cloudflare] Durable Objects are complicated, I'm increasingly convinced that it's just that they <em>sound</em> complicated.</p>
<p>Feels like we can solve 90% of it by renaming <code>DurableObject</code> to <code>StatefulWorker</code>?</p>
<p>It's just a worker that has state. And because it has state, it also has to have a name, so that you can route to the specific worker that has the state you care about. There may be a sqlite database attached, there may be a container attached. Those are just part of the state.</p></blockquote>
<p class="cite">&mdash; <a href="https://twitter.com/KentonVarda/status/1963966469148180839">Kenton Varda</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/kenton-varda">kenton-varda</a>, <a href="https://simonwillison.net/tags/sqlite">sqlite</a>, <a href="https://simonwillison.net/tags/cloudflare">cloudflare</a></p>]]></description><pubDate>Fri, 5 Sep 2025 16:43:13 +0000</pubDate></item><item><title>Introducing EmbeddingGemma</title><link>https://simonwillison.net/2025/Sep/4/embedding-gemma/#atom-everything</link><description><![CDATA[<p><strong><a href="https://developers.googleblog.com/en/introducing-embeddinggemma/">Introducing EmbeddingGemma</a></strong></p>
Brand new open weights (under the slightly janky <a href="https://ai.google.dev/gemma/terms">Gemma license</a>) 308M parameter embedding model from Google:</p>
<blockquote>
<p>Based on the Gemma 3 architecture, EmbeddingGemma is trained on 100+ languages and is small enough to run on less than 200MB of RAM with quantization.</p>
</blockquote>
<p>It's available via <a href="https://ai.google.dev/gemma/docs/embeddinggemma/fine-tuning-embeddinggemma-with-sentence-transformers">sentence-transformers</a>, <a href="https://huggingface.co/collections/ggml-org/embeddinggemma-300m-68b2a87d78ca52408f7918f3">llama.cpp</a>, <a href="https://huggingface.co/collections/mlx-community/embeddinggemma-68b9a55aac55466fbd514f7c">MLX</a>, <a href="https://ollama.com/library/embeddinggemma">Ollama</a>, <a href="https://lmstudio.ai/models/google/embedding-gemma-300m">LMStudio</a> and more. </p>
<p>As usual for these smaller models there's a <a href="https://huggingface.co/blog/embeddinggemma#transformersjs">Transformers.js</a> demo (<a href="https://twitter.com/xenovacom/status/1963638444233511016">via</a>) that runs directly in the browser (in Chrome variants) - <a href="https://huggingface.co/spaces/webml-community/semantic-galaxy">Semantic Galaxy</a> loads a ~400MB model and then lets you run embeddings against hundreds of text sentences, map them in a 2D space and run similarity searches to zoom to points within that space.</p>
<p><img alt="Screenshot of The Semantic Galaxy web application interface showing a semantic search tool with a left sidebar containing &quot;Your Dataset&quot; with sample text &quot;The sun peeked through the clouds after a drizzly&quot; and a blue &quot;Generate Galaxy&quot; button, below which is text &quot;Galaxy generated with 106 points. Ready to explore!&quot; followed by &quot;Search Results&quot; listing various text snippets with similarity scores to the search term &quot;pelican riding a bicycle&quot; such as &quot;The cyclist pedaled up the steep hill... 0.491&quot;, &quot;It was so hot that even the birds sou... 0.446&quot;, etc. The main area shows a dark starfield visualization with white dots representing semantic clusters and text snippets floating as labels near the clusters." src="https://static.simonwillison.net/static/2025/semantic-galaxy-transformers.jpg" />


    <p>Tags: <a href="https://simonwillison.net/tags/google">google</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/embeddings">embeddings</a>, <a href="https://simonwillison.net/tags/transformers-js">transformers-js</a>, <a href="https://simonwillison.net/tags/gemma">gemma</a></p>]]></description><pubDate>Thu, 4 Sep 2025 22:27:41 +0000</pubDate></item><item><title>Highlighted tools</title><link>https://simonwillison.net/2025/Sep/4/highlighted-tools/#atom-everything</link><description><![CDATA[<p>Any time I share my <a href="https://tools.simonwillison.net/">collection of tools</a> built using vibe coding and AI-assisted development (now at 124, here's <a href="https://tools.simonwillison.net/colophon">the definitive list</a>) someone will inevitably complain that they're mostly trivial.</p>
<p>A lot of them are! Here's a list of some that I think are genuinely useful and worth highlighting:</p>
<ul>
<li><a href="https://tools.simonwillison.net/ocr">OCR PDFs and images directly in your browser</a>. This is the tool that started the collection, and I still use it on a regular basis. You can open any PDF in it (even PDFs that are just scanned images with no embedded text) and it will extract out the text so you can copy-and-paste it. It uses PDF.js and Tesseract.js to do that entirely in the browser. I wrote about <a href="https://simonwillison.net/2024/Mar/30/ocr-pdfs-images/">how I originally built that here</a>.</li>
<li><a href="https://tools.simonwillison.net/annotated-presentations">Annotated Presentation Creator</a> - this one is <em>so useful</em>. I use it to turn talks that I've given into full annotated presentations, where each slide is accompanied by detailed notes. I have <a href="https://simonwillison.net/tags/annotated-talks/">29 blog entries</a> like that now and most of them were written with the help of this tool. Here's <a href="https://simonwillison.net/2023/Aug/6/annotated-presentations/">how I built that</a>, plus <a href="https://tools.simonwillison.net/colophon#annotated-presentations.html">follow-up prompts I used to improve it</a>.</li>
<li><a href="https://tools.simonwillison.net/image-resize-quality">Image resize, crop, and quality comparison</a> - I use this for every single image I post to my blog. It lets me drag (or paste) an image onto the page and then shows me a comparison of different sizes and quality settings, each of which I can download and then upload to my S3 bucket. I recently added a slightly janky but mobile-accessible cropping tool as well. <a href="https://tools.simonwillison.net/colophon#image-resize-quality.html">Prompts</a>.</li>
<li><a href="https://tools.simonwillison.net/social-media-cropper">Social Media Card Cropper</a> - this is an even more useful image tool. Bluesky, Twitter etc all benefit from a 2x1 aspect ratio "card" image. I built this custom tool for creating those - you can paste in an image and crop and zoom it to the right dimensions. I use this all the time. <a href="https://tools.simonwillison.net/colophon#social-media-cropper.html">Prompts</a>.</li>
<li><a href="https://tools.simonwillison.net/svg-render">SVG to JPEG/PNG</a> - every time I publish an <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle/">SVG of a pelican riding a bicycle</a> I use this tool to turn that SVG into a JPEG or PNG. <a href="https://tools.simonwillison.net/colophon#svg-render.html">Prompts</a>.</li>
<li><a href="https://tools.simonwillison.net/encrypt">Encrypt / decrypt message</a> - I often run workshops where I want to distribute API keys to the workshop participants. This tool lets me encrypt a message with a passphrase, then share the resulting URL to the encrypted message and tell people (with a note on a slide) how to decrypt it. <a href="https://tools.simonwillison.net/colophon#encrypt.html">Prompt</a>.</li>
<li><a href="https://tools.simonwillison.net/jina-reader">Jina Reader</a> - enter a URL, get back a Markdown version of the page. It's a thin wrapper over the Jina Reader API, but it's useful because it adds a "copy to clipboard" button which means it's one of the fastest way to turn a webpage into data on a clipboard on my mobile phone. I use this several times a week. <a href="https://tools.simonwillison.net/colophon#jina-reader.html">Prompts</a>.</li>
<li><a href="https://www.llm-prices.com/">llm-prices.com</a> - a pricing comparison and token pricing calculator for various hosted LLMs. This one started out as a tool but graduated to its own domain name. Here's the <a href="https://tools.simonwillison.net/colophon#llm-prices.html">prompting development history</a>.</li>
<li><a href="https://tools.simonwillison.net/open-sauce-2025">Open Sauce 2025</a> - an unofficial schedule for the Open Sauce conference, complete with option to export to ICS plus a search tool and now-and-next. I built this <em>entirely on my phone</em> using OpenAI Codex, including scraping the official schedule - <a href="https://simonwillison.net/2025/Jul/17/vibe-scraping/">full details here</a>. </li>
<li><a href="https://tools.simonwillison.net/hacker-news-histogram">Hacker News Multi-Term Histogram</a> - compare search terms on Hacker News to see how their relative popularity changed over time. <a href="https://tools.simonwillison.net/colophon#hacker-news-histogram.html">Prompts</a>.</li>
<li><a href="https://tools.simonwillison.net/passkeys">Passkey experiment</a> - a UI for trying out the Passkey / WebAuthn APIs that are built into browsers these days. <a href="https://tools.simonwillison.net/colophon#passkeys.html">Prompts</a>.</li>
<li><a href="https://tools.simonwillison.net/incomplete-json-printer">Incomplete JSON Pretty Printer</a> - do you ever find yourself staring at a screen full of JSON that isn't completely valid because it got truncated? This tool will pretty-print it anyway. <a href="https://tools.simonwillison.net/colophon#incomplete-json-printer.html">Prompts</a>.</li>
<li><a href="https://tools.simonwillison.net/bluesky-firehose">Bluesky WebSocket Feed Monitor</a> - I found out Bluesky has a Firehose API that can be accessed directly from the browser, so I vibe-coded up this tool to try it out. <a href="https://tools.simonwillison.net/colophon#bluesky-firehose.html">Prompts</a>.</li>
</ul>
<p>In putting this list together I realized I wanted to be able to link to the prompts for each tool... but those were hidden inside a collapsed <code>&lt;details&gt;&lt;summary&gt;</code> element for each one. So I fired up <a href="https://openai.com/codex/">OpenAI Codex</a> and prompted:</p>
<blockquote>
<p><code>Update the script that builds the colophon.html page such that the generated page has a tiny bit of extra JavaScript - when the page is loaded as e.g. https://tools.simonwillison.net/colophon#jina-reader.html it should notice the #jina-reader.html fragment identifier and ensure that the Development history details/summary for that particular tool is expanded when the page loads.</code></p>
</blockquote>
<p>It <a href="https://github.com/simonw/tools/pull/47">authored this PR for me</a> which fixed the problem.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/tools">tools</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/projects">projects</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a></p>]]></description><pubDate>Thu, 4 Sep 2025 21:58:11 +0000</pubDate></item><item><title>Beyond Vibe Coding</title><link>https://simonwillison.net/2025/Sep/4/beyond-vibe-coding/#atom-everything</link><description><![CDATA[<p><strong><a href="https://beyond.addy.ie/">Beyond Vibe Coding</a></strong></p>
Back in May I wrote <a href="https://simonwillison.net/2025/May/1/not-vibe-coding/">Two publishers and three authors fail to understand what “vibe coding” means</a> where I called out the authors of two forthcoming books on "vibe coding" for abusing that term to refer to all forms of AI-assisted development, when <a href="https://simonwillison.net/2025/Mar/19/vibe-coding/">Not all AI-assisted programming is vibe coding</a> based on the <a href="https://twitter.com/karpathy/status/1886192184808149383">original Karpathy definition</a>.</p>
<p>I'll be honest: I don't feel great about that post. I made an example of those two books to push my own agenda of encouraging "vibe coding" to avoid <a href="https://simonwillison.net/2025/Mar/23/semantic-diffusion/">semantic diffusion</a> but it felt (and feels) a bit mean.</p>
<p>... but maybe it had an effect? I recently spotted that Addy Osmani's book "Vibe Coding: The Future of Programming" has a new title, it's now called "Beyond Vibe Coding: From Coder to AI-Era Developer".</p>
<p>This title is <strong>so much better</strong>. Setting aside my earlier opinions, this positioning as a book to help people go <em>beyond</em> vibe coding and use LLMs as part of a professional engineering practice is a really great hook!</p>
<p>From Addy's new description of the book:</p>
<blockquote>
<p>Vibe coding was never meant to describe all AI-assisted coding. It's a specific approach where you don't read the AI's code before running it. There's much more to consider beyond the prototype for production systems. [...]</p>
<p>AI-assisted engineering is a more structured approach that combines the creativity of vibe coding with the rigor of traditional engineering practices. It involves specs, rigor and emphasizes collaboration between human developers and AI tools, ensuring that the final product is not only functional but also maintainable and secure.</p>
</blockquote>
<p>Amazon <a href="https://www.amazon.com/Beyond-Vibe-Coding-Leveraging-AI-Assisted/dp/B0F6S5425Y">lists it</a> as releasing on September 23rd. I'm looking forward to it.</p>
<p><img alt="O'Reilly book cover: Beyond Vibe Coding: From Coder to AI-Era Developer, by Addy Osmani. Features two hummingbirds, presumably because their wings vibrate!" src="https://static.simonwillison.net/static/2025/beyond-vibe-coding.jpg" />


    <p>Tags: <a href="https://simonwillison.net/tags/books">books</a>, <a href="https://simonwillison.net/tags/oreilly">oreilly</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/addy-osmani">addy-osmani</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a></p>]]></description><pubDate>Thu, 4 Sep 2025 20:58:21 +0000</pubDate></item><item><title>Google antitrust remedies</title><link>https://simonwillison.net/2025/Sep/3/antitrust/#atom-everything</link><description><![CDATA[<p><strong><a href="https://storage.courtlistener.com/recap/gov.uscourts.dcd.223205/gov.uscourts.dcd.223205.1436.0_1.pdf">gov.uscourts.dcd.223205.1436.0_1.pdf</a></strong></p>
Here's the 230 page PDF ruling on the 2023 <a href="https://en.wikipedia.org/wiki/United_States_v._Google_LLC_(2023)">United States v. Google LLC federal antitrust case</a> - the case that could have resulted in Google selling off Chrome and cutting most of Mozilla's funding.</p>
<p>I made it through the first dozen pages - it's actually quite readable.</p>
<p>It opens with a clear summary of the case so far, bold highlights mine:</p>
<blockquote>
<p>Last year, this court ruled that Defendant Google LLC had violated Section 2 of the Sherman Act: “Google is a monopolist, and it has acted as one to maintain its monopoly.” <strong>The court found that, for more than a decade, Google had entered into distribution agreements with browser developers, original equipment manufacturers, and wireless carriers to be the out-of-the box, default general search engine (“GSE”) at key search access points</strong>. These access points were the most efficient channels for distributing a GSE, and Google paid billions to lock them up. The agreements harmed competition. <strong>They prevented rivals from accumulating the queries and associated data, or scale, to effectively compete and discouraged investment and entry into the market</strong>. And they enabled Google to earn monopoly profits from its search text ads, to amass an unparalleled volume of scale to improve its search product, and to remain the default GSE without fear of being displaced. Taken together, these agreements effectively “froze” the search ecosystem, resulting in markets in which Google has “no true competitor.”</p>
</blockquote>
<p>There's an interesting generative AI twist: when the case was first argued in 2023 generative AI wasn't an influential issue, but more recently Google seem to be arguing that it is an existential threat that they need to be able to take on without additional hindrance:</p>
<blockquote>
<p>The emergence of GenAl changed the course of this case. No witness at the liability trial testified that GenAl products posed a near-term threat to GSEs. <strong>The very first witness at the remedies hearing, by contrast, placed GenAl front and center as a nascent competitive threat</strong>. These remedies proceedings thus have been as much about promoting competition among GSEs as ensuring that Google’s dominance in search does not carry over into the GenAlI space. Many of Plaintiffs’ proposed remedies are crafted with that latter objective in mind.</p>
</blockquote>
<p>I liked this note about the court's challenges in issuing effective remedies:</p>
<blockquote>
<p>Notwithstanding this power, courts must approach the task of crafting remedies with a healthy dose of humility. This court has done so. It has no expertise in the business of GSEs, the buying and selling of search text ads, or the engineering of GenAl technologies. <strong>And, unlike the typical case where the court’s job is to resolve a dispute based on historic facts, here the court is asked to gaze into a crystal ball and look to the future. Not exactly a judge’s forte</strong>.</p>
</blockquote>
<p>On to the remedies. These ones looked particularly important to me:</p>
<blockquote>
<ul>
<li>Google will be barred from entering or maintaining any exclusive contract
relating to the distribution of Google Search, Chrome, Google Assistant,
and the Gemini app. [...]</li>
<li>Google will not be required to divest Chrome; nor will the court include a
contingent divestiture of the Android operating system in the final
judgment. Plaintiffs overreached in seeking forced divesture of these key
assets, which Google did not use to effect any illegal restraints. [...]</li>
</ul>
</blockquote>
<p>I guess Perplexity <a href="https://www.bbc.co.uk/news/articles/c3dpr0kkyz4o">won't be buying Chrome</a> then!</p>
<blockquote>
<ul>
<li>Google will not be barred from making payments or offering other
consideration to distribution partners for preloading or placement of Google
Search, Chrome, or its GenAl products. <strong>Cutting off payments from Google
almost certainly will impose substantial —in some cases, crippling—
downstream harms to distribution partners</strong>, related markets, and consumers,
which counsels against a broad payment ban.</li>
</ul>
</blockquote>
<p>That looks like a huge sigh of relief for Mozilla, who were at risk of losing a sizable portion of their income if Google's search distribution revenue were to be cut off.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=45108548">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/chrome">chrome</a>, <a href="https://simonwillison.net/tags/google">google</a>, <a href="https://simonwillison.net/tags/law">law</a>, <a href="https://simonwillison.net/tags/mozilla">mozilla</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a></p>]]></description><pubDate>Wed, 3 Sep 2025 08:56:30 +0000</pubDate></item><item><title>Making XML human-readable without XSLT</title><link>https://simonwillison.net/2025/Sep/2/making-xml-human-readable-without-xslt/#atom-everything</link><description><![CDATA[<p><strong><a href="https://jakearchibald.com/2025/making-xml-human-readable-without-xslt/">Making XML human-readable without XSLT</a></strong></p>
In response to the <a href="https://simonwillison.net/2025/Aug/19/xslt/">recent discourse</a> about XSLT support in browsers, Jake Archibald shares a new-to-me alternative trick for making an XML document readable in a browser: adding the following element near the top of the XML:</p>
<pre><code>&lt;script
  xmlns="http://www.w3.org/1999/xhtml"
  src="script.js" defer="" /&gt;
</code></pre>
<p>That <code>script.js</code> will then be executed by the browser, and can swap out the XML with HTML by creating new elements using the correct namespace:</p>
<pre><code>const htmlEl = document.createElementNS(
  'http://www.w3.org/1999/xhtml',
  'html',
);
document.documentElement.replaceWith(htmlEl);
// Now populate the new DOM
</code></pre>


    <p>Tags: <a href="https://simonwillison.net/tags/browsers">browsers</a>, <a href="https://simonwillison.net/tags/javascript">javascript</a>, <a href="https://simonwillison.net/tags/rss">rss</a>, <a href="https://simonwillison.net/tags/xml">xml</a>, <a href="https://simonwillison.net/tags/xslt">xslt</a>, <a href="https://simonwillison.net/tags/jake-archibald">jake-archibald</a></p>]]></description><pubDate>Tue, 2 Sep 2025 19:32:57 +0000</pubDate></item><item><title>Rich Pixels</title><link>https://simonwillison.net/2025/Sep/2/rich-pixels/#atom-everything</link><description><![CDATA[<p><strong><a href="https://github.com/darrenburns/rich-pixels">Rich Pixels</a></strong></p>
Neat Python library by Darren Burns adding pixel image support to the Rich terminal library, using tricks to render an image using full or half-height colored blocks.</p>
<p>Here's <a href="https://github.com/darrenburns/rich-pixels/blob/a0745ebcc26b966d9dbac5875720364ee5c6a1d3/rich_pixels/_renderer.py#L123C25-L123C26">the key trick</a> - it renders Unicode ▄ (U+2584, "lower half block") characters after setting a foreground and background color for the two pixels it needs to display.</p>
<p>I got GPT-5 to <a href="https://chatgpt.com/share/68b6c443-2408-8006-8f4a-6862755cd1e4">vibe code up</a> a <code>show_image.py</code> terminal command which resizes the provided image to fit the width and height of the current terminal and displays it using Rich Pixels. That <a href="https://github.com/simonw/tools/blob/main/python/show_image.py">script is here</a>, you can run it with <code>uv</code> like this:</p>
<pre><code>uv run https://tools.simonwillison.net/python/show_image.py \
  image.jpg
</code></pre>
<p>Here's what I got when I ran it against my V&amp;A East Storehouse photo from <a href="https://simonwillison.net/2025/Aug/27/london-culture/">this post</a>:</p>
<p><img alt="Terminal window. I ran that command and it spat out quite a pleasing and recognizable pixel art version of the photograph." src="https://static.simonwillison.net/static/2025/pixel-storehouse.jpg" />


    <p>Tags: <a href="https://simonwillison.net/tags/ascii-art">ascii-art</a>, <a href="https://simonwillison.net/tags/cli">cli</a>, <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/uv">uv</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a>, <a href="https://simonwillison.net/tags/gpt-5">gpt-5</a>, <a href="https://simonwillison.net/tags/rich">rich</a></p>]]></description><pubDate>Tue, 2 Sep 2025 11:05:23 +0000</pubDate></item><item><title>August 2025 newsletter</title><link>https://simonwillison.net/2025/Sep/1/august-2025/#atom-everything</link><description><![CDATA[<p>I just sent out my August 2025 <strong><a href="https://github.com/sponsors/simonw">sponsors-only newsletter</a></strong> summarizing the past month in LLMs and my other work. Topics included GPT-5, gpt-oss, image editing models (Qwen-Image-Edit and Gemini Nano Banana), other significant model releases and the tools I'm using at the moment.</p>
<p>If you'd like a preview of the newsletter, here's <a href="https://gist.github.com/simonw/722fc2f242977cb185838353776d14f4">the July 2025 edition</a> I sent out a month ago.</p>
<p>New sponsors get access to the full archive. If you start sponsoring for $10/month or more right now you'll get instant access to <a href="https://github.com/simonw-private/monthly/blob/main/2025-08-august.md">the August edition</a> in my <code>simonw-private/monthly</code> GitHub repository.</p>
<p>If you've already read <a href="https://simonwillison.net/2025/Aug/">all 85 posts</a> I wrote in August the newsletter acts mainly as a recap, but I've had positive feedback from people who prefer to get the monthly edited highlights over reading the firehose that is my blog!</p>
<p>Here's the table of contents for the August newsletter:</p>
<blockquote>
<ul>
<li>GPT-5</li>
<li>OpenAl's open models: gpt-oss-120b and gpt-oss-20b</li>
<li>Other significant model releases in
August</li>
<li>Image editing: Qwen-Image-Edit and
Gemini Nano Banana</li>
<li>More prompt injection and more lethal trifecta</li>
<li>Tools I'm using at the moment</li>
<li>Bonus links</li>
</ul>
</blockquote>

    <p>Tags: <a href="https://simonwillison.net/tags/newsletter">newsletter</a></p>]]></description><pubDate>Mon, 1 Sep 2025 19:41:45 +0000</pubDate></item><item><title>Introducing gpt-realtime</title><link>https://simonwillison.net/2025/Sep/1/introducing-gpt-realtime/#atom-everything</link><description><![CDATA[<p><strong><a href="https://openai.com/index/introducing-gpt-realtime/">Introducing gpt-realtime</a></strong></p>
Released a few days ago (August 28th), <code>gpt-realtime</code> is OpenAI's new "most advanced speech-to-speech model". It looks like this is a replacement for the older <code>gpt-4o-realtime-preview</code> model that was released <a href="https://openai.com/index/introducing-the-realtime-api/">last October</a>.</p>
<p>This is a slightly confusing release. The previous realtime model was clearly described as a variant of GPT-4o, sharing the same October 2023 training cut-off date as that model.</p>
<p>I had expected that <code>gpt-realtime</code> might be a GPT-5 relative, but its training date is still October 2023 whereas GPT-5 is September 2024.</p>
<p><code>gpt-realtime</code> also shares the relatively low 32,000 context token and 4,096 maximum output token limits of <code>gpt-4o-realtime-preview</code>.</p>
<p>The only reference I found to GPT-5 in the documentation for the new model was a note saying "Ambiguity and conflicting instructions degrade performance, similar to GPT-5."</p>
<p>The <a href="https://platform.openai.com/docs/guides/realtime-models-prompting#general-usage-tips">usage tips</a> for <code>gpt-realtime</code> have a few surprises:</p>
<blockquote>
<p><strong>Iterate relentlessly</strong>. Small wording changes can make or break behavior.</p>
<p>Example: Swapping “inaudible” → “unintelligible” improved noisy input handling. [...]</p>
<p><strong>Convert non-text rules to text</strong>: The model responds better to clearly written text.</p>
<p>Example: Instead of writing, "IF x &gt; 3 THEN ESCALATE", write, "IF MORE THAN THREE FAILURES THEN ESCALATE."</p>
</blockquote>
<p>There are a whole lot more prompting tips in the new <a href="https://cookbook.openai.com/examples/realtime_prompting_guide">Realtime Prompting Guide</a>.</p>
<p>OpenAI list several key improvements to <code>gpt-realtime</code> including the ability to configure it with a list of MCP servers, "better instruction following" and the ability to send it images.</p>
<p>My biggest confusion came from <a href="https://openai.com/api/pricing/">the pricing page</a>, which lists separate pricing for using the Realtime API with <code>gpt-realtime</code> and GPT-4o mini. This suggests to me that the old <a href="https://platform.openai.com/docs/models/gpt-4o-mini-realtime-preview">gpt-4o-mini-realtime-preview</a> model is still available, despite it no longer being listed on the <a href="https://platform.openai.com/docs/models">OpenAI models page</a>.</p>
<p><code>gpt-4o-mini-realtime-preview</code> is a <strong>lot</strong> cheaper:</p>
<table>
    <thead>
        <tr>
            <th>Model</th>
            <th>Token Type</th>
            <th>Input</th>
            <th>Cached Input</th>
            <th>Output</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan="3">gpt-realtime</td>
            <td>Text</td>
            <td>$4.00</td>
            <td>$0.40</td>
            <td>$16.00</td>
        </tr>
        <tr>
            <td>Audio</td>
            <td>$32.00</td>
            <td>$0.40</td>
            <td>$64.00</td>
        </tr>
        <tr>
            <td>Image</td>
            <td>$5.00</td>
            <td>$0.50</td>
            <td>-</td>
        </tr>
        <tr>
            <td rowspan="2">gpt-4o-mini-realtime-preview</td>
            <td>Text</td>
            <td>$0.60</td>
            <td>$0.30</td>
            <td>$2.40</td>
        </tr>
        <tr>
            <td>Audio</td>
            <td>$10.00</td>
            <td>$0.30</td>
            <td>$20.00</td>
        </tr>
    </tbody>
</table>

<p>The mini model also has a much longer 128,000 token context window.</p>
<p><strong>Update</strong>: Turns out that was <a href="https://twitter.com/_agamble/status/1962839472837361807">a mistake in the documentation</a>, that mini model has a 16,000 token context size.</p>
<p><strong>Update 2</strong>: OpenAI's <a href="https://twitter.com/pbbakkum/status/1962901822135525695">Peter Bakkum clarifies</a>:</p>
<blockquote>
<p>There are different voice models in API and ChatGPT, but they share some recent improvements. The voices are also different.</p>
<p>gpt-realtime has a mix of data specific enough to itself that its not really 4o or 5</p>
</blockquote>


    <p>Tags: <a href="https://simonwillison.net/tags/audio">audio</a>, <a href="https://simonwillison.net/tags/realtime">realtime</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/llm-pricing">llm-pricing</a>, <a href="https://simonwillison.net/tags/multi-modal-output">multi-modal-output</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a></p>]]></description><pubDate>Mon, 1 Sep 2025 17:34:55 +0000</pubDate></item><item><title>Cloudflare Radar: AI Insights</title><link>https://simonwillison.net/2025/Sep/1/cloudflare-radar-ai-insights/#atom-everything</link><description><![CDATA[<p><strong><a href="https://radar.cloudflare.com/ai-insights">Cloudflare Radar: AI Insights</a></strong></p>
Cloudflare launched this dashboard <a href="https://blog.cloudflare.com/expanded-ai-insights-on-cloudflare-radar/">back in February</a>, incorporating traffic analysis from Cloudflare's network along with insights from their popular 1.1.1.1 DNS service.</p>
<p>I found this chart particularly interesting, showing which documented AI crawlers are most active collecting training data - lead by GPTBot, ClaudeBot and Meta-ExternalAgent:</p>
<p><img alt="Line chart showing HTTP traffic by bot over time from August 26 to September 1. HTTP traffic by bot - HTTP request trends for top five most active AI bots. Crawl purpose: Training. GPTBot 31.7% (orange line), ClaudeBot 27.1% (blue line), Meta-ExternalAgent 25.3% (light blue line), Bytespider 9.3% (yellow-green line), Applebot 5.2% (green line). Max scale shown on y-axis. X-axis shows dates: Tue, Aug 26, Wed, Aug 27, Thu, Aug 28, Fri, Aug 29, Sat, Aug 30, Sun, Aug 31, Mon, Sep 1. Top right shows Crawl purpose dropdown set to &quot;Training&quot; with X and checkmark buttons." src="https://static.simonwillison.net/static/2025/http-traffic-by-bot.jpg" /></p>
<p>Cloudflare's DNS data also hints at the popularity of different services. ChatGPT holds the first place, which is unsurprising - but second place is a hotly contested race between Claude and Perplexity and #4/#5/#6 is contested by GitHub Copilot, Perplexity, and Codeium/Windsurf.</p>
<p>Google Gemini comes in 7th, though since this is DNS based I imagine this is undercounting instances of Gemini on <code>google.com</code> as opposed to <code>gemini.google.com</code>.</p>
<p><img alt="Line chart showing generative AI services popularity rankings over time. Title: &quot;Generative AI services popularity&quot; with subtitle &quot;Top 10 services based on 1.1.1.1 DNS resolver traffic&quot; and question mark and share icons. Legend shows: ChatGPT/OpenAI (dark blue), Character.AI (light blue), Claude/Anthropic (orange), Perplexity (olive green), GitHub Copilot (green), Codeium/Windsurf AI (pink), Google Gemini (purple), QuillBot (red), Grok/xAI (brown), DeepSeek (yellow). Y-axis shows ranks #1-#10, X-axis shows dates from Mon, Aug 25 to Mon, Sep 1 (partially visible). ChatGPT maintains #1 position throughout. Other services show various ranking changes over the week-long period." src="https://static.simonwillison.net/static/2025/cloudflare-gen-ai.jpg" />

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=45093090">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/crawling">crawling</a>, <a href="https://simonwillison.net/tags/dns">dns</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/cloudflare">cloudflare</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Mon, 1 Sep 2025 17:06:56 +0000</pubDate></item><item><title>Claude Opus 4.1 and Opus 4 degraded quality</title><link>https://simonwillison.net/2025/Aug/30/claude-degraded-quality/#atom-everything</link><description><![CDATA[<p><strong><a href="https://status.anthropic.com/incidents/h26lykctfnsz">Claude Opus 4.1 and Opus 4 degraded quality</a></strong></p>
Notable because often when people complain of degraded model quality it turns out to be unfounded - Anthropic in the past have emphasized that they don't change the model weights after releasing them without changing the version number.</p>
<p>In this case a botched upgrade of their inference stack cause a genuine model degradation for 56.5 hours:</p>
<blockquote>
<p>From 17:30 UTC on Aug 25th to 02:00 UTC on Aug 28th, Claude Opus 4.1 experienced a degradation in quality for some requests. Users may have seen lower intelligence, malformed responses or issues with tool calling in Claude Code.</p>
<p>This was caused by a rollout of our inference stack, which we have since rolled back for Claude Opus 4.1. [...]</p>
<p>We’ve also discovered that Claude Opus 4.0 has been affected by the same issue and we are in the process of rolling it back.</p>
</blockquote>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/claude-4">claude-4</a></p>]]></description><pubDate>Sat, 30 Aug 2025 21:04:13 +0000</pubDate></item><item><title>Quoting Benj Edwards</title><link>https://simonwillison.net/2025/Aug/30/benj-edwards/#atom-everything</link><description><![CDATA[<blockquote cite="https://arstechnica.com/information-technology/2025/08/the-personhood-trap-how-ai-fakes-human-personality/"><p>LLMs are intelligence without agency—what we might call "vox sine persona": voice without person. Not the voice of someone, not even the collective voice of many someones, but a voice emanating from no one at all.</p></blockquote>
<p class="cite">&mdash; <a href="https://arstechnica.com/information-technology/2025/08/the-personhood-trap-how-ai-fakes-human-personality/">Benj Edwards</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/benj-edwards">benj-edwards</a>, <a href="https://simonwillison.net/tags/ai-personality">ai-personality</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Sat, 30 Aug 2025 06:52:53 +0000</pubDate></item><item><title>Talk Python: Celebrating Django&apos;s 20th Birthday With Its Creators</title><link>https://simonwillison.net/2025/Aug/29/talk-python/#atom-everything</link><description><![CDATA[<p><strong><a href="https://talkpython.fm/episodes/show/518/celebrating-djangos-20th-birthday-with-its-creators">Talk Python: Celebrating Django&#x27;s 20th Birthday With Its Creators</a></strong></p>
I recorded this podcast episode recently to celebrate Django's 20th birthday with Adrian Holovaty, Will Vincent, Jeff Triplet, and Thibaud Colas.</p>
<blockquote>
<p>We didn’t know that it was a web framework. We thought it was a tool for building local newspaper websites. [...]</p>
<p>Django’s original tagline was ‘Web development on journalism deadlines’. That’s always been my favorite description of the project.</p>
</blockquote>


    <p>Tags: <a href="https://simonwillison.net/tags/adrian-holovaty">adrian-holovaty</a>, <a href="https://simonwillison.net/tags/django">django</a>, <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/podcast-appearances">podcast-appearances</a></p>]]></description><pubDate>Fri, 29 Aug 2025 20:02:50 +0000</pubDate></item></channel></rss>