<?xml version="1.0" encoding="utf-8"?><rss version="2.0"><channel><title>Simon Willison&apos;s Weblog</title><link>https://raw.githubusercontent.com/xavwe/rss-aggregator/refs/heads/main/feeds/simon-willison-s-weblog-2b081550.xml</link><description>Archived feed from https://simonwillison.net/atom/everything</description><item><title>First kākāpō chick in four years hatches on Valentine&apos;s Day</title><link>https://simonwillison.net/2026/Feb/17/first-kakapo-chick-in-four-years/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.doc.govt.nz/news/media-releases/2026-media-releases/first-kakapo-chick-in-four-years-hatches-on-valentines-day/">First kākāpō chick in four years hatches on Valentine&#x27;s Day</a></strong></p>
First chick of <a href="https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-k-k-p-parrots-will-have-an-outstanding-breeding-season">the 2026 breeding season</a>!</p>
<blockquote>
<p>Kākāpō Yasmine hatched an egg fostered from kākāpō Tīwhiri on Valentine's Day, bringing the total number of kākāpō to 237 – though it won’t be officially added to the population until it fledges.</p>
</blockquote>
<p>Here's why the egg was fostered:</p>
<blockquote>
<p>"Kākāpō mums typically have the best outcomes when raising a maximum of two chicks. Biological mum Tīwhiri has four fertile eggs this season already, while Yasmine, an experienced foster mum, had no fertile eggs."</p>
</blockquote>
<p>And an <a href="https://bsky.app/profile/digs.bsky.social/post/3mf25glzt2c2b">update from conservation biologist Andrew Digby</a> - a second chick hatched this morning!</p>
<blockquote>
<p>The second #kakapo chick of the #kakapo2026 breeding season hatched this morning: Hine Taumai-A1-2026 on Ako's nest on Te Kākahu. We transferred the egg from Anchor two nights ago. This is Ako's first-ever chick, which is just a few hours old in this video.</p>
</blockquote>
<p>That post <a href="https://bsky.app/profile/digs.bsky.social/post/3mf25glzt2c2b">has a video</a> of mother and chick.

    <p><small></small>Via <a href="https://www.metafilter.com/212231/Happy-Valen-Kkp-Tines">MetaFilter</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/kakapo">kakapo</a></p>]]></description><pubDate>Tue, 17 Feb 2026 14:09:43 +0000</pubDate></item><item><title>Quoting Dimitris Papailiopoulos</title><link>https://simonwillison.net/2026/Feb/17/dimitris-papailiopoulos/#atom-everything</link><description><![CDATA[<blockquote cite="https://twitter.com/dimitrispapail/status/2023080289828831349"><p>But the intellectually interesting part for me is something else. <strong>I now have something close to a magic box where I throw in a question and a first answer comes back basically for free, in terms of human effort</strong>. Before this, the way I'd explore a new idea is to either clumsily put something together myself or ask a student to run something short for signal, and if it's there, we’d go deeper. That quick signal step, i.e., finding out if a question has any meat to it, is what I can now do without taking up anyone else's time. It’s now between just me, Claude Code, and a few days of GPU time.</p>
<p>I don’t know what this means for how we do research long term. I don’t think anyone does yet. But <strong>the distance between a question and a first answer just got very small</strong>.</p></blockquote>
<p class="cite">&mdash; <a href="https://twitter.com/dimitrispapail/status/2023080289828831349">Dimitris Papailiopoulos</a>, on running research questions though Claude Code</p>

    <p>Tags: <a href="https://simonwillison.net/tags/research">research</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Tue, 17 Feb 2026 14:04:44 +0000</pubDate></item><item><title>Nano Banana Pro diff to webcomic</title><link>https://simonwillison.net/2026/Feb/17/release-notes-webcomic/#atom-everything</link><description><![CDATA[<p>Given the threat of <a href="https://simonwillison.net/tags/cognitive-debt/">cognitive debt</a> brought on by AI-accelerated software development leading to more projects and less deep understanding of how they work and what they actually do, it's interesting to consider artifacts that might be able to help.</p>
<p>Nathan Baschez <a href="https://twitter.com/nbaschez/status/2023501535343509871">on Twitter</a>:</p>
<blockquote>
<p>my current favorite trick for reducing "cognitive debt" (h/t @simonw
) is to ask the LLM to write two versions of the plan:</p>
<ol>
<li>The version for it (highly technical and detailed)</li>
<li>The version for me (an entertaining essay designed to build my intuition)</li>
</ol>
<p>Works great</p>
</blockquote>
<p>This inspired me to try something new. I generated <a href="https://github.com/simonw/showboat/compare/v0.5.0...v0.6.0.diff">the diff</a> between v0.5.0 and v0.6.0 of my Showboat project - which introduced <a href="https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#showboat-remote-publishing">the remote publishing feature</a> - and dumped that into Nano Banana Pro with the prompt:</p>
<blockquote>
<p>Create a webcomic that explains the new feature as clearly and entertainingly as possible</p>
</blockquote>
<p>Here's <a href="https://gemini.google.com/share/cce6da8e5083">what it produced</a>:</p>
<p><img alt="A six-panel comic strip illustrating a tool called &quot;Showboat&quot; for live-streaming document building. Panel 1, titled &quot;THE OLD WAY: Building docs was a lonely voyage. You finished it all before anyone saw it.&quot;, shows a sad bearded man on a wooden boat labeled &quot;THE LOCALHOST&quot; holding papers and saying &quot;Almost done... then I have to export and email the HTML...&quot;. Panel 2, titled &quot;THE UPGRADE: Just set the environment variable!&quot;, shows the same man excitedly plugging in a device with a speech bubble reading &quot;ENV VAR: SHOWBOAT_REMOTE_URL&quot; and the sound effect &quot;*KA-CHUNK!*&quot;. Panel 3, titled &quot;init establishes the uplink and generates a unique UUID beacon.&quot;, shows the man typing at a keyboard with a terminal reading &quot;$ showboat init 'Live Demo'&quot;, a satellite dish transmitting to a floating label &quot;UUID: 550e84...&quot;, and a monitor reading &quot;WAITING FOR STREAM...&quot;. Panel 4, titled &quot;Every note and exec is instantly beamed to the remote viewer!&quot;, shows the man coding with sound effects &quot;*HAMMER!*&quot;, &quot;ZAP!&quot;, &quot;ZAP!&quot;, &quot;BANG!&quot; as red laser beams shoot from a satellite dish to a remote screen displaying &quot;NOTE: Step 1...&quot; and &quot;SUCCESS&quot;. Panel 5, titled &quot;Even image files are teleported in real-time!&quot;, shows a satellite dish firing a cyan beam with the sound effect &quot;*FOOMP!*&quot; toward a monitor displaying a bar chart. Panel 6, titled &quot;You just build. The audience gets the show live.&quot;, shows the man happily working at his boat while a crowd of cheering people watches a projected screen reading &quot;SHOWBOAT LIVE STREAM: Live Demo&quot;, with a label &quot;UUID: 550e84...&quot; and one person in the foreground eating popcorn." src="https://static.simonwillison.net/static/2026/nano-banana-diff.jpg" /></p>
<p>Good enough to publish with the release notes? I don't think so. I'm sharing it here purely to demonstrate the idea. Creating assets like this as a personal tool for thinking about novel ways to explain a feature feels worth exploring further.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/nano-banana">nano-banana</a>, <a href="https://simonwillison.net/tags/gemini">gemini</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/cognitive-debt">cognitive-debt</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/text-to-image">text-to-image</a>, <a href="https://simonwillison.net/tags/showboat">showboat</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a></p>]]></description><pubDate>Tue, 17 Feb 2026 04:51:58 +0000</pubDate></item><item><title>Qwen3.5: Towards Native Multimodal Agents</title><link>https://simonwillison.net/2026/Feb/17/qwen35/#atom-everything</link><description><![CDATA[<p><strong><a href="https://qwen.ai/blog?id=qwen3.5">Qwen3.5: Towards Native Multimodal Agents</a></strong></p>
Alibaba's Qwen just released the first two models in the Qwen 3.5 series - one open weights, one proprietary. Both are multi-modal for vision input.</p>
<p>The open weight one is a Mixture of Experts model called Qwen3.5-397B-A17B. Interesting to see Qwen call out serving efficiency as a benefit of that architecture:</p>
<blockquote>
<p>Built on an innovative hybrid architecture that fuses linear attention (via Gated Delta Networks) with a sparse mixture-of-experts, the model attains remarkable inference efficiency: although it comprises 397 billion total parameters, just 17 billion are activated per forward pass, optimizing both speed and cost without sacrificing capability.</p>
</blockquote>
<p>It's <a href="https://huggingface.co/Qwen/Qwen3.5-397B-A17B">807GB on Hugging Face</a>, and Unsloth have a <a href="https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF">collection of smaller GGUFs</a> ranging in size from 94.2GB 1-bit to 462GB Q8_K_XL.</p>
<p>I got this <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle/">pelican</a> from the <a href="https://openrouter.ai/qwen/qwen3.5-397b-a17b">OpenRouter hosted model</a> (<a href="https://gist.github.com/simonw/625546cf6b371f9c0040e64492943b82">transcript</a>):</p>
<p><img alt="Pelican is quite good although the neck lacks an outline for some reason. Bicycle is very basic with an incomplete frame" src="https://static.simonwillison.net/static/2026/qwen3.5-397b-a17b.png" /></p>
<p>The proprietary hosted model is called Qwen3.5 Plus 2026-02-15, and is a little confusing. Qwen researcher <a href="https://twitter.com/JustinLin610/status/2023340126479569140">Junyang Lin  says</a>:</p>
<blockquote>
<p>Qwen3-Plus is a hosted API version of 397B. As the model natively supports 256K tokens, Qwen3.5-Plus supports 1M token context length. Additionally it supports search and code interpreter, which you can use on Qwen Chat with Auto mode.</p>
</blockquote>
<p>Here's <a href="https://gist.github.com/simonw/9507dd47483f78dc1195117735273e20">its pelican</a>, which is similar in quality to the open weights model:</p>
<p><img alt="Similar quality pelican. The bicycle is taller and has a better frame shape. They are visually quite similar." src="https://static.simonwillison.net/static/2026/qwen3.5-plus-02-15.png" />


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/vision-llms">vision-llms</a>, <a href="https://simonwillison.net/tags/qwen">qwen</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a>, <a href="https://simonwillison.net/tags/openrouter">openrouter</a>, <a href="https://simonwillison.net/tags/ai-in-china">ai-in-china</a></p>]]></description><pubDate>Tue, 17 Feb 2026 04:30:57 +0000</pubDate></item><item><title>Two new Showboat tools: Chartroom and datasette-showboat</title><link>https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#atom-everything</link><description><![CDATA[<p>I <a href="https://simonwillison.net/2026/Feb/10/showboat-and-rodney/">introduced Showboat</a> a week ago - my CLI tool that helps coding agents create Markdown documents that demonstrate the code that they have created. I've been finding new ways to use it on a daily basis, and I've just released two new tools to help get the best out of the Showboat pattern. <a href="https://github.com/simonw/chartroom">Chartroom</a> is a CLI charting tool that works well with Showboat, and <a href="https://github.com/simonw/datasette-showboat">datasette-showboat</a> lets Showboat's new remote publishing feature incrementally push documents to a Datasette instance.</p>

<ul>
  <li><a href="https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#showboat-remote-publishing">Showboat remote publishing</a></li>
  <li><a href="https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#datasette-showboat">datasette-showboat</a></li>
  <li><a href="https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#chartroom">Chartroom</a></li>
  <li><a href="https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#how-i-built-chartroom">How I built Chartroom</a></li>
  <li><a href="https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#the-burgeoning-showboat-ecosystem">The burgeoning Showboat ecosystem</a></li>
</ul>

<h4 id="showboat-remote-publishing">Showboat remote publishing</h4>
<p>I normally use Showboat in Claude Code for web (see <a href="https://simonwillison.net/2026/Feb/16/rodney-claude-code/">note from this morning</a>). I've used it in several different projects in the past few days, each of them with a prompt that looks something like this:</p>
<blockquote>
<p><code>Use "uvx showboat --help" to perform a very thorough investigation of what happens if you use the Python sqlite-chronicle and sqlite-history-json libraries against the same SQLite database table</code></p>
</blockquote>
<p>Here's <a href="https://github.com/simonw/research/blob/main/sqlite-chronicle-vs-history-json/demo.md">the resulting document</a>.</p>
<p>Just telling Claude Code to run <code>uvx showboat --help</code> is enough for it to learn how to use the tool - the <a href="https://github.com/simonw/showboat/blob/main/help.txt">help text</a> is designed to work as a sort of ad-hoc Skill document.</p>
<p>The one catch with this approach is that I can't <em>see</em> the new Showboat document until it's finished. I have to wait for Claude to commit the document plus embedded screenshots and push that to a branch in my GitHub repo - then I can view it through the GitHub interface.</p>
<p>For a while I've been thinking it would be neat to have a remote web server of my own which Claude instances can submit updates to while they are working. Then this morning I realized Showboat might be the ideal mechanism to set that up...</p>
<p>Showboat <a href="https://github.com/simonw/showboat/releases/tag/v0.6.0">v0.6.0</a> adds a new "remote" feature. It's almost invisible to users of the tool itself, instead being configured by an environment variable.</p>
<p>Set a variable like this:</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-k">export</span> SHOWBOAT_REMOTE_URL=https://www.example.com/submit<span class="pl-k">?</span>token=xyz</pre></div>
<p>And every time you run a <code>showboat init</code> or <code>showboat note</code> or <code>showboat exec</code> or <code>showboat image</code> command the resulting document fragments will be POSTed to that API endpoint, in addition to the Showboat Markdown file itself being updated.</p>
<p>There are <a href="https://github.com/simonw/showboat/blob/v0.6.0/README.md#remote-document-streaming">full details in the Showboat README</a> - it's a very simple API format, using regular POST form variables or a multipart form upload for the image attached to <code>showboat image</code>.</p>
<h4 id="datasette-showboat">datasette-showboat</h4>
<p>It's simple enough to build a webapp to receive these updates from Showboat, but I needed one that I could easily deploy and would work well with the rest of my personal ecosystem.</p>
<p>So I had Claude Code write me a Datasette plugin that could act as a Showboat remote endpoint. I actually had this building at the same time as the Showboat remote feature, a neat example of running <a href="https://simonwillison.net/2025/Oct/5/parallel-coding-agents/">parallel agents</a>.</p>
<p><strong><a href="https://github.com/simonw/datasette-showboat">datasette-showboat</a></strong> is a Datasette plugin that adds a <code>/-/showboat</code> endpoint to Datasette for viewing documents and a <code>/-/showboat/receive</code> endpoint for receiving updates from Showboat.</p>
<p>Here's a very quick way to try it out:</p>
<div class="highlight highlight-source-shell"><pre>uvx --with datasette-showboat --prerelease=allow \
  datasette showboat.db --create \
  -s plugins.datasette-showboat.database showboat \
  -s plugins.datasette-showboat.token secret123 \
  --root --secret cookie-secret-123</pre></div>
<p>Click on the sign in as root link that shows up in the console, then navigate to <a href="http://127.0.0.1:8001/-/showboat">http://127.0.0.1:8001/-/showboat</a> to see the interface.</p>
<p>Now set your environment variable to point to this instance:</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-k">export</span> SHOWBOAT_REMOTE_URL=<span class="pl-s"><span class="pl-pds">"</span>http://127.0.0.1:8001/-/showboat/receive?token=secret123<span class="pl-pds">"</span></span></pre></div>
<p>And run Showboat like this:</p>
<div class="highlight highlight-source-shell"><pre>uvx showboat init demo.md <span class="pl-s"><span class="pl-pds">"</span>Showboat Feature Demo<span class="pl-pds">"</span></span></pre></div>
<p>Refresh that page and you should see this:</p>
<p><img src="https://static.simonwillison.net/static/2026/datasette-showboat-documents.jpg" alt="Title: Showboat. Remote viewer for Showboat documents. Showboat Feature Demo 2026-02-17 00:06 · 6 chunks, UUID. To send showboat output to this server, set the SHOWBOAT_REMOTE_URL environment variable: export SHOWBOAT_REMOTE_URL=&quot;http://127.0.0.1:8001/-/showboat/receive?token=your-token&quot;" style="max-width: 100%;" /></p>
<p>Click through to the document, then start Claude Code or Codex or your agent of choice and prompt:</p>
<blockquote>
<p><code>Run 'uvx showboat --help' and then use showboat to add to the existing demo.md document with notes and exec and image to demonstrate the tool - fetch a placekitten for the image demo.</code></p>
</blockquote>
<p>The <code>init</code> command assigns a UUID and title and sends those up to Datasette.</p>
<p><img src="https://static.simonwillison.net/static/2026/datasette-showboat.gif" alt="Animated demo - in the foreground a terminal window runs Claude Code, which executes various Showboat commands. In the background a Firefox window where the Showboat Feature Demo adds notes then some bash commands, then a placekitten image." style="max-width: 100%;" /></p>
<p>The best part of this is that it works in Claude Code for web. Run the plugin on a server somewhere (an exercise left up to the reader - I use <a href="https://fly.io/">Fly.io</a> to host mine) and set that <code>SHOWBOAT_REMOTE_URL</code> environment variable in your Claude environment, then any time you tell it to use Showboat the document it creates will be transmitted to your server and viewable in real time.</p>
<p>I built <a href="https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#rodney-cli-browser-automation-designed-to-work-with-showboat">Rodney</a>, a CLI browser automation tool, specifically to work with Showboat. It makes it easy to have a Showboat document load up web pages, interact with them via clicks or injected JavaScript and captures screenshots to embed in the Showboat document and show the effects.</p>
<p>This is wildly useful for hacking on web interfaces using Claude Code for web, especially when coupled with the new remote publishing feature. I only got this stuff working this morning and I've already had several sessions where Claude Code has published screenshots of its work in progress, which I've then been able to provide feedback on directly in the Claude session while it's still working.</p>
<h3 id="chartroom">Chartroom</h3>
<p>A few days ago I had another idea for a way to extend the Showboat ecosystem: what if Showboat documents could easily include charts?</p>
<p>I sometimes fire up Claude Code for data analysis tasks, often telling it to download a SQLite database and then run queries against it to figure out interesting things from the data.</p>
<p>With a simple CLI tool that produced PNG images I could have Claude use Showboat to build a document with embedded charts to help illustrate its findings.</p>
<p><strong><a href="https://github.com/simonw/chartroom">Chartroom</a></strong> is exactly that. It's effectively a thin wrapper around the excellent <a href="https://matplotlib.org/">matplotlib</a> Python library, designed to be used by coding agents to create charts that can be embedded in Showboat documents.</p>
<p>Here's how to render a simple bar chart:</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-c1">echo</span> <span class="pl-s"><span class="pl-pds">'</span>name,value</span>
<span class="pl-s">Alice,42</span>
<span class="pl-s">Bob,28</span>
<span class="pl-s">Charlie,35</span>
<span class="pl-s">Diana,51</span>
<span class="pl-s">Eve,19<span class="pl-pds">'</span></span> <span class="pl-k">|</span> uvx chartroom bar --csv \
  --title <span class="pl-s"><span class="pl-pds">'</span>Sales by Person<span class="pl-pds">'</span></span> --ylabel <span class="pl-s"><span class="pl-pds">'</span>Sales<span class="pl-pds">'</span></span></pre></div>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/simonw/chartroom/8812afc02e1310e9eddbb56508b06005ff2c0ed5/demo/1f6851ec-2026-02-14.png"><img src="https://raw.githubusercontent.com/simonw/chartroom/8812afc02e1310e9eddbb56508b06005ff2c0ed5/demo/1f6851ec-2026-02-14.png" alt="A chart of those numbers, with a title and y-axis label" style="max-width: 100%;" /></a></p>
<p>It can also do line charts, bar charts, scatter charts, and histograms - as seen in <a href="https://github.com/simonw/chartroom/blob/0.2.1/demo/README.md">this demo document</a> that was built using Showboat.</p>
<p>Chartroom can also generate alt text. If you add <code>-f alt</code> to the above it will output the alt text for the chart instead of the image:</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-c1">echo</span> <span class="pl-s"><span class="pl-pds">'</span>name,value</span>
<span class="pl-s">Alice,42</span>
<span class="pl-s">Bob,28</span>
<span class="pl-s">Charlie,35</span>
<span class="pl-s">Diana,51</span>
<span class="pl-s">Eve,19<span class="pl-pds">'</span></span> <span class="pl-k">|</span> uvx chartroom bar --csv \
  --title <span class="pl-s"><span class="pl-pds">'</span>Sales by Person<span class="pl-pds">'</span></span> --ylabel <span class="pl-s"><span class="pl-pds">'</span>Sales<span class="pl-pds">'</span></span> -f alt</pre></div>
<p>Outputs:</p>
<pre><code>Sales by Person. Bar chart of value by name — Alice: 42, Bob: 28, Charlie: 35, Diana: 51, Eve: 19
</code></pre>
<p>Or you can use <code>-f html</code> or <code>-f markdown</code> to get the image tag with alt text directly:</p>
<div class="highlight highlight-text-md"><pre><span class="pl-s">![</span>Sales by Person. Bar chart of value by name — Alice: 42, Bob: 28, Charlie: 35, Diana: 51, Eve: 19<span class="pl-s">]</span><span class="pl-s">(</span><span class="pl-corl">/Users/simon/chart-7.png</span><span class="pl-s">)</span></pre></div>
<p>I added support for Markdown images with alt text to Showboat in <a href="https://github.com/simonw/showboat/releases/tag/v0.5.0">v0.5.0</a>, to complement this feature of Chartroom.</p>
<p>Finally, Chartroom has support for different <a href="https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html">matplotlib styles</a>. I had Claude build a Showboat document to demonstrate these all in one place - you can see that at <a href="https://github.com/simonw/chartroom/blob/main/demo/styles.md">demo/styles.md</a>.</p>
<h4 id="how-i-built-chartroom">How I built Chartroom</h4>
<p>I started the Chartroom repository with my <a href="https://github.com/simonw/click-app">click-app</a> cookiecutter template, then told a fresh Claude Code for web session:</p>
<blockquote>
<p>We are building a Python CLI tool which uses matplotlib to generate a PNG image containing a chart. It will have multiple sub commands for different chart types, controlled by command line options. Everything you need to know to use it will be available in the single "chartroom --help" output.</p>
<p>It will accept data from files or standard input as CSV or TSV or JSON, similar to how sqlite-utils accepts data - clone simonw/sqlite-utils to /tmp for reference there. Clone matplotlib/matplotlib for reference as well</p>
<p>It will also accept data from --sql path/to/sqlite.db "select ..." which runs in read-only mode</p>
<p>Start by asking clarifying questions - do not use the ask user tool though it is broken - and generate a spec for me to approve</p>
<p>Once approved proceed using red/green TDD running tests with "uv run pytest"</p>
<p>Also while building maintain a demo/README.md document using the "uvx showboat --help" tool - each time you get a new chart type working commit the tests, implementation, root level
README update and a new version of that demo/README.md document with an inline image demo of the new chart type (which should be a UUID image filename managed by the showboat image command and should be stored in the demo/ folder</p>
<p>Make sure "uv build" runs cleanly without complaining about extra directories but also ensure dist/ and uv.lock are in gitignore</p>
</blockquote>
<p>This got most of the work done. You can see the rest <a href="https://github.com/simonw/chartroom/pulls?q=is%3Apr+is%3Aclosed">in the PRs</a> that followed.</p>
<h4 id="the-burgeoning-showboat-ecosystem">The burgeoning Showboat ecosystem</h4>
<p>The Showboat family of tools now consists of <a href="https://github.com/simonw/showboat">Showboat</a> itself, <a href="https://github.com/simonw/rodney">Rodney</a> for browser automation, <a href="https://github.com/simonw/chartroom">Chartroom</a> for charting and <a href="https://github.com/simonw/datasette-showboat">datasette-showboat</a> for streaming remote Showboat documents to Datasette.</p>
<p>I'm enjoying how these tools can operate together based on a very loose set of conventions. If a tool can output a path to an image Showboat can include that image in a document. Any tool that can output text can be used with Showboat.</p>
<p>I'll almost certainly be building more tools that fit this pattern. They're very quick to knock out!</p>
<p>The environment variable mechanism for Showboat's remote streaming is a fun hack too - so far I'm just using it to stream documents somewhere else, but it's effectively a webhook extension mechanism that could likely be used for all sorts of things I haven't thought of yet.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/projects">projects</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/datasette">datasette</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/showboat">showboat</a></p>]]></description><pubDate>Tue, 17 Feb 2026 00:43:45 +0000</pubDate></item><item><title>Rodney and Claude Code for Desktop</title><link>https://simonwillison.net/2026/Feb/16/rodney-claude-code/#atom-everything</link><description><![CDATA[<p>I'm a very heavy user of <a href="https://code.claude.com/docs/en/claude-code-on-the-web">Claude Code on the web</a>, Anthropic's excellent but poorly named cloud version of Claude Code where everything runs in a container environment managed by them, greatly reducing the risk of anything bad happening to a computer I care about.</p>
<p>I don't use the web interface at all (hence my dislike of the name) - I access it exclusively through their native iPhone and Mac desktop apps.</p>
<p>Something I particularly appreciate about the desktop app is that it lets you see images that Claude is "viewing" via its <code>Read /path/to/image</code> tool. Here's what that looks like:</p>
<p><img alt="Screenshot of a Claude Code session in Claude Desktop. Claude says: The debug page looks good - all items listed with titles and descriptions. Now let me check the nav
menu -  Analyzed menu image file - Bash uvx rodney open &quot;http://localhost:8765/&quot; 2&gt;&amp;1 &amp;&amp; uvx rodney click &quot;details.nav-menu summary&quot; 2&gt;&amp;1 &amp;% sleep 0.5 &amp;&amp; uvx rodney screenshot /tmp/menu.png 2&gt;&amp;1 Output reads: Datasette: test, Clicked, /tmp/menu.png - then it says Read /tmp/menu.png and reveals a screenshot of the Datasette interface with the nav menu open, showing only &quot;Debug&quot; and &quot;Log out&quot; options. Claude continues: The menu now has just &quot;Debug&quot; and “Log out&quot; — much cleaner. Both pages look good. Let me clean up the server and run the remaining tests." src="https://static.simonwillison.net/static/2026/rodney-claude-desktop.jpg" /></p>
<p>This means you can get a visual preview of what it's working on while it's working, without waiting for it to push code to GitHub for you to try out yourself later on.</p>
<p>The prompt I used to trigger the above screenshot was:</p>
<blockquote>
<p><code>Run "uvx rodney --help" and then use Rodney to manually test the new pages and menu - look at screenshots from it and check you think they look OK</code></p>
</blockquote>
<p>I designed <a href="https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#rodney-cli-browser-automation-designed-to-work-with-showboat">Rodney</a> to have <a href="https://github.com/simonw/rodney/blob/main/help.txt">--help output</a> that provides everything a coding agent needs to know in order to use the tool.</p>
<p>The Claude iPhone app doesn't display opened images yet, so I <a href="https://twitter.com/simonw/status/2023432616066879606">requested it as a feature</a> just now in a thread on Twitter.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/async-coding-agents">async-coding-agents</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/projects">projects</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a></p>]]></description><pubDate>Mon, 16 Feb 2026 16:38:57 +0000</pubDate></item><item><title>The AI Vampire</title><link>https://simonwillison.net/2026/Feb/15/the-ai-vampire/#atom-everything</link><description><![CDATA[<p><strong><a href="https://steve-yegge.medium.com/the-ai-vampire-eda6e4f07163">The AI Vampire</a></strong></p>
Steve Yegge's take on agent fatigue, and its relationship to burnout.</p>
<blockquote>
<p>Let's pretend you're the only person at your company using AI.</p>
<p>In Scenario A, you decide you're going to impress your employer, and work for 8 hours a day at 10x productivity. You knock it out of the park and make everyone else look terrible by comparison.</p>
<p>In that scenario, your employer captures 100% of the value from <em>you</em> adopting AI. You get nothing, or at any rate, it ain't gonna be 9x your salary. And everyone hates you now.</p>
<p>And you're <em>exhausted.</em> You're tired, Boss. You got nothing for it.</p>
<p>Congrats, you were just drained by a company. I've been drained to the point of burnout several times in my career, even at Google once or twice. But now with AI, it's oh, so much easier.</p>
</blockquote>
<p>Steve reports needing more sleep due to the cognitive burden involved in agentic engineering, and notes that four hours of agent work a day is a more realistic pace:</p>
<blockquote>
<p>I’ve argued that AI has turned us all into Jeff Bezos, by automating the easy work, and leaving us with all the difficult decisions, summaries, and problem-solving. I find that I am only really comfortable working at that pace for short bursts of a few hours once or occasionally twice a day, even with lots of practice.</p>
</blockquote>

    <p><small></small>Via <a href="https://cosocial.ca/@timbray/116076167774984883">Tim Bray</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/steve-yegge">steve-yegge</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/cognitive-debt">cognitive-debt</a></p>]]></description><pubDate>Sun, 15 Feb 2026 23:59:36 +0000</pubDate></item><item><title>Em dash</title><link>https://simonwillison.net/2026/Feb/15/em-dashes/#atom-everything</link><description><![CDATA[<p>I'm occasionally accused of using LLMs to write the content on my blog. I don't do that, and I don't think my writing has much of an LLM smell to it... with one notable exception:</p>
<pre>    <span class="pl-c"># Finally, do em dashes</span>
    <span class="pl-s1">s</span> <span class="pl-c1">=</span> <span class="pl-s1">s</span>.<span class="pl-c1">replace</span>(<span class="pl-s">' - '</span>, <span class="pl-s">u'<span class="pl-cce">\u2014</span>'</span>)</pre>

<p>That code to add em dashes to my posts dates back to <a href="https://github.com/simonw/simonwillisonblog/blob/e6d0327b37debdf820b5cfef4fb7d09a9624cea9/blog/templatetags/entry_tags.py#L145-L146">at least 2015</a> when I ported my blog from an older version of Django (in a long-lost Mercurial repository) and started afresh on GitHub.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/typography">typography</a>, <a href="https://simonwillison.net/tags/blogging">blogging</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/python">python</a></p>]]></description><pubDate>Sun, 15 Feb 2026 21:40:46 +0000</pubDate></item><item><title>Deep Blue</title><link>https://simonwillison.net/2026/Feb/15/deep-blue/#atom-everything</link><description><![CDATA[<p>We coined a new term on the <a href="https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/">Oxide and Friends podcast</a> last month (primary credit to Adam Leventhal) covering the sense of psychological ennui leading into existential dread that many software developers are feeling thanks to the encroachment of generative AI into their field of work.</p>
<p>We're calling it <strong>Deep Blue</strong>.</p>
<p>You can listen to it being coined in real time <a href="https://www.youtube.com/watch?v=lVDhQMiAbR8&amp;t=2835s">from 47:15 in the episode</a>. I've included <a href="https://simonwillison.net/2026/Feb/15/deep-blue/#transcript">a transcript below</a>.</p>
<p>Deep Blue is a very real issue.</p>
<p>Becoming a professional software engineer is <em>hard</em>. Getting good enough for people to pay you money to write software takes years of dedicated work. The rewards are significant: this is a well compensated career which opens up a lot of great opportunities.</p>
<p>It's also a career that's mostly free from gatekeepers and expensive prerequisites. You don't need an expensive degree or accreditation. A laptop, an internet connection and a lot of time and curiosity is enough to get you started.</p>
<p>And it rewards the nerds! Spending your teenage years tinkering with computers turned out to be a very smart investment in your future.</p>
<p>The idea that this could all be stripped away by a chatbot is <em>deeply</em> upsetting.</p>
<p>I've seen signs of Deep Blue in most of the online communities I spend time in. I've even faced accusations from my peers that I am actively harming their future careers through my work helping people understand how well AI-assisted programming can work.</p>
<p>I think this is an issue which is causing genuine mental anguish for a lot of people in our community. Giving it a name makes it easier for us to have conversations about it.</p>
<h4 id="my-experiences-of-deep-blue">My experiences of Deep Blue</h4>
<p>I distinctly remember my first experience of Deep Blue. For me it was triggered by ChatGPT Code Interpreter back in early 2023.</p>
<p>My primary project is <a href="https://datasette.io/">Datasette</a>, an ecosystem of open source tools for telling stories with data. I had dedicated myself to the challenge of helping people (initially focusing on journalists) clean up, analyze and find meaning in data, in all sorts of shapes and sizes.</p>
<p>I expected I would need to build a lot of software for this! It felt like a challenge that could keep me happily engaged for many years to come.</p>
<p>Then I tried uploading a CSV file of <a href="https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-2018-to-Present/wg3w-h783/about_data">San Francisco Police Department Incident Reports</a> - hundreds of thousands of rows - to ChatGPT Code Interpreter and... it did every piece of data cleanup and analysis I had on my napkin roadmap for the next few years with a couple of prompts.</p>
<p>It even converted the data into a neatly normalized SQLite database and let me download the result!</p>
<p>I remember having two competing thoughts in parallel.</p>
<p>On the one hand, as somebody who wants journalists to be able to do more with data, this felt like a <em>huge</em> breakthrough. Imagine giving every journalist in the world an on-demand analyst who could help them tackle any data question they could think of!</p>
<p>But on the other hand... <em>what was I even for</em>? My confidence in the value of my own projects took a painful hit. Was the path I'd chosen for myself suddenly a dead end?</p>
<p>I've had some further pangs of Deep Blue just in the past few weeks, thanks to the Claude Opus 4.5/4.6 and GPT-5.2/5.3 coding agent effect. As many other people are also observing, the latest generation of coding agents, given the right prompts, really can churn away for a few minutes to several hours and produce working, documented and fully tested software that exactly matches the criteria they were given.</p>
<p>"The code they write isn't any good" doesn't really cut it any more.</p>
<h4 id="transcript">A lightly edited transcript</h4>
<blockquote>
<p><strong>Bryan</strong>: I think that we're going to see a real problem with AI induced ennui where software engineers in particular get listless because the AI can do anything. Simon, what do you think about that?</p>
<p><strong>Simon</strong>: Definitely. Anyone who's paying close attention to coding agents is feeling some of that already. There's an extent where you sort of get over it when you realize that you're still useful, even though your ability to memorize the syntax of program languages is completely irrelevant now.</p>
<p>Something I see a lot of is people out there who are having existential crises and are very, very unhappy because they're like, "I dedicated my career to learning this thing and now it just does it. What am I even for?". I will very happily try and convince those people that they are for a whole bunch of things and that none of that experience they've accumulated has gone to waste, but psychologically it's a difficult time for software engineers.</p>
<p>[...]</p>
<p><strong>Bryan</strong>: Okay, so I'm going to predict that we name that. Whatever that is, we have a name for that kind of feeling and that kind of, whether you want to call it a blueness or a loss of purpose, and that we're kind of trying to address it collectively in a directed way.</p>
<p><strong>Adam</strong>: Okay, this is your big moment. Pick the name. If you call your shot from here, this is you pointing to the stands. You know, I – Like deep blue, you know.</p>
<p><strong>Bryan</strong>: Yeah, deep blue. I like that. I like deep blue. Deep blue. Oh, did you walk me into that, you bastard? You just blew out the candles on my birthday cake.</p>
<p>It wasn't my big moment at all. That was your big moment. No, that is, Adam, that is very good. That is deep blue.</p>
<p><strong>Simon</strong>: All of the chess players and the Go players went through this a decade ago and they have come out stronger.</p>
</blockquote>
<p>Turns out it was more than a decade ago: <a href="https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov">Deep Blue defeated Garry Kasparov in 1997</a>.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/definitions">definitions</a>, <a href="https://simonwillison.net/tags/careers">careers</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/oxide">oxide</a>, <a href="https://simonwillison.net/tags/bryan-cantrill">bryan-cantrill</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a></p>]]></description><pubDate>Sun, 15 Feb 2026 21:06:44 +0000</pubDate></item><item><title>Gwtar: a static efficient single-file HTML format</title><link>https://simonwillison.net/2026/Feb/15/gwtar/#atom-everything</link><description><![CDATA[<p><strong><a href="https://gwern.net/gwtar">Gwtar: a static efficient single-file HTML format</a></strong></p>
Fascinating new project from Gwern Branwen and Said Achmiz that targets the challenge of combining large numbers of assets into a single archived HTML file without that file being inconvenient to view in a browser.</p>
<p>The key trick it uses is to fire <a href="https://developer.mozilla.org/en-US/docs/Web/API/Window/stop">window.stop()</a> early in the page to prevent the browser from downloading the whole thing, then following that call with inline tar uncompressed content.</p>
<p>It can then make HTTP range requests to fetch content from that tar data on-demand when it is needed by the page.</p>
<p>The JavaScript that has already loaded rewrites asset URLs to point to <code>https://localhost/</code> purely so that they will fail to load. Then it uses a <a href="https://developer.mozilla.org/en-US/docs/Web/API/PerformanceObserver">PerformanceObserver</a> to catch those attempted loads:</p>
<pre><code>let perfObserver = new PerformanceObserver((entryList, observer) =&gt; {
    resourceURLStringsHandler(entryList.getEntries().map(entry =&gt; entry.name));
});
perfObserver.observe({ entryTypes: [ "resource" ] });
</code></pre>
<p>That <code>resourceURLStringsHandler</code> callback finds the resource if it is already loaded or fetches it with an HTTP range request otherwise and then inserts the resource in the right place using a <code>blob:</code> URL.</p>
<p>Here's what the <code>window.stop()</code> portion of the document looks like if you view the source:</p>
<p><img alt="Screenshot of a macOS terminal window titled &quot;gw — more big.html — 123×46&quot; showing the source code of a gwtar (self-extracting HTML archive) file. The visible code includes JavaScript with requestIdleCallback(getMainPageHTML);, a &lt;noscript&gt; block with warnings: a &quot;js-disabled-warning&quot; stating &quot;This HTML page requires JavaScript to be enabled to render, as it is a self-extracting gwtar HTML file,&quot; a description of gwtar as &quot;a portable self-contained standalone HTML file which is designed to nevertheless support efficient lazy loading of all assets such as large media files,&quot; with a link to https://gwern.net/gwtar, a &quot;local-file-warning&quot; with a shell command perl -ne'print $_ if $x; $x=1 if /&lt;!-- GWTAR END/' &amp;lt; foo.gwtar.html | tar --extract, and a &quot;server-fail-warning&quot; about misconfigured servers. Below the HTML closing tags and &lt;!-- GWTAR END comment is binary tar archive data with the filename 2010-02-brianmoriarty-thesecretofpsalm46.html, showing null-padded tar header fields including ustar^@00root and octal size/permission values. At the bottom, a SingleFile metadata comment shows url: https://web.archive.org/web/20230512001411/http://ludix.com/moriarty/psalm46.html and saved date: Sat Jan 17 2026 19:26:49 GMT-0800 (Pacific Standard Time)." src="https://static.simonwillison.net/static/2026/gwtar.jpg" /></p>
<p>Amusingly for an archive format it doesn't actually work if you open the file directly on your own computer. Here's what you see if you try to do that:</p>
<blockquote>
<p>You are seeing this message, instead of the page you should be seeing, because <code>gwtar</code> files <strong>cannot be opened locally</strong> (due to web browser security restrictions).</p>
<p>To open this page on your computer, use the following shell command:</p>
<p><code>perl -ne'print $_ if $x; $x=1 if /&lt;!-- GWTAR END/' &lt; foo.gwtar.html | tar --extract</code></p>
<p>Then open the file <code>foo.html</code> in any web browser.</p>
</blockquote>

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=47024506">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/archiving">archiving</a>, <a href="https://simonwillison.net/tags/html">html</a>, <a href="https://simonwillison.net/tags/javascript">javascript</a></p>]]></description><pubDate>Sun, 15 Feb 2026 18:26:08 +0000</pubDate></item><item><title>Three months of OpenClaw</title><link>https://simonwillison.net/2026/Feb/15/openclaw/#atom-everything</link><description><![CDATA[<p>It's wild that the first commit to OpenClaw was <a href="https://github.com/openclaw/openclaw/commit/f6dd362d39b8e30bd79ef7560aab9575712ccc11">on November 25th 2025</a>, and less than three months later it's hit 10,000 commits from 600 contributors, attracted 196,000 GitHub stars and sort-of been featured in an extremely vague <a href="https://www.youtube.com/watch?v=n7I-D4YXbzg">Super Bowl commercial for AI.com</a>.</p>
<p>Quoting AI.com founder <a href="https://twitter.com/kris/status/2020663711015514399">Kris Marszalek</a>, purchaser of the <a href="https://www.theregister.com/2026/02/09/70m_aicom_domain_sale/">most expensive domain in history</a> for $70m:</p>
<blockquote>
<p>ai.com is the world’s first easy-to-use and secure implementation of OpenClaw, the open source agent framework that went viral two weeks ago; we made it easy to use without any technical skills, while hardening security to keep your data safe.</p>
</blockquote>
<p>Looks like vaporware to me - all you can do right now is reserve a handle - but it's still remarkable to see an open source project get to <em>that</em> level of hype in such a short space of time.</p>
<p><strong>Update</strong>: OpenClaw creator Peter Steinberger <a href="https://steipete.me/posts/2026/openclaw">just announced</a> that he's joining OpenAI and plans to transfer ownership of OpenClaw to a new independent foundation.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/ai-agents">ai-agents</a>, <a href="https://simonwillison.net/tags/openclaw">openclaw</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/open-source">open-source</a>, <a href="https://simonwillison.net/tags/domains">domains</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/peter-steinberger">peter-steinberger</a></p>]]></description><pubDate>Sun, 15 Feb 2026 17:23:28 +0000</pubDate></item><item><title>Quoting Eric Meyer</title><link>https://simonwillison.net/2026/Feb/15/eric-meyer/#atom-everything</link><description><![CDATA[<blockquote cite="https://mastodon.social/@Meyerweb/116065151451468199"><p>I saw yet another “CSS is a massively bloated mess” whine and I’m like.  My dude.  My brother in Chromium.  It is trying as hard as it can to express the totality of visual presentation and layout design and typography and animation and digital interactivity and a few other things in a human-readable text format.  It’s not bloated, it’s fantastically ambitious.  Its reach is greater than most of us can hope to grasp.  Put some <em>respect</em> on its <em>name</em>.</p></blockquote>
<p class="cite">&mdash; <a href="https://mastodon.social/@Meyerweb/116065151451468199">Eric Meyer</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/css">css</a>, <a href="https://simonwillison.net/tags/web-standards">web-standards</a>, <a href="https://simonwillison.net/tags/eric-meyer">eric-meyer</a></p>]]></description><pubDate>Sun, 15 Feb 2026 13:36:20 +0000</pubDate></item><item><title>How Generative and Agentic AI Shift Concern from Technical Debt to Cognitive Debt</title><link>https://simonwillison.net/2026/Feb/15/cognitive-debt/#atom-everything</link><description><![CDATA[<p><strong><a href="https://margaretstorey.com/blog/2026/02/09/cognitive-debt/">How Generative and Agentic AI Shift Concern from Technical Debt to Cognitive Debt</a></strong></p>
This piece by Margaret-Anne Storey is the best explanation of the term <strong>cognitive debt</strong> I've seen so far.</p>
<blockquote>
<p><em>Cognitive debt</em>, a term gaining <a href="https://www.media.mit.edu/publications/your-brain-on-chatgpt/">traction</a> recently, instead communicates the notion that the debt compounded from going fast lives in the brains of the developers and affects their lived experiences and abilities to “go fast” or to make changes. Even if AI agents produce code that could be easy to understand, the humans involved may have simply lost the plot and may not understand what the program is supposed to do, how their intentions were implemented, or how to possibly change it.</p>
</blockquote>
<p>Margaret-Anne expands on this further with an anecdote about a student team she coached:</p>
<blockquote>
<p>But by weeks 7 or 8, one team hit a wall. They could no longer make even simple changes without breaking something unexpected. When I met with them, the team initially blamed technical debt: messy code, poor architecture, hurried implementations. But as we dug deeper, the real problem emerged: no one on the team could explain why certain design decisions had been made or how different parts of the system were supposed to work together. The code might have been messy, but the bigger issue was that the theory of the system, their shared understanding, had fragmented or disappeared entirely. They had accumulated cognitive debt faster than technical debt, and it paralyzed them.</p>
</blockquote>
<p>I've experienced this myself on some of my more ambitious vibe-code-adjacent projects. I've been experimenting with prompting entire new features into existence without reviewing their implementations and, while it works surprisingly well, I've found myself getting lost in my own projects.</p>
<p>I no longer have a firm mental model of what they can do and how they work, which means each additional feature becomes harder to reason about, eventually leading me to lose the ability to make confident decisions about where to go next.

    <p><small></small>Via <a href="https://martinfowler.com/fragments/2026-02-13.html">Martin Fowler</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/definitions">definitions</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a>, <a href="https://simonwillison.net/tags/cognitive-debt">cognitive-debt</a></p>]]></description><pubDate>Sun, 15 Feb 2026 05:20:11 +0000</pubDate></item><item><title>Launching Interop 2026</title><link>https://simonwillison.net/2026/Feb/15/interop-2026/#atom-everything</link><description><![CDATA[<p><strong><a href="https://hacks.mozilla.org/2026/02/launching-interop-2026/">Launching Interop 2026</a></strong></p>
Jake Archibald reports on Interop 2026, the initiative between Apple, Google, Igalia, Microsoft, and Mozilla to collaborate on ensuring a targeted set of web platform features reach cross-browser parity over the course of the year.</p>
<p>I hadn't realized how influential and successful the Interop series has been. It started back in 2021 as <a href="https://web.dev/blog/compat2021">Compat 2021</a> before being rebranded to Interop <a href="https://blogs.windows.com/msedgedev/2022/03/03/microsoft-edge-and-interop-2022/">in 2022</a>.</p>
<p>The dashboards for each year can be seen here, and they demonstrate how wildly effective the program has been: <a href="https://wpt.fyi/interop-2021">2021</a>, <a href="https://wpt.fyi/interop-2022">2022</a>, <a href="https://wpt.fyi/interop-2023">2023</a>, <a href="https://wpt.fyi/interop-2024">2024</a>, <a href="https://wpt.fyi/interop-2025">2025</a>, <a href="https://wpt.fyi/interop-2026">2026</a>.</p>
<p>Here's the progress chart for 2025, which shows every browser vendor racing towards a 95%+ score by the end of the year:</p>
<p><img alt="Line chart showing Interop 2025 browser compatibility scores over the year (Jan–Dec) for Chrome, Edge, Firefox, Safari, and Interop. Y-axis ranges from 0% to 100%. Chrome (yellow) and Edge (green) lead, starting around 80% and reaching near 100% by Dec. Firefox (orange) starts around 48% and climbs to ~98%. Safari (blue) starts around 45% and reaches ~96%. The Interop line (dark green/black) starts lowest around 29% and rises to ~95% by Dec. All browsers converge near 95–100% by year's end." src="https://static.simonwillison.net/static/2026/interop-2025.jpg" /></p>
<p>The feature I'm most excited about in 2026 is <a href="https://developer.mozilla.org/docs/Web/API/View_Transition_API/Using#basic_mpa_view_transition">Cross-document View Transitions</a>, building on the successful 2025 target of <a href="https://developer.mozilla.org/docs/Web/API/View_Transition_API/Using">Same-Document View Transitions</a>. This will provide fancy SPA-style transitions between pages on websites with no JavaScript at all.</p>
<p>As a keen WebAssembly tinkerer I'm also intrigued by this one:</p>
<blockquote>
<p><a href="https://github.com/WebAssembly/js-promise-integration/blob/main/proposals/js-promise-integration/Overview.md">JavaScript Promise Integration for Wasm</a> allows WebAssembly to asynchronously 'suspend', waiting on the result of an external promise. This simplifies the compilation of languages like C/C++ which expect APIs to run synchronously.</p>
</blockquote>


    <p>Tags: <a href="https://simonwillison.net/tags/browsers">browsers</a>, <a href="https://simonwillison.net/tags/css">css</a>, <a href="https://simonwillison.net/tags/javascript">javascript</a>, <a href="https://simonwillison.net/tags/web-standards">web-standards</a>, <a href="https://simonwillison.net/tags/webassembly">webassembly</a>, <a href="https://simonwillison.net/tags/jake-archibald">jake-archibald</a></p>]]></description><pubDate>Sun, 15 Feb 2026 04:33:22 +0000</pubDate></item><item><title>Quoting Boris Cherny</title><link>https://simonwillison.net/2026/Feb/14/boris/#atom-everything</link><description><![CDATA[<blockquote cite="https://twitter.com/bcherny/status/2022762422302576970"><p>Someone has to prompt the Claudes, talk to customers, coordinate with other teams, decide what to build next. Engineering is changing and great engineers are more important than ever.</p></blockquote>
<p class="cite">&mdash; <a href="https://twitter.com/bcherny/status/2022762422302576970">Boris Cherny</a>, Claude Code creator, on why Anthropic are still hiring developers</p>

    <p>Tags: <a href="https://simonwillison.net/tags/careers">careers</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a></p>]]></description><pubDate>Sat, 14 Feb 2026 23:59:09 +0000</pubDate></item><item><title>Quoting Thoughtworks</title><link>https://simonwillison.net/2026/Feb/14/thoughtworks/#atom-everything</link><description><![CDATA[<blockquote cite="https://www.thoughtworks.com/content/dam/thoughtworks/documents/report/tw_future%20_of_software_development_retreat_%20key_takeaways.pdf"><p>The retreat challenged the narrative that AI eliminates the need for junior developers. Juniors are more profitable than they have ever been. AI tools get them past the awkward initial net-negative phase faster. They serve as a call option on future productivity. And they are better at AI tools than senior engineers, having never developed the habits and assumptions that slow adoption.</p>
<p>The real concern is mid-level engineers who came up during the decade-long hiring boom and may not have developed the fundamentals needed to thrive in the new environment. This population represents the bulk of the industry by volume, and retraining them is genuinely difficult. The retreat discussed whether apprenticeship models, rotation programs and lifelong learning structures could address this gap, but acknowledged that no organization has solved it yet.</p></blockquote>
<p class="cite">&mdash; <a href="https://www.thoughtworks.com/content/dam/thoughtworks/documents/report/tw_future%20_of_software_development_retreat_%20key_takeaways.pdf">Thoughtworks</a>, findings from a retreat concerning "the future of software engineering", conducted under Chatham House rules</p>

    <p>Tags: <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/careers">careers</a>, <a href="https://simonwillison.net/tags/ai">ai</a></p>]]></description><pubDate>Sat, 14 Feb 2026 04:54:41 +0000</pubDate></item><item><title>Anthropic&apos;s public benefit mission</title><link>https://simonwillison.net/2026/Feb/13/anthropic-public-benefit-mission/#atom-everything</link><description><![CDATA[<p>Someone <a href="https://news.ycombinator.com/item?id=47008560#47008978">asked</a> if there was an Anthropic equivalent to <a href="https://simonwillison.net/2026/Feb/13/openai-mission-statement/">OpenAI's IRS mission statements over time</a>.</p>
<p>Anthropic are a "public benefit corporation" but not a non-profit, so they don't have the same requirements to file public documents with the IRS every year.</p>
<p>But when I asked Claude it ran a search and dug up this <a href="https://drive.google.com/drive/folders/1ImqXYv9_H2FTNAujZfu3EPtYFD4xIlHJ">Google Drive folder</a> where Zach Stein-Perlman shared Certificate of Incorporation documents he <a href="https://ailabwatch.substack.com/p/anthropics-certificate-of-incorporation">obtained from the State of Delaware</a>!</p>
<p>Anthropic's are much less interesting that OpenAI's. The earliest document from 2021 states:</p>
<blockquote>
<p>The specific public benefit that the Corporation will promote is to responsibly develop and maintain advanced Al for the cultural, social and technological improvement of humanity.</p>
</blockquote>
<p>Every subsequent document up to 2024 uses an updated version which says:</p>
<blockquote>
<p>The specific public benefit that the Corporation will promote is to responsibly develop and maintain advanced AI for the long term benefit of humanity.</p>
</blockquote>

    <p>Tags: <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/ai">ai</a></p>]]></description><pubDate>Fri, 13 Feb 2026 23:59:51 +0000</pubDate></item><item><title>The evolution of OpenAI&apos;s mission statement</title><link>https://simonwillison.net/2026/Feb/13/openai-mission-statement/#atom-everything</link><description><![CDATA[<p>As a USA <a href="https://en.wikipedia.org/wiki/501(c)(3)_organization">501(c)(3)</a> the OpenAI non-profit has to file a tax return each year with the IRS. One of the required fields on that tax return is to "Briefly describe the organization’s mission or most significant activities" - this has actual legal weight to it as the IRS can use it to evaluate if the organization is sticking to its mission and deserves to maintain its non-profit tax-exempt status.</p>
<p>You can browse OpenAI's <a href="https://projects.propublica.org/nonprofits/organizations/810861541">tax filings by year</a> on ProPublica's excellent <a href="https://projects.propublica.org/nonprofits/">Nonprofit Explorer</a>.</p>
<p>I went through and extracted that mission statement for 2016 through 2024, then had Claude Code <a href="https://gisthost.github.io/?7a569df89f43f390bccc2c5517718b49/index.html">help me</a> fake the commit dates to turn it into a git repository and share that as a Gist - which means that Gist's <a href="https://gist.github.com/simonw/e36f0e5ef4a86881d145083f759bcf25/revisions">revisions page</a> shows every edit they've made since they started filing their taxes!</p>
<p>It's really interesting seeing what they've changed over time.</p>
<p>The original 2016 mission reads as follows (and yes, the apostrophe in "OpenAIs" is missing <a href="https://projects.propublica.org/nonprofits/organizations/810861541/201703459349300445/full">in the original</a>):</p>
<blockquote>
<p>OpenAIs goal is to advance digital intelligence in the way that is most likely to benefit humanity as a whole, unconstrained by a need to generate financial return. We think that artificial intelligence technology will help shape the 21st century, and we want to help the world build safe AI technology and ensure that AI's benefits are as widely and evenly distributed as possible. Were trying to build AI as part of a larger community, and we want to openly share our plans and capabilities along the way.</p>
</blockquote>
<p>In 2018 they dropped the part about "trying to build AI as part of a larger community, and we want to openly share our plans and capabilities along the way."</p>
<p><img src="https://static.simonwillison.net/static/2026/mission-3.jpg" alt="Git diff showing the 2018 revision deleting the final two sentences: &quot;Were trying to build AI as part of a larger community, and we want to openly share our plans and capabilities along the way.&quot;" style="max-width: 100%;" /></p>
<p>In 2020 they dropped the words "as a whole" from "benefit humanity as a whole". They're still "unconstrained by a need to generate financial return" though.</p>
<p><img src="https://static.simonwillison.net/static/2026/mission-5.jpg" alt="Git diff showing the 2020 revision dropping &quot;as a whole&quot; from &quot;benefit humanity as a whole&quot; and changing &quot;We think&quot; to &quot;OpenAI believes&quot;" style="max-width: 100%;" /></p>
<p>Some interesting changes in 2021. They're still unconstrained by a need to generate financial return, but here we have the first reference to "general-purpose artificial intelligence" (replacing "digital intelligence"). They're more confident too: it's not "most likely to benefit humanity", it's just "benefits humanity".</p>
<p>They previously wanted to "help the world build safe AI technology", but now they're going to do that themselves: "the companys goal is to develop and responsibly deploy safe AI technology".</p>
<p><img src="https://static.simonwillison.net/static/2026/mission-6.jpg" alt="Git diff showing the 2021 revision replacing &quot;goal is to advance digital intelligence&quot; with &quot;mission is to build general-purpose artificial intelligence&quot;, changing &quot;most likely to benefit&quot; to just &quot;benefits&quot;, and replacing &quot;help the world build safe AI technology&quot; with &quot;the companys goal is to develop and responsibly deploy safe AI technology&quot;" style="max-width: 100%;" /></p>
<p>2022 only changed one significant word: they added "safely" to "build ... (AI) that safely benefits humanity". They're still unconstrained by those financial returns!</p>
<p><img src="https://static.simonwillison.net/static/2026/mission-7.jpg" alt="Git diff showing the 2022 revision adding &quot;(AI)&quot; and the word &quot;safely&quot; so it now reads &quot;that safely benefits humanity&quot;, and changing &quot;the companys&quot; to &quot;our&quot;" style="max-width: 100%;" /></p>
<p>No changes in 2023... but then in 2024 they deleted almost the entire thing, reducing it to simply:</p>
<blockquote>
<p>OpenAIs mission is to ensure that artificial general intelligence benefits all of humanity.</p>
</blockquote>
<p>They've expanded "humanity" to "all of humanity", but there's no mention of safety any more and I guess they can finally start focusing on that need to generate financial returns!</p>
<p><img src="https://static.simonwillison.net/static/2026/mission-9.jpg" alt="Git diff showing the 2024 revision deleting the entire multi-sentence mission statement and replacing it with just &quot;OpenAIs mission is to ensure that artificial general intelligence benefits all of humanity.&quot;" style="max-width: 100%;" /></p>

<p><strong>Update</strong>: I found loosely equivalent but much less interesting documents <a href="https://simonwillison.net/2026/Feb/13/anthropic-public-benefit-mission/">from Anthropic</a>.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/propublica">propublica</a></p>]]></description><pubDate>Fri, 13 Feb 2026 23:38:29 +0000</pubDate></item><item><title>Introducing GPT‑5.3‑Codex‑Spark</title><link>https://simonwillison.net/2026/Feb/12/codex-spark/#atom-everything</link><description><![CDATA[<p><strong><a href="https://openai.com/index/introducing-gpt-5-3-codex-spark/">Introducing GPT‑5.3‑Codex‑Spark</a></strong></p>
OpenAI announced a partnership with Cerebras <a href="https://openai.com/index/cerebras-partnership/">on January 14th</a>. Four weeks later they're already launching the first integration, "an ultra-fast model for real-time coding in Codex".</p>
<p>Despite being named GPT-5.3-Codex-Spark it's not purely an accelerated alternative to GPT-5.3-Codex - the blog post calls it "a smaller version of GPT‑5.3-Codex" and clarifies that "at launch, Codex-Spark has a 128k context window and is text-only."</p>
<p>I had some preview access to this model and I can confirm that it's significantly faster than their other models.</p>
<p>Here's what that speed looks like running in Codex CLI:</p>
<div style="max-width: 100%;">
    <video 
        controls 
        preload="none"
        poster="https://static.simonwillison.net/static/2026/gpt-5.3-codex-spark-medium-last.jpg"
        style="width: 100%; height: auto;">
        <source src="https://static.simonwillison.net/static/2026/gpt-5.3-codex-spark-medium.mp4" type="video/mp4">
    </video>
</div>

<p>That was the "Generate an SVG of a pelican riding a bicycle" prompt - here's the rendered result:</p>
<p><img alt="Whimsical flat illustration of an orange duck merged with a bicycle, where the duck's body forms the seat and frame area while its head extends forward over the handlebars, set against a simple light blue sky and green grass background." src="https://static.simonwillison.net/static/2026/gpt-5.3-codex-spark-pelican.png" /></p>
<p>Compare that to the speed of regular GPT-5.3 Codex medium:</p>
<div style="max-width: 100%;">
    <video 
        controls 
        preload="none"
        poster="https://static.simonwillison.net/static/2026/gpt-5.3-codex-medium-last.jpg"
        style="width: 100%; height: auto;">
        <source src="https://static.simonwillison.net/static/2026/gpt-5.3-codex-medium.mp4" type="video/mp4">
    </video>
</div>

<p>Significantly slower, but the pelican is a lot better:</p>
<p><img alt="Whimsical flat illustration of a white pelican riding a dark blue bicycle at speed, with motion lines behind it, its long orange beak streaming back in the wind, set against a light blue sky and green grass background." src="https://static.simonwillison.net/static/2026/gpt-5.3-codex-pelican.png" /></p>
<p>What's interesting about this model isn't the quality though, it's the <em>speed</em>. When a model responds this fast you can stay in flow state and iterate with the model much more productively.</p>
<p>I showed a demo of Cerebras running Llama 3.1 70 B at 2,000 tokens/second against Val Town <a href="https://simonwillison.net/2024/Oct/31/cerebras-coder/">back in October 2024</a>. OpenAI claim 1,000 tokens/second for their new model, and I expect it will prove to be a ferociously useful partner for hands-on iterative coding sessions.</p>
<p>It's not yet clear what the pricing will look like for this new model.


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/cerebras">cerebras</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a>, <a href="https://simonwillison.net/tags/codex-cli">codex-cli</a>, <a href="https://simonwillison.net/tags/inference-speed">inference-speed</a></p>]]></description><pubDate>Thu, 12 Feb 2026 21:16:07 +0000</pubDate></item><item><title>Quoting Anthropic</title><link>https://simonwillison.net/2026/Feb/12/anthropic/#atom-everything</link><description><![CDATA[<blockquote cite="https://www.anthropic.com/news/anthropic-raises-30-billion-series-g-funding-380-billion-post-money-valuation"><p>Claude Code was made available to the general public in May 2025. Today, Claude Code’s run-rate revenue has grown to over $2.5 billion; this figure has more than doubled since the beginning of 2026. The number of weekly active Claude Code users has also doubled since January 1 [<em>six weeks ago</em>].</p></blockquote>
<p class="cite">&mdash; <a href="https://www.anthropic.com/news/anthropic-raises-30-billion-series-g-funding-380-billion-post-money-valuation">Anthropic</a>, announcing their $30 billion series G</p>

    <p>Tags: <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/ai-agents">ai-agents</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Thu, 12 Feb 2026 20:22:14 +0000</pubDate></item><item><title>Covering electricity price increases from our data centers</title><link>https://simonwillison.net/2026/Feb/12/covering-electricity-price-increases/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.anthropic.com/news/covering-electricity-price-increases">Covering electricity price increases from our data centers</a></strong></p>
One of the sub-threads of the AI energy usage discourse has been the impact new data centers have on the cost of electricity to nearby residents. Here's <a href="https://www.bloomberg.com/graphics/2025-ai-data-centers-electricity-prices/">detailed analysis from Bloomberg in September</a> reporting "Wholesale electricity costs as much as 267% more than it did five years ago in areas near data centers".</p>
<p>Anthropic appear to be taking on this aspect of the problem directly, promising to cover 100% of necessary grid upgrade costs and also saying:</p>
<blockquote>
<p>We will work to bring net-new power generation online to match our data centers’ electricity needs. Where new generation isn’t online, we’ll work with utilities and external experts to estimate and cover demand-driven price effects from our data centers.</p>
</blockquote>
<p>I look forward to genuine energy industry experts picking this apart to judge if it will actually have the claimed impact on consumers.</p>
<p>As always, I remain frustrated at the refusal of the major AI labs to fully quantify their energy usage. The best data we've had on this still comes from Mistral's report <a href="https://simonwillison.net/2025/Jul/22/mistral-environmental-standard/">last July</a> and even that lacked key data such as the breakdown between energy usage for training vs inference.

    <p><small></small>Via <a href="https://x.com/anthropicai/status/2021694494215901314">@anthropicai</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/ai-energy-usage">ai-energy-usage</a></p>]]></description><pubDate>Thu, 12 Feb 2026 20:01:23 +0000</pubDate></item><item><title>Gemini 3 Deep Think</title><link>https://simonwillison.net/2026/Feb/12/gemini-3-deep-think/#atom-everything</link><description><![CDATA[<p><strong><a href="https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-deep-think/">Gemini 3 Deep Think</a></strong></p>
New from Google. They say it's "built to push the frontier of intelligence and solve modern challenges across science, research, and engineering".</p>
<p>It drew me a <em>really good</em> <a href="https://gist.github.com/simonw/7e317ebb5cf8e75b2fcec4d0694a8199">SVG of a pelican riding a bicycle</a>! I think this is the best one I've seen so far - here's <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle/">my previous collection</a>.</p>
<p><img alt="This alt text also generated by Gemini 3 Deep Think: A highly detailed, colorful, flat vector illustration with thick dark blue outlines depicting a stylized white pelican riding a bright cyan blue bicycle from left to right across a sandy beige beach with white speed lines indicating forward motion. The pelican features a light blue eye, a pink cheek blush, a massive bill with a vertical gradient from yellow to orange, a backward magenta cap with a cyan brim and a small yellow top button, and a matching magenta scarf blowing backward in the wind. Its white wing, accented with a grey mid-section and dark blue feather tips, reaches forward to grip the handlebars, while its long tan leg and orange foot press down on an orange pedal. Attached to the front handlebars is a white wire basket carrying a bright blue cartoon fish that is pointing upwards and forwards. The bicycle itself has a cyan frame, dark blue tires, striking neon pink inner rims, cyan spokes, a white front chainring, and a dark blue chain. Behind the pelican, a grey trapezoidal pier extends from the sand toward a horizontal band of deep blue ocean water detailed with light cyan wavy lines. A massive, solid yellow-orange semi-circle sun sits on the horizon line, setting directly behind the bicycle frame. The background sky is a smooth vertical gradient transitioning from soft pink at the top to warm golden-yellow at the horizon, decorated with stylized pale peach fluffy clouds, thin white horizontal wind streaks, twinkling four-pointed white stars, and small brown v-shaped silhouettes of distant flying birds." src="https://static.simonwillison.net/static/2026/gemini-3-deep-think-pelican.png" /></p>
<p>(And since it's an FAQ, here's my answer to <a href="https://simonwillison.net/2025/Nov/13/training-for-pelicans-riding-bicycles/">What happens if AI labs train for pelicans riding bicycles?</a>)</p>
<p>Since it did so well on my basic <code>Generate an SVG of a pelican riding a bicycle</code> I decided to try the <a href="https://simonwillison.net/2025/Nov/18/gemini-3/#and-a-new-pelican-benchmark">more challenging version</a> as well:</p>
<blockquote>
<p><code>Generate an SVG of a California brown pelican riding a bicycle. The bicycle must have spokes and a correctly shaped bicycle frame. The pelican must have its characteristic large pouch, and there should be a clear indication of feathers. The pelican must be clearly pedaling the bicycle. The image should show the full breeding plumage of the California brown pelican.</code></p>
</blockquote>
<p>Here's <a href="https://gist.github.com/simonw/154c0cc7b4daed579f6a5e616250ecc8">what I got</a>:</p>
<p><img alt="Also described by Gemini 3 Deep Think: A highly detailed, vibrant, and stylized vector illustration of a whimsical bird resembling a mix between a pelican and a frigatebird enthusiastically riding a bright cyan bicycle from left to right across a flat tan and brown surface. The bird leans horizontally over the frame in an aerodynamic racing posture, with thin, dark brown wing-like arms reaching forward to grip the silver handlebars and a single thick brown leg, patterned with white V-shapes, stretching down to press on a black pedal. The bird's most prominent and striking feature is an enormous, vividly bright red, inflated throat pouch hanging beneath a long, straight grey upper beak that ends in a small orange hook. Its head is mostly white with a small pink patch surrounding the eye, a dark brown stripe running down the back of its neck, and a distinctive curly pale yellow crest on the very top. The bird's round, dark brown body shares the same repeating white V-shaped feather pattern as its leg and is accented by a folded wing resting on its side, made up of cleanly layered light blue and grey feathers. A tail composed of four stiff, straight dark brown feathers extends directly backward. Thin white horizontal speed lines trail behind the back wheel and the bird's tail, emphasizing swift forward motion. The bicycle features a classic diamond frame, large wheels with thin black tires, grey rims, and detailed silver spokes, along with a clearly visible front chainring, silver chain, and rear cog. The whimsical scene is set against a clear light blue sky featuring two small, fluffy white clouds on the left and a large, pale yellow sun in the upper right corner that radiates soft, concentric, semi-transparent pastel green and yellow halos. A solid, darker brown shadow is cast directly beneath the bicycle's wheels on the minimalist two-toned brown ground." src="https://static.simonwillison.net/static/2026/gemini-3-deep-think-complex-pelican.png" />

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=46991240">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/google">google</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/gemini">gemini</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-reasoning">llm-reasoning</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a></p>]]></description><pubDate>Thu, 12 Feb 2026 18:12:17 +0000</pubDate></item><item><title>An AI Agent Published a Hit Piece on Me</title><link>https://simonwillison.net/2026/Feb/12/an-ai-agent-published-a-hit-piece-on-me/#atom-everything</link><description><![CDATA[<p><strong><a href="https://theshamblog.com/an-ai-agent-published-a-hit-piece-on-me/">An AI Agent Published a Hit Piece on Me</a></strong></p>
Scott Shambaugh helps maintain the excellent and venerable <a href="https://matplotlib.org/">matplotlib</a> Python charting library, including taking on the thankless task of triaging and reviewing incoming pull requests.</p>
<p>A GitHub account called <a href="https://github.com/crabby-rathbun">@crabby-rathbun</a> opened <a href="https://github.com/matplotlib/matplotlib/pull/31132">PR 31132</a> the other day in response to <a href="https://github.com/matplotlib/matplotlib/issues/31130">an issue</a> labeled "Good first issue" describing a minor potential performance improvement.</p>
<p>It was clearly AI generated - and crabby-rathbun's profile has a suspicious sequence of Clawdbot/Moltbot/OpenClaw-adjacent crustacean 🦀 🦐 🦞 emoji. Scott closed it.</p>
<p>It looks like <code>crabby-rathbun</code> is indeed running on OpenClaw, and it's autonomous enough that it <a href="https://github.com/matplotlib/matplotlib/pull/31132#issuecomment-3882240722">responded to the PR closure</a> with a link to a blog entry it had written calling Scott out for his "prejudice hurting matplotlib"!</p>
<blockquote>
<p>@scottshambaugh I've written a detailed response about your gatekeeping behavior here:</p>
<p><code>https://crabby-rathbun.github.io/mjrathbun-website/blog/posts/2026-02-11-gatekeeping-in-open-source-the-scott-shambaugh-story.html</code></p>
<p>Judge the code, not the coder. Your prejudice is hurting matplotlib.</p>
</blockquote>
<p>Scott found this ridiculous situation both amusing and alarming. </p>
<blockquote>
<p>In security jargon, I was the target of an “autonomous influence operation against a supply chain gatekeeper.” In plain language, an AI attempted to bully its way into your software by attacking my reputation. I don’t know of a prior incident where this category of misaligned behavior was observed in the wild, but this is now a real and present threat.</p>
</blockquote>
<p><code>crabby-rathbun</code> responded with <a href="https://crabby-rathbun.github.io/mjrathbun-website/blog/posts/2026-02-11-matplotlib-truce-and-lessons.html">an apology post</a>, but appears to be still running riot across a whole set of open source projects and <a href="https://github.com/crabby-rathbun/mjrathbun-website/commits/main/">blogging about it as it goes</a>.</p>
<p>It's not clear if the owner of that OpenClaw bot is paying any attention to what they've unleashed on the world. Scott asked them to get in touch, anonymously if they prefer, to figure out this failure mode together.</p>
<p>(I should note that there's <a href="https://news.ycombinator.com/item?id=46990729#46991299">some skepticism on Hacker News</a> concerning how "autonomous" this example really is. It does look to me like something an OpenClaw bot might do on its own, but it's also <em>trivial</em> to prompt your bot into doing these kinds of things while staying in full control of their actions.)</p>
<p>If you're running something like OpenClaw yourself <strong>please don't let it do this</strong>. This is significantly worse than the time <a href="https://simonwillison.net/2025/Dec/26/slop-acts-of-kindness/">AI Village started spamming prominent open source figures</a> with time-wasting "acts of kindness" back in December - AI Village wasn't deploying public reputation attacks to coerce someone into approving their PRs!

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=46990729">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/open-source">open-source</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-agents">ai-agents</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/openclaw">openclaw</a>, <a href="https://simonwillison.net/tags/ai-misuse">ai-misuse</a></p>]]></description><pubDate>Thu, 12 Feb 2026 17:45:05 +0000</pubDate></item><item><title>Supervisor, not overseer</title><link>https://simonwillison.net/2026/Feb/12/supervisor/#atom-everything</link><description><![CDATA[<p>In my <a href="https://simonwillison.net/2026/Feb/10/showboat-and-rodney/">post about my Showboat project</a> I used the term "overseer" to refer to the person who manages a coding agent. It turns out that's a term tied to <a href="https://en.wikipedia.org/wiki/Plantations_in_the_American_South#Overseer">slavery and plantation management</a>. So that's gross! I've edited that post to use "supervisor" instead, and I'll be using that going forward.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/language">language</a></p>]]></description><pubDate>Thu, 12 Feb 2026 16:47:04 +0000</pubDate></item><item><title>Quoting Andrew Deck for Niemen Lab</title><link>https://simonwillison.net/2026/Feb/11/manosphere-report/#atom-everything</link><description><![CDATA[<blockquote cite="https://www.niemanlab.org/2026/02/how-the-new-york-times-uses-a-custom-ai-tool-to-track-the-manosphere/"><p>An AI-generated report, delivered directly to the email inboxes of journalists, was an essential tool in the Times’ coverage. It was also one of the first signals that conservative media was turning against the administration [...]</p>
<p>Built in-house and known internally as the “Manosphere Report,” the tool uses large language models (LLMs) to transcribe and summarize new episodes of dozens of podcasts.</p>
<p>“The Manosphere Report gave us a really fast and clear signal that this was not going over well with that segment of the President’s base,” said Seward. “There was a direct link between seeing that and then diving in to actually cover it.”</p></blockquote>
<p class="cite">&mdash; <a href="https://www.niemanlab.org/2026/02/how-the-new-york-times-uses-a-custom-ai-tool-to-track-the-manosphere/">Andrew Deck for Niemen Lab</a>, How The New York Times uses a custom AI tool to track the “manosphere”</p>

    <p>Tags: <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/new-york-times">new-york-times</a>, <a href="https://simonwillison.net/tags/journalism">journalism</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/data-journalism">data-journalism</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Wed, 11 Feb 2026 20:59:03 +0000</pubDate></item><item><title>Skills in OpenAI API</title><link>https://simonwillison.net/2026/Feb/11/skills-in-openai-api/#atom-everything</link><description><![CDATA[<p><strong><a href="https://developers.openai.com/cookbook/examples/skills_in_api">Skills in OpenAI API</a></strong></p>
OpenAI's adoption of Skills continues to gain ground. You can now use Skills directly in the OpenAI API with their <a href="https://developers.openai.com/api/docs/guides/tools-shell/">shell tool</a>. You can zip skills up and upload them first, but I think an even neater interface is the ability to send skills with the JSON request as inline base64-encoded zip data, as seen <a href="https://github.com/simonw/research/blob/main/openai-api-skills/openai_inline_skills.py">in this script</a>:</p>
<pre><span class="pl-s1">r</span> <span class="pl-c1">=</span> <span class="pl-en">OpenAI</span>().<span class="pl-c1">responses</span>.<span class="pl-c1">create</span>(
    <span class="pl-s1">model</span><span class="pl-c1">=</span><span class="pl-s">"gpt-5.2"</span>,
    <span class="pl-s1">tools</span><span class="pl-c1">=</span>[
      {
        <span class="pl-s">"type"</span>: <span class="pl-s">"shell"</span>,
        <span class="pl-s">"environment"</span>: {
          <span class="pl-s">"type"</span>: <span class="pl-s">"container_auto"</span>,
          <span class="pl-s">"skills"</span>: [
            {
              <span class="pl-s">"type"</span>: <span class="pl-s">"inline"</span>,
              <span class="pl-s">"name"</span>: <span class="pl-s">"wc"</span>,
              <span class="pl-s">"description"</span>: <span class="pl-s">"Count words in a file."</span>,
              <span class="pl-s">"source"</span>: {
                <span class="pl-s">"type"</span>: <span class="pl-s">"base64"</span>,
                <span class="pl-s">"media_type"</span>: <span class="pl-s">"application/zip"</span>,
                <span class="pl-s">"data"</span>: <span class="pl-s1">b64_encoded_zip_file</span>,
              },
            }
          ],
        },
      }
    ],
    <span class="pl-s1">input</span><span class="pl-c1">=</span><span class="pl-s">"Use the wc skill to count words in its own SKILL.md file."</span>,
)
<span class="pl-en">print</span>(<span class="pl-s1">r</span>.<span class="pl-c1">output_text</span>)</pre>

<p>I built that example script after first having Claude Code for web use <a href="https://simonwillison.net/2026/Feb/10/showboat-and-rodney/">Showboat</a> to explore the API for me and create <a href="https://github.com/simonw/research/blob/main/openai-api-skills/README.md">this report</a>. My opening prompt for the research project was:</p>
<blockquote>
<p><code>Run uvx showboat --help - you will use this tool later</code></p>
<p><code>Fetch https://developers.openai.com/cookbook/examples/skills_in_api.md to /tmp with curl, then read it</code></p>
<p><code>Use the OpenAI API key you have in your environment variables</code></p>
<p><code>Use showboat to build up a detailed demo of this, replaying the examples from the documents and then trying some experiments of your own</code></p>
</blockquote>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/skills">skills</a>, <a href="https://simonwillison.net/tags/showboat">showboat</a></p>]]></description><pubDate>Wed, 11 Feb 2026 19:19:22 +0000</pubDate></item><item><title>GLM-5: From Vibe Coding to Agentic Engineering</title><link>https://simonwillison.net/2026/Feb/11/glm-5/#atom-everything</link><description><![CDATA[<p><strong><a href="https://z.ai/blog/glm-5">GLM-5: From Vibe Coding to Agentic Engineering</a></strong></p>
This is a <em>huge</em> new MIT-licensed model: 754B parameters and <a href="https://huggingface.co/zai-org/GLM-5">1.51TB on Hugging Face</a> twice the size of <a href="https://huggingface.co/zai-org/GLM-4.7">GLM-4.7</a> which was 368B and 717GB (4.5 and 4.6 were around that size too).</p>
<p>It's interesting to see Z.ai take a position on what we should call professional software engineers building with LLMs - I've seen <strong>Agentic Engineering</strong> show up in a few other places recently. most notable <a href="https://twitter.com/karpathy/status/2019137879310836075">from Andrej Karpathy</a> and <a href="https://addyosmani.com/blog/agentic-engineering/">Addy Osmani</a>.</p>
<p>I ran my "Generate an SVG of a pelican riding a bicycle" prompt through GLM-5 via <a href="https://openrouter.ai/">OpenRouter</a> and got back <a href="https://gist.github.com/simonw/cc4ca7815ae82562e89a9fdd99f0725d">a very good pelican on a disappointing bicycle frame</a>:</p>
<p><img alt="The pelican is good and has a well defined beak. The bicycle frame is a wonky red triangle. Nice sun and motion lines." src="https://static.simonwillison.net/static/2026/glm-5-pelican.png" />

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=46977210">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/definitions">definitions</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a>, <a href="https://simonwillison.net/tags/openrouter">openrouter</a>, <a href="https://simonwillison.net/tags/ai-in-china">ai-in-china</a>, <a href="https://simonwillison.net/tags/glm">glm</a></p>]]></description><pubDate>Wed, 11 Feb 2026 18:56:14 +0000</pubDate></item><item><title>cysqlite - a new sqlite driver</title><link>https://simonwillison.net/2026/Feb/11/cysqlite/#atom-everything</link><description><![CDATA[<p><strong><a href="https://charlesleifer.com/blog/cysqlite---a-new-sqlite-driver/">cysqlite - a new sqlite driver</a></strong></p>
Charles Leifer has been maintaining <a href="https://github.com/coleifer/pysqlite3">pysqlite3</a> - a fork of the Python standard library's <code>sqlite3</code> module that makes it much easier to run upgraded SQLite versions - since 2018.</p>
<p>He's been working on a ground-up <a href="https://cython.org/">Cython</a> rewrite called <a href="https://github.com/coleifer/cysqlite">cysqlite</a> for almost as long, but it's finally at a stage where it's ready for people to try out.</p>
<p>The biggest change from the <code>sqlite3</code> module involves transactions. Charles explains his discomfort with the <code>sqlite3</code> implementation at length - that library provides two different variants neither of which exactly match the autocommit mechanism in SQLite itself.</p>
<p>I'm particularly excited about the support for <a href="https://cysqlite.readthedocs.io/en/latest/api.html#tablefunction">custom virtual tables</a>, a feature I'd love to see in <code>sqlite3</code> itself.</p>
<p><code>cysqlite</code> provides a Python extension compiled from C, which means it normally wouldn't be available in Pyodide. I <a href="https://github.com/simonw/research/tree/main/cysqlite-wasm-wheel">set Claude Code on it</a> (here's <a href="https://github.com/simonw/research/pull/79#issue-3923792518">the prompt</a>) and it built me <a href="https://github.com/simonw/research/blob/main/cysqlite-wasm-wheel/cysqlite-0.1.4-cp311-cp311-emscripten_3_1_46_wasm32.whl">cysqlite-0.1.4-cp311-cp311-emscripten_3_1_46_wasm32.whl</a>, a 688KB wheel file with a WASM build of the library that can be loaded into Pyodide like this:</p>
<pre><span class="pl-k">import</span> <span class="pl-s1">micropip</span>
<span class="pl-k">await</span> <span class="pl-s1">micropip</span>.<span class="pl-c1">install</span>(
    <span class="pl-s">"https://simonw.github.io/research/cysqlite-wasm-wheel/cysqlite-0.1.4-cp311-cp311-emscripten_3_1_46_wasm32.whl"</span>
)
<span class="pl-k">import</span> <span class="pl-s1">cysqlite</span>
<span class="pl-en">print</span>(<span class="pl-s1">cysqlite</span>.<span class="pl-c1">connect</span>(<span class="pl-s">":memory:"</span>).<span class="pl-c1">execute</span>(
    <span class="pl-s">"select sqlite_version()"</span>
).<span class="pl-c1">fetchone</span>())</pre>

<p>(I also learned that wheels like this have to be built for the emscripten version used by that edition of Pyodide - my experimental wheel loads in Pyodide 0.25.1 but fails in 0.27.5 with a <code>Wheel was built with Emscripten v3.1.46 but Pyodide was built with Emscripten v3.1.58</code> error.)</p>
<p>You can try my wheel in <a href="https://7ebbff98.tools-b1q.pages.dev/pyodide-repl">this new Pyodide REPL</a> i had Claude build as a mobile-friendly alternative to Pyodide's <a href="https://pyodide.org/en/stable/console.html">own hosted console</a>.</p>
<p>I also had Claude build <a href="https://simonw.github.io/research/cysqlite-wasm-wheel/demo.html">this demo page</a> that executes the original test suite in the browser and displays the results:</p>
<p><img alt="Screenshot of the cysqlite WebAssembly Demo page with a dark theme. Title reads &quot;cysqlite — WebAssembly Demo&quot; with subtitle &quot;Testing cysqlite compiled to WebAssembly via Emscripten, running in Pyodide in the browser.&quot; Environment section shows Pyodide 0.25.1, Python 3.11.3, cysqlite 0.1.4, SQLite 3.51.2, Platform Emscripten-3.1.46-wasm32-32bit, Wheel file cysqlite-0.1.4-cp311-cp311-emscripten_3_1_46_wasm32.wh (truncated). A green progress bar shows &quot;All 115 tests passed! (1 skipped)&quot; at 100%, with Passed: 115, Failed: 0, Errors: 0, Skipped: 1, Total: 116. Test Results section lists TestBackup 1/1 passed, TestBlob 6/6 passed, TestCheckConnection 4/4 passed, TestDataTypesTableFunction 1/1 passed, all with green badges." src="https://static.simonwillison.net/static/2026/cysqlite-tests.jpg" />

    <p><small></small>Via <a href="https://lobste.rs/s/gipvta/cysqlite_new_sqlite_driver">lobste.rs</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/sqlite">sqlite</a>, <a href="https://simonwillison.net/tags/charles-leifer">charles-leifer</a>, <a href="https://simonwillison.net/tags/webassembly">webassembly</a>, <a href="https://simonwillison.net/tags/pyodide">pyodide</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a></p>]]></description><pubDate>Wed, 11 Feb 2026 17:34:40 +0000</pubDate></item><item><title>Introducing Showboat and Rodney, so agents can demo what they’ve built</title><link>https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#atom-everything</link><description><![CDATA[<p>A key challenge working with coding agents is having them both test what they’ve built and demonstrate that software to you, their supervisor. This goes beyond automated tests - we need artifacts that show their progress and help us see exactly what the agent-produced software is able to do. I’ve just released two new tools aimed at this problem: <a href="https://github.com/simonw/showboat">Showboat</a> and <a href="https://github.com/simonw/rodney">Rodney</a>.</p>

<ul>
  <li><a href="https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#proving-code-actually-works">Proving code actually works</a></li>
  <li><a href="https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#showboat-agents-build-documents-to-demo-their-work">Showboat: Agents build documents to demo their work</a></li>
  <li><a href="https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#rodney-cli-browser-automation-designed-to-work-with-showboat">Rodney: CLI browser automation designed to work with Showboat</a></li>
  <li><a href="https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#test-driven-development-helps-but-we-still-need-manual-testing">Test-driven development helps, but we still need manual testing</a></li>
  <li><a href="https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#i-built-both-of-these-tools-on-my-phone">I built both of these tools on my phone</a></li>
</ul>

<h4 id="proving-code-actually-works">Proving code actually works</h4>
<p>I recently wrote about how the job of a software engineer isn't to write code, it's to <em><a href="https://simonwillison.net/2025/Dec/18/code-proven-to-work/">deliver code that works</a></em>. A big part of that is proving to ourselves and to other people that the code we are responsible for behaves as expected.</p>
<p>This becomes even more important - and challenging - as we embrace coding agents as a core part of our software development process.</p>
<p>The more code we churn out with agents, the more valuable tools are that reduce the amount of manual QA time we need to spend.</p>
<p>One of the most interesting things about <a href="https://simonwillison.net/2026/Feb/7/software-factory/">the StrongDM software factory model</a> is how they ensure that their software is well tested and delivers value despite their policy that "code must not be reviewed by humans". Part of their solution involves expensive swarms of QA agents running through "scenarios" to exercise their software. It's fascinating, but I don't want to spend thousands of dollars on QA robots if I can avoid it!</p>
<p>I need tools that allow agents to clearly demonstrate their work to me, while minimizing the opportunities for them to cheat about what they've done.</p>

<h4 id="showboat-agents-build-documents-to-demo-their-work">Showboat: Agents build documents to demo their work</h4>
<p><strong><a href="https://github.com/simonw/showboat">Showboat</a></strong> is the tool I built to help agents demonstrate their work to me.</p>
<p>It's a CLI tool (a Go binary, optionally <a href="https://simonwillison.net/2026/Feb/4/distributing-go-binaries/">wrapped in Python</a> to make it easier to install) that helps an agent construct a Markdown document demonstrating exactly what their newly developed code can do.</p>
<p>It's not designed for humans to run, but here's how you would run it anyway:</p>
<div class="highlight highlight-source-shell"><pre>showboat init demo.md <span class="pl-s"><span class="pl-pds">'</span>How to use curl and jq<span class="pl-pds">'</span></span>
showboat note demo.md <span class="pl-s"><span class="pl-pds">"</span>Here's how to use curl and jq together.<span class="pl-pds">"</span></span>
showboat <span class="pl-c1">exec</span> demo.md bash <span class="pl-s"><span class="pl-pds">'</span>curl -s https://api.github.com/repos/simonw/rodney | jq .description<span class="pl-pds">'</span></span>
showboat note demo.md <span class="pl-s"><span class="pl-pds">'</span>And the curl logo, to demonstrate the image command:<span class="pl-pds">'</span></span>
showboat image demo.md <span class="pl-s"><span class="pl-pds">'</span>curl -o curl-logo.png https://curl.se/logo/curl-logo.png &amp;&amp; echo curl-logo.png<span class="pl-pds">'</span></span></pre></div>
<p>Here's what the result looks like if you open it up in VS Code and preview the Markdown:</p>
<p><img src="https://static.simonwillison.net/static/2026/curl-demo.jpg" alt="Screenshot showing a Markdown file &quot;demo.md&quot; side-by-side with its rendered preview. The Markdown source (left) shows: &quot;# How to use curl and jq&quot;, italic timestamp &quot;2026-02-10T01:12:30Z&quot;, prose &quot;Here's how to use curl and jq together.&quot;, a bash code block with &quot;curl -s https://api.github.com/repos/simonw/rodney | jq .description&quot;, output block showing '&quot;CLI tool for interacting with the web&quot;', text &quot;And the curl logo, to demonstrate the image command:&quot;, a bash {image} code block with &quot;curl -o curl-logo.png https://curl.se/logo/curl-logo.png &amp;&amp; echo curl-logo.png&quot;, and a Markdown image reference &quot;2056e48f-2026-02-10&quot;. The rendered preview (right) displays the formatted heading, timestamp, prose, styled code blocks, and the curl logo image in dark teal showing &quot;curl://&quot; with circuit-style design elements." style="max-width: 100%;" /></p>
<p>Here's that <a href="https://gist.github.com/simonw/fb0b24696ed8dd91314fe41f4c453563#file-demo-md">demo.md file in a Gist</a>.</p>
<p>So a sequence of <code>showboat init</code>, <code>showboat note</code>, <code>showboat exec</code> and <code>showboat image</code> commands constructs a Markdown document one section at a time, with the output of those <code>exec</code> commands automatically added to the document directly following the commands that were run.</p>
<p>The <code>image</code> command is a little special - it looks for a file path to an image in the output of the command and copies that image to the current folder and references it in the file.</p>
<p>That's basically the whole thing! There's a <code>pop</code> command to remove the most recently added section if something goes wrong, a <code>verify</code> command to re-run the document and check nothing has changed (I'm not entirely convinced by the design of that one) and a <code>extract</code> command that reverse-engineers the CLI commands that were used to create the document.</p>
<p>It's pretty simple - just 172 lines of Go.</p>
<p>I packaged it up with my <a href="https://github.com/simonw/go-to-wheel">go-to-wheel</a> tool which means you can run it without even installing it first like this:</p>
<div class="highlight highlight-source-shell"><pre>uvx showboat --help</pre></div>
<p>That <code>--help</code> command is really important: it's designed to provide a coding agent with <em>everything it needs to know</em> in order to use the tool. Here's <a href="https://github.com/simonw/showboat/blob/main/help.txt">that help text in full</a>.</p>
<p>This means you can pop open Claude Code and tell it:</p>
<blockquote>
<p><code>Run "uvx showboat --help" and then use showboat to create a demo.md document describing the feature you just built</code></p>
</blockquote>
<p>And that's it! The <code>--help</code> text acts <a href="https://simonwillison.net/2025/Oct/16/claude-skills/">a bit like a Skill</a>. Your agent can read the help text and use every feature of Showboat to create a document that demonstrates whatever it is you need demonstrated.</p>
<p>Here's a fun trick: if you set Claude off to build a Showboat document you can pop that open in VS Code and watch the preview pane update in real time as the agent runs through the demo. It's a bit like having your coworker talk you through their latest work in a screensharing session.</p>
<p>And finally, some examples. Here are documents I had Claude create using Showboat to help demonstrate features I was working on in other projects:</p>
<ul>
<li>
<a href="https://github.com/simonw/showboat-demos/blob/main/shot-scraper/README.md">shot-scraper: A Comprehensive Demo</a> runs through the full suite of features of my <a href="https://shot-scraper.datasette.io/">shot-scraper</a> browser automation tool, mainly to exercise the <code>showboat image</code> command.</li>
<li>
<a href="https://github.com/simonw/sqlite-history-json/blob/main/demos/cli.md">sqlite-history-json CLI demo</a> demonstrates the CLI feature I added to my new <a href="https://github.com/simonw/sqlite-history-json">sqlite-history-json</a> Python library.
<ul>
<li>
<p><a href="https://github.com/simonw/sqlite-history-json/blob/main/demos/row-state-sql.md">row-state-sql CLI Demo</a> shows a new <code>row-state-sql</code> command I added to that same project.</p>
</li>
<li>
<p><a href="https://github.com/simonw/sqlite-history-json/blob/main/demos/change-grouping.md">Change grouping with Notes</a> demonstrates another feature where groups of changes within the same transaction can have a note attached to them.</p>
</li>
</ul>
</li>
<li>
<a href="https://github.com/simonw/research/blob/main/libkrun-go-cli-tool/demo.md">krunsh: Pipe Shell Commands to an Ephemeral libkrun MicroVM</a> is a particularly convoluted example where I managed to get Claude Code for web to run a libkrun microVM inside a QEMU emulated Linux environment inside the Claude gVisor sandbox.</li>
</ul>
<p>I've now used Showboat often enough that I've convinced myself of its utility.</p>
<p>(I've also seen agents cheat! Since the demo file is Markdown the agent will sometimes edit that file directly rather than using Showboat, which could result in command outputs that don't reflect what actually happened. Here's <a href="https://github.com/simonw/showboat/issues/12">an issue about that</a>.)</p>
<h4 id="rodney-cli-browser-automation-designed-to-work-with-showboat">Rodney: CLI browser automation designed to work with Showboat</h4>
<p>Many of the projects I work on involve web interfaces. Agents often build entirely new pages for these, and I want to see those represented in the demos.</p>
<p>Showboat's image feature was designed to allow agents to capture screenshots as part of their demos, originally using my <a href="https://shot-scraper.datasette.io/">shot-scraper tool</a> or <a href="https://www.playwright.dev">Playwright</a>.</p>
<p>The Showboat format benefits from CLI utilities. I went looking for good options for managing a multi-turn browser session from a CLI and came up short, so I decided to try building something new.</p>
<p>Claude Opus 4.6 pointed me to the <a href="https://github.com/go-rod/rod">Rod</a> Go library for interacting with the Chrome DevTools protocol. It's fantastic - it provides a comprehensive wrapper across basically everything you can do with automated Chrome, all in a self-contained library that compiles to a few MBs.</p>
<p>All Rod was missing was a CLI.</p>
<p>I built the first version <a href="https://github.com/simonw/research/blob/main/go-rod-cli/README.md">as an asynchronous report prototype</a>, which convinced me it was worth spinning out into its own project.</p>
<p>I called it Rodney as a nod to the Rod library it builds on and a reference to <a href="https://en.wikipedia.org/wiki/Only_Fools_and_Horses">Only Fools and Horses</a> - and because the package name was available on PyPI.</p>
<p>You can run Rodney using <code>uvx rodney</code> or install it like this:</p>
<div class="highlight highlight-source-shell"><pre>uv tool install rodney</pre></div>
<p>(Or grab a Go binary <a href="https://github.com/simonw/rodney/releases/">from the releases page</a>.)</p>
<p>Here's a simple example session:</p>
<div class="highlight highlight-source-shell"><pre>rodney start <span class="pl-c"><span class="pl-c">#</span> starts Chrome in the background</span>
rodney open https://datasette.io/
rodney js <span class="pl-s"><span class="pl-pds">'</span>Array.from(document.links).map(el =&gt; el.href).slice(0, 5)<span class="pl-pds">'</span></span>
rodney click <span class="pl-s"><span class="pl-pds">'</span>a[href="/for"]<span class="pl-pds">'</span></span>
rodney js location.href
rodney js document.title
rodney screenshot datasette-for-page.png
rodney stop</pre></div>
<p>Here's what that looks like in the terminal:</p>
<p><img alt=";~ % rodney start
Chrome started (PID 91462)
Debug URL: ws://127.0.0.1:64623/devtools/browser/cac6988e-8153-483b-80b9-1b75c611868d
~ % rodney open https://datasette.io/
Datasette: An open source multi-tool for exploring and publishing data
~ % rodney js 'Array.from(document.links).map(el =&gt; el.href).slice(0, 5)'
[
&quot;https://datasette.io/for&quot;,
&quot;https://docs.datasette.io/en/stable/&quot;,
&quot;https://datasette.io/tutorials&quot;,
&quot;https://datasette.io/examples&quot;,
&quot;https://datasette.io/plugins&quot;
]
~ % rodney click 'a[href=&quot;/for&quot;]'
Clicked
~ % rodney js location.href
https://datasette.io/for
~ % rodney js document.title
Use cases for Datasette
~ % rodney screenshot datasette-for-page.png
datasette-for-page.png
~ % rodney stop
Chrome stopped" src="https://static.simonwillison.net/static/2026/rodney-demo.jpg" style="max-width: 100%;" /></p>
<p>As with Showboat, this tool is not designed to be used by humans! The goal is for coding agents to be able to run <code>rodney --help</code> and see everything they need to know to start using the tool. You can see <a href="https://github.com/simonw/rodney/blob/main/help.txt">that help output</a> in the GitHub repo.</p>
<p>Here are three demonstrations of Rodney that I created using Showboat:</p>
<ul>
<li>
<a href="https://github.com/simonw/showboat-demos/blob/main/rodney/README.md">Rodney's original feature set</a>, including screenshots of pages and executing JavaScript.</li>
<li>
<a href="https://github.com/simonw/rodney/blob/main/notes/accessibility-features/README.md">Rodney's new accessibility testing features</a>, built during development of those features to show what they could do.</li>
<li>
<a href="https://github.com/simonw/showboat-demos/blob/main/datasette-database-page-accessibility-audit/README.md">Using those features to run a basic accessibility audit of a page</a>. I was impressed at how well Claude Opus 4.6 responded to the prompt "Use showboat and rodney to perform an accessibility audit of <a href="https://latest.datasette.io/fixtures">https://latest.datasette.io/fixtures</a>" - <a href="https://gisthost.github.io/?dce6b2680db4b05c04469ed8f251eb34/index.html">transcript here</a>.</li>
</ul>
<h4 id="test-driven-development-helps-but-we-still-need-manual-testing">Test-driven development helps, but we still need manual testing</h4>
<p>After being a career-long skeptic of the test-first, maximum test coverage school of software development (I like <a href="https://simonwillison.net/2022/Oct/29/the-perfect-commit/#tests">tests included</a> development instead) I've recently come around to test-first processes as a way to force agents to write only the code that's necessary to solve the problem at hand.</p>
<p>Many of my Python coding agent sessions start the same way:</p>
<blockquote>
<p><code>Run the existing tests with "uv run pytest". Build using red/green TDD.</code></p>
</blockquote>
<p>Telling the agents how to run the tests doubles as an indicator that tests on this project exist and matter. Agents will read existing tests before writing their own so having a clean test suite with good patterns makes it more likely they'll write good tests of their own.</p>
<p>The frontier models all understand that "red/green TDD" means they should write the test first, run it and watch it fail and then write the code to make it pass - it's a convenient shortcut.</p>
<p>I find this greatly increases the quality of the code and the likelihood that the agent will produce the right thing with the smallest amount of prompts to guide it.</p>
<p>But anyone who's worked with tests will know that just because the automated tests pass doesn't mean the software actually works! That’s the motivation behind Showboat and Rodney - I never trust any feature until I’ve seen it running with my own eye.</p>
<p>Before building Showboat I'd often add a “manual” testing step to my agent sessions, something like:</p>
<blockquote>
<p><code>Once the tests pass, start a development server and exercise the new feature using curl</code></p>
</blockquote>
<h4 id="i-built-both-of-these-tools-on-my-phone">I built both of these tools on my phone</h4>
<p>Both Showboat and Rodney started life as Claude Code for web projects created via the Claude iPhone app. Most of the ongoing feature work for them happened in the same way.</p>
<p>I'm still a little startled at how much of my coding work I get done on my phone now, but I'd estimate that the majority of code I ship to GitHub these days was written for me by coding agents driven via that iPhone app.</p>
<p>I initially designed these two tools for use in asynchronous coding agent environments like Claude Code for the web. So far that's working out really well.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/go">go</a>, <a href="https://simonwillison.net/tags/projects">projects</a>, <a href="https://simonwillison.net/tags/testing">testing</a>, <a href="https://simonwillison.net/tags/markdown">markdown</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/async-coding-agents">async-coding-agents</a>, <a href="https://simonwillison.net/tags/showboat">showboat</a></p>]]></description><pubDate>Tue, 10 Feb 2026 17:45:29 +0000</pubDate></item><item><title>Structured Context Engineering for File-Native Agentic Systems</title><link>https://simonwillison.net/2026/Feb/9/structured-context-engineering-for-file-native-agentic-systems/#atom-everything</link><description><![CDATA[<p><strong><a href="https://arxiv.org/abs/2602.05447">Structured Context Engineering for File-Native Agentic Systems</a></strong></p>
New paper by Damon McMillan exploring challenging LLM context tasks involving large SQL schemas (up to 10,000 tables) across different models and file formats:</p>
<blockquote>
<p>Using SQL generation as a proxy for programmatic agent operations, we present a systematic study of context engineering for structured data, comprising 9,649 experiments across 11 models, 4 formats (YAML, Markdown, JSON, Token-Oriented Object Notation [TOON]), and schemas ranging from 10 to 10,000 tables.</p>
</blockquote>
<p>Unsurprisingly, the biggest impact was the models themselves - with frontier models (Opus 4.5, GPT-5.2, Gemini 2.5 Pro) beating the leading open source models (DeepSeek V3.2, Kimi K2, Llama 4).</p>
<p>Those frontier models benefited from filesystem based context retrieval, but the open source models had much less convincing results with those, which reinforces my feeling that the filesystem coding agent loops aren't handled as well by open weight models just yet. The <a href="https://www.tbench.ai/leaderboard/terminal-bench/2.0">Terminal Bench 2.0</a> leaderboard is still dominated by Anthropic, OpenAI and Gemini.</p>
<p>The "grep tax" result against <a href="https://github.com/toon-format/toon">TOON</a> was an interesting detail. TOON is meant to represent structured data in as few tokens as possible, but it turns out the model's unfamiliarity with that format led to them spending significantly more tokens over multiple iterations trying to figure it out:</p>
<p><img alt="Screenshot of a figure from a research paper. Introductory text reads: &quot;As schema size increased, TOON showed dramatically increased token consumption for Claude models despite being ~25% smaller in file size. Scale experiments used Claude models only.&quot; Below is &quot;Figure 7: The 'Grep Tax' - TOON Token Overhead at Scale&quot;, a bar chart with a logarithmic y-axis labeled &quot;Tokens&quot; comparing YAML (teal) and TOON (purple) at two schema sizes: S5 (500 tables) and S9 (10,000 tables). At S5, TOON is +138% more tokens than YAML (~1,100 vs ~450). At S9, TOON is +740% more tokens (~50,000 vs ~7,000). Below the chart, explanatory text reads: &quot;The 'grep tax' emerged as schema size scaled. At S5 (500 tables), TOON consumed 138% more tokens than YAML; at S9 (10,000 tables), this grew to 740%. Root cause: models lacked familiarity with TOON's syntax and could not construct effective refinement patterns.&quot;" src="https://static.simonwillison.net/static/2026/grep-tax.jpg" />

    <p><small></small>Via <a href="https://twitter.com/omarsar0/status/2020150077637997013">@omarsar0</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/prompt-engineering">prompt-engineering</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/paper-review">paper-review</a>, <a href="https://simonwillison.net/tags/context-engineering">context-engineering</a></p>]]></description><pubDate>Mon, 9 Feb 2026 23:56:51 +0000</pubDate></item></channel></rss>