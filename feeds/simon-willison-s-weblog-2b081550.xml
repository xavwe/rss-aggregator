<?xml version="1.0" encoding="utf-8"?><rss version="2.0"><channel><title>Simon Willison&apos;s Weblog</title><link>https://raw.githubusercontent.com/xavwe/rss-aggregator/refs/heads/main/feeds/simon-willison-s-weblog-2b081550.xml</link><description>Archived feed from https://simonwillison.net/atom/everything</description><item><title>Claude Skills are awesome, maybe a bigger deal than MCP</title><link>https://simonwillison.net/2025/Oct/16/claude-skills/#atom-everything</link><description><![CDATA[<p>Anthropic this morning <a href="https://www.anthropic.com/news/skills">introduced Claude Skills</a>, a new pattern for making new abilities available to their models:</p>
<blockquote>
<p>Claude can now use <em>Skills</em> to improve how it performs specific tasks. Skills are folders that include instructions, scripts, and resources that Claude can load when needed.</p>
<p>Claude will only access a skill when it's relevant to the task at hand. When used, skills make Claude better at specialized tasks like working with Excel or following your organization's brand guidelines.</p>
</blockquote>
<p>Their engineering blog has a <a href="https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills">more detailed explanation</a>. There's also a new <a href="https://github.com/anthropics/skills">anthropic/skills</a> GitHub repo.</p>
<p>(I inadvertently preempted their announcement of this feature when I reverse engineered and <a href="https://simonwillison.net/2025/Oct/10/claude-skills/">wrote about it last Friday</a>!)</p>
<p>Skills are conceptually extremely simple: a skill is a Markdown file telling the model how to do something, optionally accompanied by extra documents and pre-written scripts that the model can run to help it accomplish the tasks described by the skill.</p>
<p>Claude's new <a href="https://www.anthropic.com/news/create-files">document creation abilities</a>, which accompanied <a href="https://simonwillison.net/2025/Sep/9/claude-code-interpreter/">their new code interpreter feature</a> in September, turned out to be entirely implemented using skills. Those are <a href="https://github.com/anthropics/skills/tree/main/document-skills">now available Anthropic's repo</a> covering <code>.pdf</code>, <code>.docx</code>, <code>.xlsx</code>, and <code>.pptx</code> files.</p>
<p>There's one extra detail that makes this a feature, not just a bunch of files in disk. At the start of a session Claude's various harnesses can scan all available skill files and read a short explanation for each one from the frontmatter YAML in the Markdown file. This is <em>very</em> token efficient: each skill only takes up a few dozen extra tokens, with the full details only loaded in should the user request a task that the skill can help solve.</p>
<ul>
  <li><a href="https://simonwillison.net/2025/Oct/16/claude-skills/#trying-out-the-slack-gif-creator-skill">Trying out the slack-gif-creator skill</a></li>
  <li><a href="https://simonwillison.net/2025/Oct/16/claude-skills/#skills-depend-on-a-coding-environment">Skills depend on a coding environment</a></li>
  <li><a href="https://simonwillison.net/2025/Oct/16/claude-skills/#claude-as-a-general-agent">Claude as a General Agent</a></li>
  <li><a href="https://simonwillison.net/2025/Oct/16/claude-skills/#skills-compared-to-mcp">Skills compared to MCP</a></li>
  <li><a href="https://simonwillison.net/2025/Oct/16/claude-skills/#here-come-the-skills">Here come the Skills</a></li>

</ul>
<h4 id="trying-out-the-slack-gif-creator-skill">Trying out the slack-gif-creator skill</h4>
<p>Here's that metadata for an example <a href="https://github.com/anthropics/skills/blob/main/slack-gif-creator/SKILL.md">slack-gif-creator skill</a> that Anthropic published this morning:</p>
<blockquote>
<p>Toolkit for creating animated GIFs optimized for Slack, with validators for size constraints and composable animation primitives. This skill applies when users request animated GIFs or emoji animations for Slack from descriptions like "make me a GIF for Slack of X doing Y".</p>
</blockquote>
<p>I just tried this skill out in the Claude mobile web app, against Sonnet 4.5. First I enabled the slack-gif-creator skill <a href="https://claude.ai/settings/capabilities">in the settings</a>, then I prompted:</p>
<blockquote>
<p><code>Make me a gif for slack about how Skills are way cooler than MCPs</code></p>
</blockquote>
<p>And Claude <a href="https://claude.ai/share/eff7ae7b-b386-417b-9fa0-213fa76ace6e">made me this GIF</a>. Click to play (it's almost epilepsy inducing, hence the click-to-play mechanism):</p>
<p><img
  src="https://static.simonwillison.net/static/2025/skills_vs_mcps_still.gif"
  data-still="https://static.simonwillison.net/static/2025/skills_vs_mcps_still.gif"
  data-gif="https://static.simonwillison.net/static/2025/skills_vs_mcps.gif"
  data-state="stopped"
  role="button"
  aria-pressed="false"
  tabindex="0"
  style="cursor:pointer;max-width:100%"
  onload="(new Image).src=this.getAttribute('data-gif')"
  onclick="(function(el){
    if (el.getAttribute('data-state') !== 'playing') {
      var c = el.cloneNode(true);
      c.src = el.getAttribute('data-gif');
      c.setAttribute('data-state','playing');
      c.setAttribute('aria-pressed','true');
      el.parentNode.replaceChild(c, el);
    } else {
      el.setAttribute('data-state','stopped');
      el.setAttribute('aria-pressed','false');
      el.src = el.getAttribute('data-still');
    }
  })(this)"
  onkeydown="if(event.key===' '||event.key==='Enter'){event.preventDefault();this.onclick(event);}"
/></p>
<p>OK, this particular GIF is terrible, but the great thing about skills is that they're very easy to iterate on to make them better.</p>
<p>Here are some noteworthy snippets from <a href="https://gist.github.com/simonw/ef35bb9e6c514d1d596dac9227da482b">the Python script it wrote</a>, comments mine:</p>
<pre><span class="pl-c"># Start by adding the skill's directory to the Python path</span>
<span class="pl-k">import</span> <span class="pl-s1">sys</span>
<span class="pl-s1">sys</span>.<span class="pl-c1">path</span>.<span class="pl-c1">insert</span>(<span class="pl-c1">0</span>, <span class="pl-s">'/mnt/skills/examples/slack-gif-creator'</span>)

<span class="pl-k">from</span> <span class="pl-c1">PIL</span> <span class="pl-k">import</span> <span class="pl-v">Image</span>, <span class="pl-v">ImageDraw</span>, <span class="pl-v">ImageFont</span>
<span class="pl-c"># This class lives in the core/ directory for the skill</span>
<span class="pl-k">from</span> <span class="pl-s1">core</span>.<span class="pl-s1">gif_builder</span> <span class="pl-k">import</span> <span class="pl-v">GIFBuilder</span>

<span class="pl-c"># ... code that builds the GIF ...</span>

<span class="pl-c"># Save it to disk:</span>
<span class="pl-s1">info</span> <span class="pl-c1">=</span> <span class="pl-s1">builder</span>.<span class="pl-c1">save</span>(<span class="pl-s">'/mnt/user-data/outputs/skills_vs_mcps.gif'</span>, 
                    <span class="pl-s1">num_colors</span><span class="pl-c1">=</span><span class="pl-c1">128</span>, 
                    <span class="pl-s1">optimize_for_emoji</span><span class="pl-c1">=</span><span class="pl-c1">False</span>)

<span class="pl-en">print</span>(<span class="pl-s">f"GIF created successfully!"</span>)
<span class="pl-en">print</span>(<span class="pl-s">f"Size: <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">info</span>[<span class="pl-s">'size_kb'</span>]:.1f<span class="pl-kos">}</span></span> KB (<span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">info</span>[<span class="pl-s">'size_mb'</span>]:.2f<span class="pl-kos">}</span></span> MB)"</span>)
<span class="pl-en">print</span>(<span class="pl-s">f"Frames: <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">info</span>[<span class="pl-s">'frame_count'</span>]<span class="pl-kos">}</span></span>"</span>)
<span class="pl-en">print</span>(<span class="pl-s">f"Duration: <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">info</span>[<span class="pl-s">'duration_seconds'</span>]:.1f<span class="pl-kos">}</span></span>s"</span>)

<span class="pl-c"># Use the check_slack_size() function to confirm it's small enough for Slack:</span>
<span class="pl-s1">passes</span>, <span class="pl-s1">check_info</span> <span class="pl-c1">=</span> <span class="pl-en">check_slack_size</span>(<span class="pl-s">'/mnt/user-data/outputs/skills_vs_mcps.gif'</span>, <span class="pl-s1">is_emoji</span><span class="pl-c1">=</span><span class="pl-c1">False</span>)
<span class="pl-k">if</span> <span class="pl-s1">passes</span>:
    <span class="pl-en">print</span>(<span class="pl-s">"✓ Ready for Slack!"</span>)
<span class="pl-k">else</span>:
    <span class="pl-en">print</span>(<span class="pl-s">f"⚠ File size: <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">check_info</span>[<span class="pl-s">'size_kb'</span>]:.1f<span class="pl-kos">}</span></span> KB (limit: <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">check_info</span>[<span class="pl-s">'limit_kb'</span>]<span class="pl-kos">}</span></span> KB)"</span>)</pre>
<p>This is pretty neat. Slack GIFs need to be a maximum of 2MB, so the skill includes a validation function which the model can use to check the file size. If it's too large the model can have another go at making it smaller.</p>
<h4 id="skills-depend-on-a-coding-environment">Skills depend on a coding environment</h4>
<p>The skills mechanism is <em>entirely dependent</em> on the model having access to a filesystem, tools to navigate it and the ability to execute commands in that environment.</p>
<p>This is a common pattern for LLM tooling these days - ChatGPT Code Interpreter was the first big example of this <a href="https://simonwillison.net/2023/Apr/12/code-interpreter/">back in early 2023</a>, and the pattern later extended to local machines via coding agent tools such as Cursor, Claude Code, Codex CLI and Gemini CLI.</p>
<p>This requirement is the biggest difference between skills and other previous attempts at expanding the abilities of LLMs, such as MCP and <a href="https://simonwillison.net/tags/chatgpt-plugins/">ChatGPT Plugins</a>. It's a significant dependency, but it's somewhat bewildering how much new capability it unlocks.</p>
<p>The fact that skills are so powerful and simple to create is yet another argument in favor of making safe coding environments available to LLMs. The word <strong>safe</strong> there is doing a <em>lot</em> of work though! We really need to figure out how best to sandbox these environments such that attacks such as prompt injections are limited to an acceptable amount of damage.</p>
<h4 id="claude-as-a-general-agent">Claude as a General Agent</h4>
<p>Back in January I <a href="https://simonwillison.net/2025/Jan/10/ai-predictions/">made some foolhardy predictions about AI/LLMs</a>, including that "agents" would once again fail to happen:</p>
<blockquote>
<p>I think we are going to see a <em>lot</em> more froth about agents in 2025, but I expect the results will be a great disappointment to most of the people who are excited about this term. I expect a lot of money will be lost chasing after several different poorly defined dreams that share that name.</p>
</blockquote>
<p>I was entirely wrong about that. 2025 really has been the year of "agents", no matter which of the many <a href="https://simonwillison.net/tags/agent-definitions/">conflicting definitions</a> you decide to use (I eventually settled on "<a href="https://simonwillison.net/2025/Sep/18/agents/">tools in a loop</a>").</p>
<p><a href="https://www.claude.com/product/claude-code">Claude Code</a> is, with hindsight, poorly named. It's not purely a coding tool: it's a tool for general computer automation. <em>Anything</em> you can achieve by typing commands into a computer is something that can now be automated by Claude Code. It's best described as a <strong>general agent</strong>. Skills make this a whole lot more obvious and explicit.</p>
<p>I find the potential applications of this trick somewhat dizzying. Just thinking about this with my data journalism hat on: imagine a folder full of skills that covers tasks like the following:</p>
<ul>
<li>Where to get US census data from and how to understand its structure</li>
<li>How to load data from different formats into SQLite or DuckDB using appropriate Python libraries</li>
<li>How to publish data online, as Parquet files in S3 or pushed as tables to Datasette Cloud</li>
<li>A skill defined by an experienced data reporter talking about how best to find the interesting stories in a new set of data</li>
<li>A skill that describes how to build clean, readable data visualizations using D3</li>
</ul>
<p>Congratulations, you just built a "data journalism agent" that can discover and help publish stories against fresh drops of US census data. And you did it with a folder full of Markdown files and maybe a couple of example Python scripts.</p>
<h4 id="skills-compared-to-mcp">Skills compared to MCP</h4>
<p><a href="https://modelcontextprotocol.io/">Model Context Protocol</a> has attracted an enormous amount of buzz since its initial release back <a href="https://simonwillison.net/2024/Nov/25/model-context-protocol/">in November last year</a>. I like to joke that one of the reasons it took off is that every company knew they needed an "AI strategy", and building (or announcing) an MCP implementation was an easy way to tick that box.</p>
<p>Over time the limitations of MCP have started to emerge. The most significant is in terms of token usage: GitHub's official MCP on its own famously consumes tens of thousands of tokens of context, and once you've added a few more to that there's precious little space left for the LLM to actually do useful work.</p>
<p>My own interest in MCPs has waned ever since I started taking coding agents seriously. Almost everything I might achieve with an MCP can be handled by a CLI tool instead. LLMs know how to call <code>cli-tool --help</code>, which means you don't have to spend many tokens describing how to use them - the model can figure it out later when it needs to.</p>
<p>Skills have exactly the same advantage, only now I don't even need to implement a new CLI tool. I can drop a Markdown file in describing how to do a task instead, adding extra scripts only if they'll help make things more reliable or efficient.</p>
<h4 id="here-come-the-skills">Here come the Skills</h4>
<p>One of the most exciting things about Skills is how easy they are to share. I expect many skills will be implemented as a single file - more sophisticated ones will be a folder with a few more.</p>
<p>Anthropic have <a href="https://docs.claude.com/en/docs/agents-and-tools/agent-skills/overview">Agent Skills documentation</a> and a <a href="https://github.com/anthropics/claude-cookbooks/tree/main/skills">Claude Skills Cookbook</a>. I'm already thinking through ideas of skills I might build myself, like one on <a href="https://simonwillison.net/2025/Oct/8/claude-datasette-plugins/">how to build Datasette plugins</a>.</p>
<p>Something else I love about the design of skills is there is nothing at all preventing them from being used with other models.</p>
<p>You can grab a skills folder right now, point Codex CLI or Gemini CLI at it and say "read pdf/SKILL.md and then create me a PDF describing this project" and it will work, despite those tools and models having no baked in knowledge of the skills system.</p>
<p>I expect we'll see a Cambrian explosion in Skills which will make this year's MCP rush look pedestrian by comparison.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/prompt-engineering">prompt-engineering</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/code-interpreter">code-interpreter</a>, <a href="https://simonwillison.net/tags/ai-agents">ai-agents</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/skills">skills</a></p>]]></description><pubDate>Thu, 16 Oct 2025 21:25:18 +0000</pubDate></item><item><title>NVIDIA DGX Spark + Apple Mac Studio = 4x Faster LLM Inference with EXO 1.0</title><link>https://simonwillison.net/2025/Oct/16/nvidia-dgx-spark-apple-mac-studio/#atom-everything</link><description><![CDATA[<p><strong><a href="https://blog.exolabs.net/nvidia-dgx-spark">NVIDIA DGX Spark + Apple Mac Studio = 4x Faster LLM Inference with EXO 1.0</a></strong></p>
EXO Labs wired a 256GB M3 Ultra Mac Studio up to an NVIDIA DGX Spark and got a 2.8x performance boost serving Llama-3.1 8B (FP16) with an 8,192 token prompt.</p>
<p>Their detailed explanation taught me a lot about LLM performance.</p>
<p>There are two key steps in executing a prompt. The first is the <strong>prefill</strong> phase that reads the incoming prompt and builds a KV cache for each of the transformer layers in the model. This is compute-bound as it needs to process every token in the input and perform large matrix multiplications across all of the layers to initialize the model's internal state.</p>
<p>Performance in the prefill stage influences TTFT - time‑to‑first‑token.</p>
<p>The second step is the <strong>decode</strong> phase, which generates the output one token at a time. This part is limited by memory bandwidth - there's less arithmetic, but each token needs to consider the entire KV cache.</p>
<p>Decode performance influences TPS - tokens per second.</p>
<p>EXO noted that the Spark has 100 TFLOPS but only 273GB/s of memory bandwidth, making it a better fit for prefill. The M3 Ultra has 26 TFLOPS but 819GB/s of memory bandwidth, making it ideal for the decode phase.</p>
<p>They run prefill on the Spark, streaming the KV cache to the Mac over 10Gb Ethernet. They can start streaming earlier layers while the later layers are still being calculated. Then the Mac runs the decode phase, returning tokens faster than if the Spark had run the full process end-to-end.

    <p><small></small>Via <a href="https://twitter.com/exolabs/status/1978525767739883736">@exolabs</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/apple">apple</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/local-llms">local-llms</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/nvidia">nvidia</a></p>]]></description><pubDate>Thu, 16 Oct 2025 05:34:41 +0000</pubDate></item><item><title>Quoting Riana Pfefferkorn</title><link>https://simonwillison.net/2025/Oct/16/riana-pfefferkorn/#atom-everything</link><description><![CDATA[<blockquote cite="https://cyberlaw.stanford.edu/blog/2025/10/whos-submitting-ai-tainted-filings-in-court/"><p>Pro se litigants [people representing themselves in court without a lawyer] account for the majority of the cases in the United States where a party submitted a court filing containing AI hallucinations. In a country where legal representation is <a href="https://law.stanford.edu/2024/06/13/justice-for-all-why-we-have-an-access-to-justice-gap-in-america-and-what-can-we-do-about-it/">unaffordable</a> for most people, it is no wonder that pro se litigants are depending on free or low-cost AI tools. But it is a scandal that so many have been betrayed by them, to the detriment of the cases they are litigating all on their own.</p></blockquote>
<p class="cite">&mdash; <a href="https://cyberlaw.stanford.edu/blog/2025/10/whos-submitting-ai-tainted-filings-in-court/">Riana Pfefferkorn</a>, analyzing the <a href="https://www.damiencharlotin.com/hallucinations/">AI Hallucination Cases</a> database for CIS at Stanford Law</p>

    <p>Tags: <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/law">law</a>, <a href="https://simonwillison.net/tags/hallucinations">hallucinations</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Thu, 16 Oct 2025 04:16:45 +0000</pubDate></item><item><title>Coding without typing the code</title><link>https://simonwillison.net/2025/Oct/16/coding-without-typing-the-code/#atom-everything</link><description><![CDATA[<p>Last year the most useful exercise for getting a feel for how good LLMs were at writing code was vibe coding (before that name had even been coined) - seeing if you could create a useful small application through prompting alone.</p>
<p>Today I think there's a new, more ambitious and significantly more intimidating exercise: spend a day working on <strong>real production code</strong> through prompting alone, making no manual edits yourself.</p>
<p>This doesn't mean you can't control exactly what goes into each file - you can even tell the model "update line 15 to use this instead" if you have to - but it's a great way to get more of a feel for how well the latest coding agents can wield their edit tools.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Thu, 16 Oct 2025 03:58:31 +0000</pubDate></item><item><title>Quoting Catherine Wu</title><link>https://simonwillison.net/2025/Oct/15/catherine-wu/#atom-everything</link><description><![CDATA[<blockquote cite="https://twitter.com/_catwu/status/1978509174897053925"><p>While Sonnet 4.5 remains the default [in Claude Code], Haiku 4.5 now powers the Explore subagent which can rapidly gather context on your codebase to build apps even faster.</p>
<p>You can select Haiku 4.5 to be your default model in /model. When selected, you’ll automatically use Sonnet 4.5 in Plan mode and Haiku 4.5 for execution for smarter plans and faster results.</p></blockquote>
<p class="cite">&mdash; <a href="https://twitter.com/_catwu/status/1978509174897053925">Catherine Wu</a>, Claude Code PM, Anthropic</p>

    <p>Tags: <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/sub-agents">sub-agents</a></p>]]></description><pubDate>Wed, 15 Oct 2025 20:34:09 +0000</pubDate></item><item><title>Introducing Claude Haiku 4.5</title><link>https://simonwillison.net/2025/Oct/15/claude-haiku-45/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.anthropic.com/news/claude-haiku-4-5">Introducing Claude Haiku 4.5</a></strong></p>
Anthropic released Claude Haiku 4.5 today, the cheapest member of the Claude 4.5 family that started with Sonnet 4.5 <a href="https://simonwillison.net/2025/Sep/29/claude-sonnet-4-5/">a couple of weeks ago</a>.</p>
<p>It's priced at $1/million input tokens and $5/million output tokens, slightly more expensive than Haiku 3.5 ($0.80/$4) and a <em>lot</em> more expensive than the original Claude 3 Haiku ($0.25/$1.25), both of which remain available at those prices.</p>
<p>It's a third of the price of Sonnet 4 and Sonnet 4.5 (both $3/$15) which is notable because Anthropic's benchmarks put it in a similar space to that older Sonnet 4 model. As they put it:</p>
<blockquote>
<p>What was recently at the frontier is now cheaper and faster. Five months ago, Claude Sonnet 4 was a state-of-the-art model. Today, Claude Haiku 4.5 gives you similar levels of coding performance but at one-third the cost and more than twice the speed.</p>
</blockquote>
<p>I've been hoping to see Anthropic release a fast, inexpensive model that's price competitive with the cheapest models from OpenAI and Gemini, currently $0.05/$0.40 (GPT-5-Nano) and $0.075/$0.30 (Gemini 2.0 Flash Lite). Haiku 4.5 certainly isn't that, it looks like they're continuing to focus squarely on the "great at code" part of the market.</p>
<p>The new Haiku is the first Haiku model to support reasoning. It sports a 200,000 token context window, 64,000 maximum output (up from just 8,192 for Haiku 3.5) and a "reliable knowledge cutoff" of February 2025, one month later than the January 2025 date for Sonnet 4 and 4.5 and Opus 4 and 4.1.</p>
<p>Something that caught my eye in the accompanying <a href="https://assets.anthropic.com/m/99128ddd009bdcb/original/Claude-Haiku-4-5-System-Card.pdf">system card</a> was this note about context length:</p>
<blockquote>
<p>For Claude Haiku 4.5, we trained the model to be explicitly context-aware, with precise information about how much context-window has been used. This has two effects: the model learns when and how to wrap up its answer when the limit is approaching, and the model learns to continue reasoning more persistently when the limit is further away. We found this intervention—along with others—to be effective at limiting agentic “laziness” (the phenomenon where models stop working on a problem prematurely, give incomplete answers, or cut corners on tasks).</p>
</blockquote>
<p>I've added the new price to <a href="https://www.llm-prices.com/">llm-prices.com</a>, released <a href="https://github.com/simonw/llm-anthropic/releases/tag/0.20">llm-anthropic 0.20</a> with the new model and updated my <a href="https://tools.simonwillison.net/haiku">Haiku-from-your-webcam</a> demo (<a href="https://github.com/simonw/tools/blob/main/haiku.html">source</a>) to use Haiku 4.5 as well.</p>
<p>Here's <code>llm -m claude-haiku-4.5 'Generate an SVG of a pelican riding a bicycle'</code> (<a href="https://gist.github.com/simonw/31256c523fa502eeb303b8e0bbe30eee">transcript</a>).</p>
<p><img alt="Described by Haiku 4.5: A whimsical illustration of a bird with a round tan body, pink beak, and orange legs riding a bicycle against a blue sky and green grass background." src="https://static.simonwillison.net/static/2025/claude-haiku-4.5-pelican.jpg" /></p>
<p>18 input tokens and 1513 output tokens = <a href="https://www.llm-prices.com/#it=18&amp;ot=1513&amp;ic=1&amp;oc=5">0.7583 cents</a>.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=45595403">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/llm">llm</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/llm-pricing">llm-pricing</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-reasoning">llm-reasoning</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a></p>]]></description><pubDate>Wed, 15 Oct 2025 19:36:34 +0000</pubDate></item><item><title>Quoting Claude Haiku 4.5 System Card</title><link>https://simonwillison.net/2025/Oct/15/system-card/#atom-everything</link><description><![CDATA[<blockquote cite="https://assets.anthropic.com/m/99128ddd009bdcb/original/Claude-Haiku-4-5-System-Card.pdf"><p>Previous system cards have reported results on an expanded version of our earlier <a href="https://www.anthropic.com/research/agentic-misalignment">agentic misalignment evaluation suite</a>: three families of exotic scenarios meant to elicit the model to commit blackmail, attempt a murder, and frame someone for financial crimes. We choose not to report full results here because, similarly to Claude Sonnet 4.5, Claude Haiku 4.5 showed many clear examples of verbalized evaluation awareness on all three of the scenarios tested in this suite. Since the suite only consisted of many similar variants of three core scenarios, we expect that the model maintained high unverbalized awareness across the board, and we do not trust it to be representative of behavior in the real extreme situations the suite is meant to emulate.</p></blockquote>
<p class="cite">&mdash; <a href="https://assets.anthropic.com/m/99128ddd009bdcb/original/Claude-Haiku-4-5-System-Card.pdf">Claude Haiku 4.5 System Card</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Wed, 15 Oct 2025 19:17:36 +0000</pubDate></item><item><title>A modern approach to preventing CSRF in Go</title><link>https://simonwillison.net/2025/Oct/15/csrf-in-go/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.alexedwards.net/blog/preventing-csrf-in-go">A modern approach to preventing CSRF in Go</a></strong></p>
Alex Edwards writes about the new <code>http.CrossOriginProtection</code> middleware that was added to the Go standard library in <a href="https://tip.golang.org/doc/go1.25">version 1.25</a> in August and asks:</p>
<blockquote>
<p>Have we finally reached the point where CSRF attacks can be prevented without relying on a token-based check (like double-submit cookies)?</p>
</blockquote>
<p>It looks like the answer might be <em>yes</em>, which is extremely exciting. I've been <a href="https://simonwillison.net/tags/csrf/">tracking CSRF</a> since I first learned about it <a href="https://simonwillison.net/2005/May/6/bad/">20 years ago in May 2005</a> and a cleaner solution than those janky hidden form fields would be very welcome.</p>
<p>The code for the new Go middleware lives in <a href="https://github.com/golang/go/blob/go1.25.0/src/net/http/csrf.go">src/net/http/csrf.go</a>. It works using the <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/Sec-Fetch-Site">Sec-Fetch-Site</a> HTTP header, which Can I Use shows as having <a href="https://caniuse.com/mdn-http_headers_sec-fetch-site">94.18%</a> global availability - the holdouts are mainly IE11, iOS versions prior to iOS 17 (which came out in 2023 but can be installed on any phone released since 2017) and some other ancient browser versions.</p>
<p>If <code>Sec-Fetch-Site</code> is <code>same-origin</code> or <code>none</code> then the page submitting the form was either on the same origin or was navigated to directly by the user - in both cases safe from CSRF. If it's <code>cross-site</code> or <code>same-site</code> (<code>tools.simonwillison.net</code> and <code>til.simonwillison.net</code> are considered <code>same-site</code> but not <code>same-origin</code>) the submission is denied.</p>
<p>If that header isn't available the middleware falls back on comparing other headers: <code>Origin</code> - a value like <code>https://simonwillison.net</code> - with <code>Host</code>, a value like <code>simonwillison.net</code>. This should cover the tiny fraction of browsers that don't have the new header, though it's not clear to me if there are any weird edge-cases beyond that.</p>
<p>Note that this fallback comparison can't take the scheme into account since <code>Host</code> doesn't list that, so administrators are encouraged to use <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/Strict-Transport-Security">HSTS</a> to protect against HTTP to HTTPS cross-origin requests.</p>
<p>On Lobste.rs I questioned if this would work for <code>localhost</code>, since that normally isn't served using HTTPS. Firefox security engineer Frederik Braun <a href="https://lobste.rs/s/fzw9g7/modern_approach_preventing_csrf_go#c_e24o9q">reassured me</a> that <code>*.localhost</code> is treated as a Secure Context, so gets the <code>Sec-Fetch-Site</code> header despite not being served via HTTPS.</p>
<p><strong>Update</strong>: Also relevant is <a href="https://words.filippo.io/csrf/">Filippo Valsorda's article in CSRF</a> which includes detailed research conducted as part of building the new Go middleware, plus this related <a href="https://bsky.app/profile/filippo.abyssdomain.expert/post/3lmyu7c25zq2o">Bluesky conversation</a> about that research from six months ago.

    <p><small></small>Via <a href="https://lobste.rs/s/fzw9g7/modern_approach_preventing_csrf_go">lobste.rs</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/browsers">browsers</a>, <a href="https://simonwillison.net/tags/csrf">csrf</a>, <a href="https://simonwillison.net/tags/go">go</a>, <a href="https://simonwillison.net/tags/security">security</a>, <a href="https://simonwillison.net/tags/filippo-valsorda">filippo-valsorda</a></p>]]></description><pubDate>Wed, 15 Oct 2025 05:03:46 +0000</pubDate></item><item><title>NVIDIA DGX Spark: great hardware, early days for the ecosystem</title><link>https://simonwillison.net/2025/Oct/14/nvidia-dgx-spark/#atom-everything</link><description><![CDATA[<p>NVIDIA sent me a preview unit of their new <a href="https://www.nvidia.com/en-us/products/workstations/dgx-spark/">DGX Spark</a> desktop "AI supercomputer". I've never had hardware to review before! You can consider this my first ever sponsored post if you like, but they did not pay me any cash and aside from an embargo date they did not request (nor would I grant) any editorial input into what I write about the device.</p>
<p>The device retails for around $4,000. They officially go on sale tomorrow.</p>
<p>First impressions are that this is a snazzy little computer. It's similar in size to a Mac mini, but with an exciting textured surface that feels refreshingly different and a little bit <a href="https://www.indiewire.com/awards/industry/devs-cinematography-rob-hardy-alex-garland-1234583396/">science fiction</a>.</p>
<p><img src="https://static.simonwillison.net/static/2025/nvidia-spark.jpg" alt="A rectangular small computer, sitting horizontally on a box. It is about the width of a Mac Mini. It has a NVIDIA logo on  a reflective handle portion, then textured silver metal front, then another reflective handle at the other end. It's pretty and a bit weird looking. It sits on the box it came in, which has NVIDIA DGX Spark written on it in white text on green." style="max-width: 100%;" /></p>
<p>There is a <em>very</em> powerful machine tucked into that little box. Here are the specs, which I had Claude Code figure out for me by <a href="https://gist.github.com/simonw/021651a14e6c5bf9876c9c4244ed6c2d">poking around on the device itself</a>:</p>
<blockquote>
<p><strong>Hardware Specifications</strong></p>
<ul>
<li>Architecture: aarch64 (ARM64)</li>
<li>CPU: 20 cores
<ul>
<li>10x Cortex-X925 (performance cores)</li>
<li>10x Cortex-A725 (efficiency cores)</li>
</ul>
</li>
<li>RAM: 119 GB total (112 GB available) - <em>I’m not sure why Claude reported it differently here, the machine is listed as 128GB - it looks like a <a href="https://news.ycombinator.com/item?id=45586776#45588329">128GB == 119GiB thing</a> because Claude <a href="https://gist.github.com/simonw/021651a14e6c5bf9876c9c4244ed6c2d#file-nvidia-claude-code-txt-L41">used free -h</a></em></li>
<li>Storage: 3.7 TB (6% used, 3.3 TB available)</li>
</ul>
<p><strong>GPU Specifications</strong></p>
<ul>
<li>Model: NVIDIA GB10 (Blackwell architecture)</li>
<li>Compute Capability: sm_121 (12.1)</li>
<li>Memory: 119.68 GB</li>
<li>Multi-processor Count: 48 streaming multiprocessors</li>
<li>Architecture: Blackwell</li>
</ul>
</blockquote>
<p>Short version: this is an ARM64 device with 128GB of memory that's available to both the GPU and the 20 CPU cores at the same time, strapped onto a 4TB NVMe SSD.</p>
<p>The Spark is firmly targeted at “AI researchers”. It’s designed for both training and running models.</p>
<h4 id="the-tricky-bit-cuda-on-arm64">The tricky bit: CUDA on ARM64</h4>
<p>Until now almost all of my own model running experiments have taken place on a Mac. This has gotten far less painful over the past year and a half thanks to the amazing work of the <a href="https://simonwillison.net/tags/mlx/">MLX</a> team and community, but it's still left me deeply frustrated at my lack of access to the NVIDIA CUDA ecosystem. I've lost count of the number of libraries and tutorials which expect you to be able to use Hugging Face Transformers or PyTorch with CUDA, and leave you high and dry if you don't have an NVIDIA GPU to run things on.</p>
<p>Armed (ha) with my new NVIDIA GPU I was excited to dive into this world that had long eluded me... only to find that there was another assumption baked in to much of this software: x86 architecture for the rest of the machine.</p>
<p>This resulted in all kinds of unexpected new traps for me to navigate. I eventually managed to get a PyTorch 2.7 wheel for CUDA on ARM, but failed to do so for 2.8. I'm not confident there because the wheel itself is unavailable but I'm finding navigating the PyTorch ARM ecosystem pretty confusing.</p>
<p>NVIDIA are trying to make this easier, with mixed success. A lot of my initial challenges got easier when I found their <a href="https://docs.nvidia.com/dgx/dgx-spark/nvidia-container-runtime-for-docker.html">official Docker container</a>, so now I'm figuring out how best to use Docker with GPUs. Here's the current incantation that's been working for me:</p>
<div class="highlight highlight-source-shell"><pre>docker run -it --gpus=all \
  -v /usr/local/cuda:/usr/local/cuda:ro \
  nvcr.io/nvidia/cuda:13.0.1-devel-ubuntu24.04 \
  bash</pre></div>
<p>I have not yet got my head around the difference between CUDA 12 and 13. 13 appears to be very new, and a lot of the existing tutorials and libraries appear to expect 12.</p>
<h4 id="the-missing-documentation-isn-t-missing-any-more">The missing documentation isn't missing any more</h4>
<p>When I first received this machine around a month ago there was very little in the way of documentation to help get me started. This meant climbing the steep NVIDIA+CUDA learning curve mostly on my own.</p>
<p>This has changed <em>substantially</em> in just the last week. NVIDIA now have extensive guides for getting things working on the Spark and they are a huge breath of fresh air - exactly the information I needed when I started exploring this hardware.</p>
<p>Here's the <a href="https://developer.nvidia.com/topics/ai/dgx-spark">getting started guide</a>, details on the <a href="https://build.nvidia.com/spark/dgx-dashboard/instructions">DGX dashboard web app</a>, and the essential collection of <a href="https://build.nvidia.com/spark">playbooks</a>. There's still a lot I haven't tried yet just in this official set of guides.</p>
<h4 id="claude-code-for-everything">Claude Code for everything</h4>
<p><a href="https://www.claude.com/product/claude-code">Claude Code</a> was an absolute lifesaver for me while I was trying to figure out how best to use this device. My Ubuntu skills were a little rusty, and I also needed to figure out CUDA drivers and Docker incantations and how to install the right versions of PyTorch. Claude 4.5 Sonnet is <em>much better than me</em> at all of these things.</p>
<p>Since many of my experiments took place in disposable Docker containers I had no qualms at all about running it in YOLO mode:</p>
<div class="highlight highlight-source-shell"><pre>IS_SANDBOX=1 claude --dangerously-skip-permissions</pre></div>

<p>The <code>IS_SANDBOX=1</code> environment variable stops Claude from complaining about running as root.</p>

<details><summary style="font-style: italic">Before I found out about IS_SANDBOX</summary>

<p><br /><em>I was <a href="https://twitter.com/lawrencecchen/status/1978255934938886409">tipped off</a> about IS_SANDBOX after I published this article. Here's my original workaround:</em></p>

<p>Claude understandably won't let you do this as root, even in a Docker container, so I found myself using the following incantation in a fresh <code>nvcr.io/nvidia/cuda:13.0.1-devel-ubuntu24.04</code> instance pretty often:</p>
<div class="highlight highlight-source-shell"><pre>apt-get update <span class="pl-k">&amp;&amp;</span> apt-get install -y sudo
<span class="pl-c"><span class="pl-c">#</span> pick the first free UID &gt;=1000</span>
U=<span class="pl-s"><span class="pl-pds">$(</span>for i <span class="pl-k">in</span> <span class="pl-s"><span class="pl-pds">$(</span>seq 1000 65000<span class="pl-pds">)</span></span><span class="pl-k">;</span> <span class="pl-k">do</span> <span class="pl-k">if</span> <span class="pl-k">!</span> getent passwd <span class="pl-smi">$i</span> <span class="pl-k">&gt;</span>/dev/null<span class="pl-k">;</span> <span class="pl-k">then</span> <span class="pl-c1">echo</span> <span class="pl-smi">$i</span><span class="pl-k">;</span> <span class="pl-c1">break</span><span class="pl-k">;</span> <span class="pl-k">fi</span><span class="pl-k">;</span> done<span class="pl-pds">)</span></span>
<span class="pl-c1">echo</span> <span class="pl-s"><span class="pl-pds">"</span>Chosen UID: <span class="pl-smi">$U</span><span class="pl-pds">"</span></span>
<span class="pl-c"><span class="pl-c">#</span> same for a GID</span>
G=<span class="pl-s"><span class="pl-pds">$(</span>for i <span class="pl-k">in</span> <span class="pl-s"><span class="pl-pds">$(</span>seq 1000 65000<span class="pl-pds">)</span></span><span class="pl-k">;</span> <span class="pl-k">do</span> <span class="pl-k">if</span> <span class="pl-k">!</span> getent group <span class="pl-smi">$i</span> <span class="pl-k">&gt;</span>/dev/null<span class="pl-k">;</span> <span class="pl-k">then</span> <span class="pl-c1">echo</span> <span class="pl-smi">$i</span><span class="pl-k">;</span> <span class="pl-c1">break</span><span class="pl-k">;</span> <span class="pl-k">fi</span><span class="pl-k">;</span> done<span class="pl-pds">)</span></span>
<span class="pl-c1">echo</span> <span class="pl-s"><span class="pl-pds">"</span>Chosen GID: <span class="pl-smi">$G</span><span class="pl-pds">"</span></span>
<span class="pl-c"><span class="pl-c">#</span> create user+group</span>
groupadd -g <span class="pl-s"><span class="pl-pds">"</span><span class="pl-smi">$G</span><span class="pl-pds">"</span></span> devgrp
useradd -m -u <span class="pl-s"><span class="pl-pds">"</span><span class="pl-smi">$U</span><span class="pl-pds">"</span></span> -g <span class="pl-s"><span class="pl-pds">"</span><span class="pl-smi">$G</span><span class="pl-pds">"</span></span> -s /bin/bash dev
<span class="pl-c"><span class="pl-c">#</span> enable password-less sudo:</span>
<span class="pl-c1">printf</span> <span class="pl-s"><span class="pl-pds">'</span>dev ALL=(ALL) NOPASSWD:ALL\n<span class="pl-pds">'</span></span> <span class="pl-k">&gt;</span> /etc/sudoers.d/90-dev-nopasswd
chmod 0440 /etc/sudoers.d/90-dev-nopasswd
<span class="pl-c"><span class="pl-c">#</span> Install npm</span>
DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC apt-get install -y npm
<span class="pl-c"><span class="pl-c">#</span> Install Claude</span>
npm install -g @anthropic-ai/claude-code</pre></div>
<p>Then switch to the <code>dev</code> user and run Claude for the first time:</p>
<div class="highlight highlight-source-shell"><pre>su - dev
claude --dangerously-skip-permissions</pre></div>

</details><br />

<p>This will provide a URL which you can visit to authenticate with your Anthropic account, confirming by copying back a token and pasting it into the terminal.</p>
<p>Docker tip: you can create a snapshot of the current image (with Claude installed) by running <code>docker ps</code> to get the container ID and then:</p>
<div class="highlight highlight-source-shell"><pre>docker commit --pause=false <span class="pl-k">&lt;</span>container_id<span class="pl-k">&gt;</span> cc:snapshot</pre></div>
<p>Then later you can start a similar container using:</p>
<div class="highlight highlight-source-shell"><pre>docker run -it \
  --gpus=all \
  -v /usr/local/cuda:/usr/local/cuda:ro \
  cc:snapshot bash</pre></div>
<p>Here's an example of the kinds of prompts I've been running in Claude Code inside the container:</p>
<blockquote>
<p><code>I want to run https://huggingface.co/unsloth/Qwen3-4B-GGUF using llama.cpp - figure out how to get llama cpp working on this machine  such that it runs with the GPU, then install it in this directory and get that model to work to serve a prompt. Goal is to get this  command to run: llama-cli -hf unsloth/Qwen3-4B-GGUF -p "I believe the meaning of life is" -n 128 -no-cnv</code></p>
</blockquote>
<p>That one worked flawlessly - Claude checked out the <code>llama.cpp</code> repo, compiled it for me and iterated on it until it could run that model on the GPU. Here's a <a href="https://gist.github.com/simonw/3e7d28d9ed222d842f729bfca46d6673">full transcript</a>, converted from Claude's <code>.jsonl</code> log format to Markdown using a script I <a href="https://github.com/simonw/tools/blob/main/python/claude_to_markdown.py">vibe coded just now</a>.</p>
<p>I later told it:</p>
<blockquote>
<p><code>Write out a markdown file with detailed notes on what you did. Start with the shortest form of notes on how to get a successful build, then add a full account of everything you tried, what went wrong and how you fixed it.</code></p>
</blockquote>
<p>Which produced <a href="https://gist.github.com/simonw/0942d96f616b9e328568ab27d911c8ed">this handy set of notes</a>.</p>
<h4 id="tailscale-was-made-for-this">Tailscale was made for this</h4>
<p>Having a machine like this on my local network is neat, but what's even neater is being able to access it from anywhere else in the world, from both my phone and my laptop.</p>
<p><a href="https://tailscale.com/">Tailscale</a> is <em>perfect</em> for this. I installed it on the Spark (using the <a href="https://tailscale.com/kb/1031/install-linux">Ubuntu instructions here</a>), signed in with my SSO account (via Google)... and the Spark showed up in the "Network Devices" panel on my laptop and phone instantly.</p>
<p>I can SSH in from my laptop or using the <a href="https://termius.com/free-ssh-client-for-iphone">Termius iPhone app</a> on my phone. I've also been running tools like <a href="https://openwebui.com/">Open WebUI</a> which give me a mobile-friendly web interface for interacting with LLMs on the Spark.</p>
<h4 id="here-comes-the-ecosystem">Here comes the ecosystem</h4>
<p>The embargo on these devices dropped yesterday afternoon, and it turns out a whole bunch of relevant projects have had similar preview access to myself. This is <em>fantastic news</em> as many of the things I've been trying to figure out myself suddenly got a whole lot easier.</p>
<p>Four particularly notable examples:</p>
<ul>
<li>Ollama <a href="https://ollama.com/blog/nvidia-spark">works out of the box</a>. They actually had a build that worked a few weeks ago, and were the first success I had running an LLM on the machine.</li>
<li>
<code>llama.cpp</code> creator Georgi Gerganov just published  <a href="https://github.com/ggml-org/llama.cpp/discussions/16578">extensive benchmark results</a> from running <code>llama.cpp</code> on a Spark. He's getting ~3,600 tokens/second to read the prompt and ~59 tokens/second to generate a response with the MXFP4 version of GPT-OSS 20B and ~817 tokens/second to read and ~18 tokens/second to generate for GLM-4.5-Air-GGUF.</li>
<li>LM Studio now have <a href="https://lmstudio.ai/blog/dgx-spark">a build for the Spark</a>. I haven't tried this one yet as I'm currently using my machine exclusively via SSH.</li>
<li>vLLM - one of the most popular engines for serving production LLMs - had <a href="https://x.com/eqhylxx/status/1977928690945360049">early access</a> and there's now an official <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/vllm?version=25.09-py3">NVIDIA vLLM NGC Container</a> for running their stack.</li>
</ul>
<p>Here's <a href="https://docs.unsloth.ai/new/fine-tuning-llms-with-nvidia-dgx-spark-and-unsloth">a tutorial from Unsloth</a> on fine-tuning gpt-oss-20b on the Spark.</p>
<h4 id="should-you-get-one-">Should you get one?</h4>
<p>It's a bit too early for me to provide a confident recommendation concerning this machine. As indicated above, I've had a tough time figuring out how best to put it to use, largely through my own inexperience with CUDA, ARM64 and Ubuntu GPU machines in general.</p>
<p>The ecosystem improvements in just the past 24 hours have been very reassuring though. I expect it will be clear within a few weeks how well supported this machine is going to be.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/hardware">hardware</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/tailscale">tailscale</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/local-llms">local-llms</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/nvidia">nvidia</a>, <a href="https://simonwillison.net/tags/ollama">ollama</a>, <a href="https://simonwillison.net/tags/llama-cpp">llama-cpp</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/lm-studio">lm-studio</a>, <a href="https://simonwillison.net/tags/disclosures">disclosures</a></p>]]></description><pubDate>Tue, 14 Oct 2025 23:36:21 +0000</pubDate></item><item><title>Just Talk To It - the no-bs Way of Agentic Engineering</title><link>https://simonwillison.net/2025/Oct/14/agentic-engineering/#atom-everything</link><description><![CDATA[<p><strong><a href="https://steipete.me/posts/just-talk-to-it">Just Talk To It - the no-bs Way of Agentic Engineering</a></strong></p>
Peter Steinberger's long, detailed description of his current process for using Codex CLI and GPT-5 Codex. This is information dense and full of actionable tips, plus plenty of strong opinions about the differences between Claude 4.5 an GPT-5:</p>
<blockquote>
<p>While Claude reacts well to 🚨 SCREAMING ALL-CAPS 🚨 commands that threaten it that it will imply ultimate failure and 100 kittens will die if it runs command X, that freaks out GPT-5. (Rightfully so). So drop all of that and just use words like a human.</p>
</blockquote>
<p>Peter is a <em>heavy</em> user of parallel agents:</p>
<blockquote>
<p>I've completely moved to <code>codex</code> cli as daily driver. I run between 3-8 in parallel in a 3x3 terminal grid, most of them <a href="https://x.com/steipete/status/1977771686176174352">in the same folder</a>, some experiments go in separate folders. I experimented with worktrees, PRs but always revert back to this setup as it gets stuff done the fastest.</p>
</blockquote>
<p>He shares my preference for CLI utilities over MCPs:</p>
<blockquote>
<p>I can just refer to a cli by name. I don't need any explanation in my agents file. The agent will try $randomcrap on the first call, the cli will present the help menu, context now has full info how this works and from now on we good. I don't have to pay a price for any tools, unlike MCPs which are a constant cost and garbage in my context. Use GitHub's MCP and see 23k tokens gone. Heck, they did make it better because it was almost 50.000 tokens when it first launched. Or use the <code>gh</code> cli which has basically the same feature set, models already know how to use it, and pay zero context tax.</p>
</blockquote>
<p>It's worth reading the <a href="https://steipete.me/posts/just-talk-to-it#do-you-do-spec-driven-development">section on why he abandoned spec driven development</a> in full.


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/model-context-protocol">model-context-protocol</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/codex-cli">codex-cli</a>, <a href="https://simonwillison.net/tags/parallel-agents">parallel-agents</a></p>]]></description><pubDate>Tue, 14 Oct 2025 21:26:40 +0000</pubDate></item><item><title>nanochat</title><link>https://simonwillison.net/2025/Oct/13/nanochat/#atom-everything</link><description><![CDATA[<p><strong><a href="https://github.com/karpathy/nanochat">nanochat</a></strong></p>
Really interesting new project from Andrej Karpathy, described at length <a href="https://github.com/karpathy/nanochat/discussions/1">in this discussion post</a>.</p>
<p>It provides a full ChatGPT-style LLM, including training, inference and a web Ui, that can be trained for as little as $100:</p>
<blockquote>
<p>This repo is a full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase.</p>
</blockquote>
<p>It's around 8,000 lines of code, mostly Python (using PyTorch) plus a little bit of Rust for <a href="https://github.com/karpathy/nanochat/tree/master/rustbpe">training the tokenizer</a>.</p>
<p>Andrej suggests renting a 8XH100 NVIDA node for around $24/ hour to train the model. 4 hours (~$100) is enough to get a model that can hold a conversation - <a href="https://twitter.com/karpathy/status/1977755430093980034">almost coherent example here</a>. Run it for 12 hours and you get something that slightly outperforms GPT-2. I'm looking forward to hearing results from longer training runs!</p>
<p>The resulting model is ~561M parameters, so it should run on almost anything. I've run a 4B model on my iPhone, 561M should easily fit on even an inexpensive Raspberry Pi.</p>
<p>The model defaults to training on ~24GB from <a href="https://huggingface.co/datasets/karpathy/fineweb-edu-100b-shuffle">karpathy/fineweb-edu-100b-shuffle</a> derived from <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu">FineWeb-Edu</a>, and then <a href="https://github.com/karpathy/nanochat/blob/5fd0b138860a76beb60cf099fa46f74191b50941/scripts/mid_train.py">midtrains</a> on 568K examples from <a href="https://huggingface.co/datasets/HuggingFaceTB/smol-smoltalk">SmolTalk</a> (460K), <a href="https://huggingface.co/datasets/cais/mmlu">MMLU auxiliary train</a> (100K), and <a href="https://huggingface.co/datasets/openai/gsm8k">GSM8K</a> (8K), followed by <a href="https://github.com/karpathy/nanochat/blob/5fd0b138860a76beb60cf099fa46f74191b50941/scripts/chat_sft.py">supervised finetuning</a> on 21.4K examples from <a href="https://huggingface.co/datasets/allenai/ai2_arc#arc-easy-1">ARC-Easy</a> (2.3K), <a href="https://huggingface.co/datasets/allenai/ai2_arc#arc-challenge">ARC-Challenge</a> (1.1K), <a href="https://huggingface.co/datasets/openai/gsm8k">GSM8K</a> (8K), and <a href="https://huggingface.co/datasets/HuggingFaceTB/smol-smoltalk">SmolTalk</a> (10K).</p>
<p>Here's the code for the <a href="https://github.com/karpathy/nanochat/blob/5fd0b138860a76beb60cf099fa46f74191b50941/scripts/chat_web.py">web server</a>, which is fronted by this pleasantly succinct vanilla JavaScript <a href="https://github.com/karpathy/nanochat/blob/5fd0b138860a76beb60cf099fa46f74191b50941/nanochat/ui.html">HTML+JavaScript frontend</a>.</p>
<p><strong>Update</strong>: Sam Dobson pushed a build of the model to <a href="https://huggingface.co/sdobson/nanochat">sdobson/nanochat</a> on Hugging Face. It's designed to run on CUDA but I pointed Claude Code at a checkout and had it hack around until it figured out how to run it on CPU on macOS, which eventually resulted in <a href="https://gist.github.com/simonw/912623bf00d6c13cc0211508969a100a">this script</a> which I've published as a Gist. You should be able to try out the model using uv like this:</p>
<pre><code>cd /tmp
git clone https://huggingface.co/sdobson/nanochat
uv run https://gist.githubusercontent.com/simonw/912623bf00d6c13cc0211508969a100a/raw/80f79c6a6f1e1b5d4485368ef3ddafa5ce853131/generate_cpu.py \
--model-dir /tmp/nanochat \
--prompt "Tell me about dogs."
</code></pre>
<p>I got this (truncated because it ran out of tokens):</p>
<blockquote>
<p>I'm delighted to share my passion for dogs with you. As a veterinary doctor, I've had the privilege of helping many pet owners care for their furry friends. There's something special about training, about being a part of their lives, and about seeing their faces light up when they see their favorite treats or toys.</p>
<p>I've had the chance to work with over 1,000 dogs, and I must say, it's a rewarding experience. The bond between owner and pet</p>
</blockquote>

    <p><small></small>Via <a href="https://twitter.com/karpathy/status/1977755427569111362">@karpathy</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/rust">rust</a>, <a href="https://simonwillison.net/tags/pytorch">pytorch</a>, <a href="https://simonwillison.net/tags/andrej-karpathy">andrej-karpathy</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/training-data">training-data</a>, <a href="https://simonwillison.net/tags/uv">uv</a>, <a href="https://simonwillison.net/tags/gpus">gpus</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a></p>]]></description><pubDate>Mon, 13 Oct 2025 20:29:58 +0000</pubDate></item><item><title>Quoting Slashdot</title><link>https://simonwillison.net/2025/Oct/12/slashdot/#atom-everything</link><description><![CDATA[<blockquote cite="https://hardware.slashdot.org/story/25/10/11/0238213/microsofts-onedrive-begins-testing-face-recognizing-ai-for-photos-for-some-preview-users"><p>Slashdot: What's the reason OneDrive tells users this setting can only be turned off 3 times a year? (And are those any three times — or does that mean three specific days, like Christmas, New Year's Day, etc.)</p>
<p><img alt="People section. You can only turn off this setting 3 times a year. OneDrive uses Al to recognize faces in your photos to help you find photos of friends and family. Learn how it works" src="https://static.simonwillison.net/static/2025/one-drive-3-times.jpeg" style="max-width: 100%"></p>
<p>[Microsoft's publicist chose not to answer this question.]</p></blockquote>
<p class="cite">&mdash; <a href="https://hardware.slashdot.org/story/25/10/11/0238213/microsofts-onedrive-begins-testing-face-recognizing-ai-for-photos-for-some-preview-users">Slashdot</a>, asking the <em>obvious</em> question</p>

    <p>Tags: <a href="https://simonwillison.net/tags/slashdot">slashdot</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/microsoft">microsoft</a></p>]]></description><pubDate>Sun, 12 Oct 2025 16:18:05 +0000</pubDate></item><item><title>Claude Code sub-agents</title><link>https://simonwillison.net/2025/Oct/11/sub-agents/#atom-everything</link><description><![CDATA[<p>Claude Code includes the ability to run sub-agents, where a separate agent loop with a fresh token context is dispatched to achieve a goal and report back when it's done. I wrote a bit about how these work <a href="https://simonwillison.net/2025/Jun/2/claude-trace/">in June</a> when I traced Claude Code's activity by intercepting its API calls.</p>
<p>I recently learned that you can <em>deliberately</em> trigger Claude Code to use sub-agents just by telling it to do so!</p>
<p>Here's an example. I ran this using Claude Code against a fresh checkout of <a href="https://github.com/simonw/datasette">simonw/datasette</a> (commit <code>ec38ad37689f3c14d307770977a17aed5efc5cb9</code>), with no additional custom instructions:</p>
<blockquote>
<p><code>Write markdown documentation for the context passed to each of the templates in this project. Use sub-agents.</code></p>
</blockquote>
<p>Here's a snippet of the output:</p>
<blockquote>
<p>⏺ I'll help you write markdown documentation for the context passed to each template in this project. Let me start by exploring the codebase to understand the template structure and then use sub-agents to document them comprehensively.</p>
<pre><code>⏺ Search(pattern: "**/*.html")
  ⎿  Found 47 files (ctrl+o to expand)

⏺ Search(pattern: "render.*template|render_template")
  ⎿  Found 16 files (ctrl+o to expand)
</code></pre>
<p>[...]</p>
<p>⏺ Now let me launch sub-agents to analyze different template contexts in parallel:</p>
<pre><code>⏺ Task(Document index template context)
  ⎿  Done (12 tool uses · 86.4k tokens · 1m 13s)

⏺ Task(Document database template context)
  ⎿  Done (15 tool uses · 55.4k tokens · 1m 41s)

⏺ Task(Document table template context)
  ⎿  Done (14 tool uses · 85.2k tokens · 1m 44s)

⏺ Task(Document query/row template contexts)
  ⎿  Done (17 tool uses · 78.7k tokens · 1m 55s)

⏺ Task(Document special template contexts)
  ⎿  Done (26 tool uses · 116.1k tokens · 1m 50s)
</code></pre>
</blockquote>
<p>It churned away for a couple of minutes (those sub-agents execute in parallel, another benefit of this pattern) and made me this <a href="https://gist.github.com/simonw/5b7d87fe5ffa36e7a4c104fcb18fc097">
template_contexts.md
</a> markdown file. It's very comprehensive.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/sub-agents">sub-agents</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Sat, 11 Oct 2025 21:29:39 +0000</pubDate></item><item><title>Vibing a Non-Trivial Ghostty Feature</title><link>https://simonwillison.net/2025/Oct/11/vibing-a-non-trivial-ghostty-feature/#atom-everything</link><description><![CDATA[<p><strong><a href="https://mitchellh.com/writing/non-trivial-vibing">Vibing a Non-Trivial Ghostty Feature</a></strong></p>
Mitchell Hashimoto provides a comprehensive answer to the <em>frequent</em> demand for a detailed description of shipping a non-trivial production feature to an existing project using AI-assistance. In this case it's a slick unobtrusive auto-update UI for his <a href="https://ghostty.org/">Ghostty</a> terminal emulator, written in Swift.</p>
<p>Mitchell shares full transcripts of the 16 coding sessions he carried out using <a href="https://ampcode.com/">Amp Code</a> across 2 days and around 8 hours of computer time, at a token cost of $15.98.</p>
<p>Amp has the nicest shared transcript feature of any of the coding agent tools, as seen <a href="https://ampcode.com/threads/T-9fc3eb88-5aa2-45e4-8f6d-03697f53102d">in this example</a>. I'd love to see Claude Code and Codex CLI and Gemini CLI and friends imitate this.</p>
<p>There are plenty of useful tips in here. I like this note about the importance of a cleanup step:</p>
<blockquote>
<p>The cleanup step is really important. To cleanup effectively you have to have a pretty good understanding of the code, so this forces me to not blindly accept AI-written code. Subsequently, better organized and documented code helps future agentic sessions perform better.</p>
<p>I sometimes tongue-in-cheek refer to this as the "anti-slop session".</p>
</blockquote>
<p>And this on how sometimes you can write manual code in a way that puts the agent the right track:</p>
<blockquote>
<p>I spent some time manually restructured the view model. This involved switching to a tagged union rather than the struct with a bunch of optionals. I renamed some types, moved stuff around.</p>
<p>I knew from experience that this small bit of manual work in the middle would set the agents up for success in future sessions for both the frontend and backend. After completing it, I continued with a marathon set of cleanup sessions.</p>
</blockquote>
<p>Here's one of those refactoring prompts:</p>
<blockquote>
<p><code>Turn each @macos/Sources/Features/Update/UpdatePopoverView.swift case into a dedicated fileprivate Swift view that takes the typed value as its parameter so that we can remove the guards.</code></p>
</blockquote>
<p>Mitchell advises ending every session with a prompt like this one, asking the agent about any obvious omissions:</p>
<blockquote>
<p><code>Are there any other improvements you can see to be made with the @macos/Sources/Features/Update feature? Don't write any code. Consult the oracle. Consider parts of the code that can also get more unit tests added.</code></p>
</blockquote>
<p>("Consult the oracle" is an Amp-specific pattern for running a task through a more expensive, more capable model.)</p>
<p>Is this all worthwhile? Mitchell thinks so:</p>
<blockquote>
<p>Many people on the internet argue whether AI enables you to work faster or not. In this case, I think I shipped this faster than I would have if I had done it all myself, in particular because iterating on minor SwiftUI styling is so tedious and time consuming for me personally and AI does it so well.</p>
<p>I think the faster/slower argument for me personally is missing the thing I like the most: the AI can work for me while I step away to do other things.</p>
</blockquote>
<p>Here's <a href="https://github.com/ghostty-org/ghostty/pull/9116/files">the resulting PR</a>, which touches 21 files.

    <p><small></small>Via <a href="https://twitter.com/mitchellh/status/1977016133409820684">@mitchellh</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/mitchell-hashimoto">mitchell-hashimoto</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a></p>]]></description><pubDate>Sat, 11 Oct 2025 16:33:59 +0000</pubDate></item><item><title>Note on 11th October 2025</title><link>https://simonwillison.net/2025/Oct/11/uncomfortable/#atom-everything</link><description><![CDATA[<p>I'm beginning to suspect that a key skill in working effectively with coding agents is developing an intuition for when you <em>don't</em> need to closely review every line of code they produce. This feels deeply uncomfortable!</p>

    <p>Tags: <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Sat, 11 Oct 2025 12:31:09 +0000</pubDate></item><item><title>An MVCC-like columnar table on S3 with constant-time deletes</title><link>https://simonwillison.net/2025/Oct/11/mvcc-s3/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.shayon.dev/post/2025/277/an-mvcc-like-columnar-table-on-s3-with-constant-time-deletes/">An MVCC-like columnar table on S3 with constant-time deletes</a></strong></p>
s3's support for conditional writes (<a href="https://simonwillison.net/2024/Nov/26/s3-conditional-writes/">previously</a>) makes it an interesting, scalable and often inexpensive platform for all kinds of database patterns.</p>
<p>Shayon Mukherjee presents an ingenious design for a Parquet-backed database in S3 which accepts concurrent writes, presents a single atomic view for readers and even supports reliable row deletion despite Parquet requiring a complete file rewrite in order to remove data.</p>
<p>The key to the design is a <code>_latest_manifest</code> JSON file at the top of the bucket, containing an integer version number. Clients use compare-and-swap to increment that version - only one client can succeed at this, so the incremented version they get back is guaranteed unique to them.</p>
<p>Having reserved a version number the client can write a unique manifest file for that version - <code>manifest/v00000123.json</code> - with a more complex data structure referencing the current versions of every persisted file, including the one they just uploaded.</p>
<p>Deleted rows are written to tombstone files as either a list of primary keys or a list of of ranges. Clients consult these when executing reads, filtering out deleted rows as part of resolving a query.</p>
<p>The pricing estimates are especially noteworthy:</p>
<blockquote>
<p>For a workload ingesting 6 TB/day with 2 TB of deletes and 50K queries/day:</p>
<ul>
<li>PUT requests: ~380K/day (≈4 req/s) = $1.88/day</li>
<li>GET requests: highly variable, depends on partitioning effectiveness<ul>
<li>Best case (good time-based partitioning): ~100K-200K/day = $0.04-$0.08/day</li>
<li>Worst case (poor partitioning, scanning many files): ~2M/day = $0.80/day</li>
</ul>
</li>
</ul>
</blockquote>
<p>~$3/day for ingesting 6TB of data is pretty fantastic!</p>
<p>Watch out for storage costs though - each new TB of data at $0.023/GB/month adds $23.55 to the ongoing monthly bill.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=45493158">Hacked News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/aws">aws</a>, <a href="https://simonwillison.net/tags/databases">databases</a>, <a href="https://simonwillison.net/tags/s3">s3</a>, <a href="https://simonwillison.net/tags/parquet">parquet</a></p>]]></description><pubDate>Sat, 11 Oct 2025 03:47:16 +0000</pubDate></item><item><title>simonw/claude-skills</title><link>https://simonwillison.net/2025/Oct/10/claude-skills/#atom-everything</link><description><![CDATA[<p><strong><a href="https://github.com/simonw/claude-skills">simonw/claude-skills</a></strong></p>
One of the tips I picked up from <a href="https://blog.fsck.com/2025/10/09/superpowers/">Jesse Vincent's Claude Code Superpowers post</a> (<a href="https://simonwillison.net/2025/Oct/10/superpowers/">previously</a>) was this:</p>
<blockquote>
<p>Skills are what give your agents Superpowers.</p>
<p>The first time they really popped up on my radar was a few weeks ago when Anthropic rolled out improved Office document creation. When the feature rolled out, I went poking around a bit -- I asked Claude to tell me all about its new skills. And it was <a href="https://claude.ai/share/0fe5a9c0-4e5a-42a1-9df7-c5b7636dad92">only too happy to dish</a>.</p>
</blockquote>
<p>In that transcript Jesse asks how the skill system works and Claude replies:</p>
<blockquote>
<p>Looking at the skills system, here's how it works:</p>
<p><strong>Current Skills</strong> (in <code>/mnt/skills/public/</code>):</p>
<ul>
<li><strong>docx</strong> - Word document handling</li>
<li><strong>pdf</strong> - PDF processing and form filling</li>
<li><strong>pptx</strong> - PowerPoint presentations</li>
<li><strong>xlsx</strong> - Excel spreadsheets</li>
</ul>
</blockquote>
<p>This looks like it's part of Claude's brand new Code Interpreter feature! I <a href="https://simonwillison.net/2025/Sep/9/claude-code-interpreter/">wrote about that extensively</a> last month, but I missed that there was a <code>/mnt/skills/public/</code> folder full of fascinating implementation details.</p>
<p>So I fired up a fresh Claude instance (fun fact: Code Interpreter also works in the Claude iOS app now, which it didn't when they first launched) and prompted:</p>
<blockquote>
<p><code>Create a zip file of everything in your /mnt/skills folder</code></p>
</blockquote>
<p>This worked, and gave me a <code>.zip</code> to download. You can <a href="https://claude.ai/new?q=Create%20a%20zip%20file%20of%20everything%20in%20your%20%2Fmnt%2Fskills%20folder">run the prompt yourself here</a>, though you'll need to <a href="https://simonwillison.net/2025/Sep/9/claude-code-interpreter/#switching-it-on-in-settings-features">enable the new feature first</a>.</p>
<p>I've pushed the contents of that zip to my <a href="https://github.com/simonw/claude-skills">new simonw/claude-skills GitHub repo</a>.</p>
<p>So now you can see the prompts Anthropic wrote to enable the creation and manipulation of the following files in their Claude consumer applications:</p>
<ul>
<li><a href="https://github.com/simonw/claude-skills/blob/initial/mnt/skills/public/pdf/SKILL.md">pdf</a> - PDF files</li>
<li><a href="https://github.com/simonw/claude-skills/blob/initial/mnt/skills/public/docx/SKILL.md">docx</a> - Microsoft Word</li>
<li><a href="https://github.com/simonw/claude-skills/blob/initial/mnt/skills/public/pptx/SKILL.md">pptx</a> - Microsoft PowerPoint decks</li>
<li><a href="https://github.com/simonw/claude-skills/blob/initial/mnt/skills/public/xlsx/SKILL.md">xlsx</a> - Microsoft Excel</li>
</ul>
<p>In each case the prompts spell out detailed instructions for manipulating those file types using Python, using libraries that come pre-installed on Claude's containers.</p>
<p>Skills are more than just prompts though: the repository also includes dozens of pre-written Python scripts for performing common operations.</p>
<p><a href="https://github.com/simonw/claude-skills/blob/initial/mnt/skills/public/pdf/scripts/fill_fillable_fields.py">pdf/scripts/fill_fillable_fields.py</a> for example is a custom CLI tool that uses <a href="https://pypi.org/project/pypdf/">pypdf</a> to find and then fill in a bunch of PDF form fields, specified as JSON, then render out the resulting combined PDF.</p>
<p>This is a really sophisticated set of tools for document manipulation, and I love that Anthropic have made those visible - presumably deliberately - to users of Claude who know how to ask for them.


    <p>Tags: <a href="https://simonwillison.net/tags/pdf">pdf</a>, <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/prompt-engineering">prompt-engineering</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/code-interpreter">code-interpreter</a>, <a href="https://simonwillison.net/tags/jesse-vincent">jesse-vincent</a>, <a href="https://simonwillison.net/tags/skills">skills</a></p>]]></description><pubDate>Fri, 10 Oct 2025 23:57:19 +0000</pubDate></item><item><title>Superpowers: How I&apos;m using coding agents in October 2025</title><link>https://simonwillison.net/2025/Oct/10/superpowers/#atom-everything</link><description><![CDATA[<p><strong><a href="https://blog.fsck.com/2025/10/09/superpowers/">Superpowers: How I&#x27;m using coding agents in October 2025</a></strong></p>
A follow-up to Jesse Vincent's post <a href="https://blog.fsck.com/2025/10/05/how-im-using-coding-agents-in-september-2025/">about September</a>, but this is a really significant piece in its own right.</p>
<p>Jesse is one of the most creative users of coding agents (Claude Code in particular) that I know. He's put a great amount of work into evolving an effective process for working with them, encourage red/green TDD (watch the test fail first), planning steps, self-updating memory notes and even implementing a <a href="https://blog.fsck.com/2025/05/28/dear-diary-the-user-asked-me-if-im-alive/">feelings journal</a> ("I feel engaged and curious about this project" - Claude).</p>
<p>Claude Code <a href="https://www.anthropic.com/news/claude-code-plugins">just launched plugins</a>, and Jesse is celebrating by wrapping up a whole host of his accumulated tricks as a new plugin called <a href="https://github.com/obra/superpowers">Superpowers</a>. You can add it to your Claude Code like this:</p>
<pre><code>/plugin marketplace add obra/superpowers-marketplace
/plugin install superpowers@superpowers-marketplace
</code></pre>
<p>There's a lot in here! It's worth spending some time <a href="https://github.com/obra/superpowers">browsing the repository</a> - here's just one fun example, in <a href="https://github.com/obra/superpowers/blob/main/skills/debugging/root-cause-tracing/SKILL.md">skills/debugging/root-cause-tracing/SKILL.md</a>:</p>
<blockquote>
<pre><code>---
name: Root Cause Tracing
description: Systematically trace bugs backward through call stack to find original trigger
when_to_use: Bug appears deep in call stack but you need to find where it originates
version: 1.0.0
languages: all
---
</code></pre>
<p><strong>Overview</strong></p>
<p>Bugs often manifest deep in the call stack (git init in wrong directory, file created in wrong location, database opened with wrong path). Your instinct is to fix where the error appears, but that's treating a symptom.</p>
<p><strong>Core principle:</strong> Trace backward through the call chain until you find the original trigger, then fix at the source.</p>
<p><strong>When to Use</strong></p>
<pre><code>digraph when_to_use {
    "Bug appears deep in stack?" [shape=diamond];
    "Can trace backwards?" [shape=diamond];
    "Fix at symptom point" [shape=box];
    "Trace to original trigger" [shape=box];
    "BETTER: Also add defense-in-depth" [shape=box];

    "Bug appears deep in stack?" -&gt; "Can trace backwards?" [label="yes"];
    "Can trace backwards?" -&gt; "Trace to original trigger" [label="yes"];
    "Can trace backwards?" -&gt; "Fix at symptom point" [label="no - dead end"];
    "Trace to original trigger" -&gt; "BETTER: Also add defense-in-depth";
}
</code></pre>
<p>[...]</p>
</blockquote>
<p>This one is particularly fun because it then includes a <a href="https://en.wikipedia.org/wiki/DOT_(graph_description_language)">Graphviz DOT graph</a> illustrating the process - it turns out Claude can interpret those as workflow instructions just fine, and Jesse has been <a href="https://blog.fsck.com/2025/09/29/using-graphviz-for-claudemd/">wildly experimenting with them</a>.</p>
<p>I <a href="https://claude.ai/share/2b78a93e-cdc3-4b1d-9b02-457eb62140a5">vibe-coded up</a> a quick URL-based DOT visualizer, <a href="https://tools.simonwillison.net/dot#digraph%20when_to_use%20%7B%0A%20%20%20%20%22Bug%20appears%20deep%20in%20stack%3F%22%20%5Bshape%3Ddiamond%5D%3B%0A%20%20%20%20%22Can%20trace%20backwards%3F%22%20%5Bshape%3Ddiamond%5D%3B%0A%20%20%20%20%22Fix%20at%20symptom%20point%22%20%5Bshape%3Dbox%5D%3B%0A%20%20%20%20%22Trace%20to%20original%20trigger%22%20%5Bshape%3Dbox%5D%3B%0A%20%20%20%20%22BETTER%3A%20Also%20add%20defense-in-depth%22%20%5Bshape%3Dbox%5D%3B%0A%0A%20%20%20%20%22Bug%20appears%20deep%20in%20stack%3F%22%20-%3E%20%22Can%20trace%20backwards%3F%22%20%5Blabel%3D%22yes%22%5D%3B%0A%20%20%20%20%22Can%20trace%20backwards%3F%22%20-%3E%20%22Trace%20to%20original%20trigger%22%20%5Blabel%3D%22yes%22%5D%3B%0A%20%20%20%20%22Can%20trace%20backwards%3F%22%20-%3E%20%22Fix%20at%20symptom%20point%22%20%5Blabel%3D%22no%20-%20dead%20end%22%5D%3B%0A%20%20%20%20%22Trace%20to%20original%20trigger%22%20-%3E%20%22BETTER%3A%20Also%20add%20defense-in-depth%22%3B%0A%7D">here's that one rendered</a>:</p>
<p><img alt="The above DOT rendered as an image" src="https://static.simonwillison.net/static/2025/jesse-dot.jpg" /></p>
<p>There is <em>so much</em> to learn about putting these tools to work in the most effective way possible. Jesse is way ahead of the curve, so it's absolutely worth spending some time exploring what he's shared so far.</p>
<p>And if you're worried about filling up your context with a bunch of extra stuff, here's <a href="https://bsky.app/profile/s.ly/post/3m2srmkergc2p">a reassuring note from Jesse</a>:</p>
<blockquote>
<p>The core of it is VERY token light. It pulls in one doc of fewer than 2k tokens. As it needs bits of the process, it runs a shell script to search for them.  The long end to end chat for the planning and implementation process for that todo list app was 100k tokens.</p>
<p>It uses subagents to manage token-heavy stuff, including all the actual implementation.</p>
</blockquote>
<p>(Jesse's post also tipped me off about Claude's <code>/mnt/skills/public</code> folder, see <a href="https://simonwillison.net/2025/Oct/10/claude-skills/">my notes here</a>.)


    <p>Tags: <a href="https://simonwillison.net/tags/plugins">plugins</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/prompt-engineering">prompt-engineering</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/sub-agents">sub-agents</a>, <a href="https://simonwillison.net/tags/jesse-vincent">jesse-vincent</a>, <a href="https://simonwillison.net/tags/skills">skills</a></p>]]></description><pubDate>Fri, 10 Oct 2025 23:30:14 +0000</pubDate></item><item><title>A Retrospective Survey of 2024/2025 Open Source Supply Chain Compromises</title><link>https://simonwillison.net/2025/Oct/10/a-retrospective-survey/#atom-everything</link><description><![CDATA[<p><strong><a href="https://words.filippo.io/compromise-survey/">A Retrospective Survey of 2024/2025 Open Source Supply Chain Compromises</a></strong></p>
Filippo Valsorda surveyed 18 incidents from the past year of open source supply chain attacks, where package updates were infected with malware thanks to a compromise of the project itself.</p>
<p>These are important lessons:</p>
<blockquote>
<p>I have the growing impression that software supply chain compromises have a few predominant causes which we might have a responsibility as a professional open source maintainers to robustly mitigate.</p>
<p>To test this impression and figure out any such mitigations, I collected all 2024/2025 open source supply chain compromises I could find, and categorized their root cause.</p>
</blockquote>
<p>This is a fascinating piece of research. 5 were the result of phishing (maintainers should use passkeys/WebAuthn!), ~5 were stolen long-lived credentials, 3 were "control handoff" where a maintainer gave project access to someone who later turned out to be untrustworthy, 4 were caused by GitHub Actions workflows that triggered on pull requests or issue comments in a way that could leak credentials, and one (<a href="https://blog.oversecured.com/Introducing-MavenGate-a-supply-chain-attack-method-for-Java-and-Android-applications/">MavenGate</a>) was caused by <a href="https://blog.oversecured.com/Introducing-MavenGate-a-supply-chain-attack-method-for-Java-and-Android-applications/#method-of-attacks">an expired domain</a> being resurrected.

    <p><small></small>Via <a href="https://lobste.rs/s/0ua1s5/retrospective_survey_2024_2025_open">lobste.rs</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/open-source">open-source</a>, <a href="https://simonwillison.net/tags/security">security</a>, <a href="https://simonwillison.net/tags/supply-chain">supply-chain</a>, <a href="https://simonwillison.net/tags/filippo-valsorda">filippo-valsorda</a></p>]]></description><pubDate>Fri, 10 Oct 2025 23:00:52 +0000</pubDate></item><item><title>Video of GPT-OSS 20B running on a phone</title><link>https://simonwillison.net/2025/Oct/10/gpt-oss-20b-snapdragon/#atom-everything</link><description><![CDATA[<p><strong><a href="https://twitter.com/nexa_ai/status/1975232300985291008">Video of GPT-OSS 20B running on a phone</a></strong></p>
GPT-OSS 20B is a <a href="https://simonwillison.net/2025/Aug/5/gpt-oss/">very good model</a>. At launch OpenAI claimed:</p>
<blockquote>
<p>The gpt-oss-20b model delivers similar results to OpenAI o3‑mini on common benchmarks and can run on edge devices with just 16 GB of memory</p>
</blockquote>
<p><a href="https://nexa.ai/">Nexa AI</a> just posted a video on Twitter demonstrating exactly that: the full GPT-OSS 20B running on a Snapdragon Gen 5 phone in their <a href="https://play.google.com/store/apps/details?id=com.nexa.studio">Nexa Studio</a> Android app. It requires at least 16GB of RAM, and benefits from Snapdragon using a similar trick to Apple Silicon where the system RAM is available to both the CPU and the GPU.</p>
<p>The latest iPhone 17 Pro Max is still stuck at 12GB of RAM, presumably not enough to run this same model.


    <p>Tags: <a href="https://simonwillison.net/tags/android">android</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/local-llms">local-llms</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/gpt-oss">gpt-oss</a></p>]]></description><pubDate>Fri, 10 Oct 2025 22:37:21 +0000</pubDate></item><item><title>Quoting Gergely Orosz</title><link>https://simonwillison.net/2025/Oct/9/gergely-orosz/#atom-everything</link><description><![CDATA[<blockquote cite="https://twitter.com/gergelyorosz/status/1976242900670480763"><p>I get a feeling that working with multiple AI agents is something that comes VERY natural to most  senior+ engineers or tech lead who worked at a large company</p>
<p>You already got used to overseeing parallel work (the goto code reviewer!) + making progress with small chunks of work... because your day has been a series of nonstop interactions, so you had to figure out how to do deep work in small chunks that could have been interrupted</p></blockquote>
<p class="cite">&mdash; <a href="https://twitter.com/gergelyorosz/status/1976242900670480763">Gergely Orosz</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/gergely-orosz">gergely-orosz</a>, <a href="https://simonwillison.net/tags/parallel-agents">parallel-agents</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Thu, 9 Oct 2025 13:56:12 +0000</pubDate></item><item><title>TIL: Testing different Python versions with uv with-editable and uv-test</title><link>https://simonwillison.net/2025/Oct/9/uv-test/#atom-everything</link><description><![CDATA[<p><strong><a href="https://til.simonwillison.net/python/uv-tests">TIL: Testing different Python versions with uv with-editable and uv-test</a></strong></p>
While tinkering with upgrading various projects to handle Python 3.14 I finally figured out a universal <code>uv</code> recipe for running the tests for the current project in any specified version of Python:</p>
<pre><code>uv run --python 3.14 --isolated --with-editable '.[test]' pytest
</code></pre>
<p>This should work in any directory with a <code>pyproject.toml</code> (or even a <code>setup.py</code>) that defines a <code>test</code> set of extra dependencies and uses <code>pytest</code>.</p>
<p>The <code>--with-editable '.[test]'</code> bit ensures that changes you make to that directory will be picked up by future test runs. The <code>--isolated</code> flag ensures no other environments will affect your test run.</p>
<p>I like this pattern so much I built a little shell script that uses it, <a href="https://til.simonwillison.net/python/uv-tests#user-content-uv-test">shown here</a>. Now I can change to any Python project directory and run:</p>
<pre><code>uv-test
</code></pre>
<p>Or for a different Python version:</p>
<pre><code>uv-test -p 3.11
</code></pre>
<p>I can pass additional <code>pytest</code> options too:</p>
<pre><code>uv-test -p 3.11 -k permissions
</code></pre>


    <p>Tags: <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/testing">testing</a>, <a href="https://simonwillison.net/tags/pytest">pytest</a>, <a href="https://simonwillison.net/tags/til">til</a>, <a href="https://simonwillison.net/tags/uv">uv</a></p>]]></description><pubDate>Thu, 9 Oct 2025 03:37:06 +0000</pubDate></item><item><title>Claude can write complete Datasette plugins now</title><link>https://simonwillison.net/2025/Oct/8/claude-datasette-plugins/#atom-everything</link><description><![CDATA[<p>This isn't necessarily surprising, but it's worth noting anyway. Claude Sonnet 4.5 is capable of building a full Datasette plugin now.</p>
<p>I've seen models complete aspects of this in the past, but today is the first time I've shipped a new plugin where every line of code and test was written by Claude, with minimal prompting from myself.</p>
<p>The plugin is called <strong><a href="https://github.com/datasette/datasette-os-info">datasette-os-info</a></strong>. It's a simple debugging tool - all it does is add a <code>/-/os</code> JSON page which dumps out as much information as it can about the OS it's running on. Here's a <a href="https://til.simonwillison.net/-/os">live demo</a> on my TIL website.</p>
<p>I built it to help experiment with changing the Docker base container that Datasette uses to <a href="https://docs.datasette.io/en/stable/publish.html">publish images</a> to one that uses Python 3.14.</p>
<p>Here's the full set of commands I used to create the plugin. I started with my <a href="https://github.com/simonw/datasette-plugin">datasette-plugin</a> cookiecutter template:</p>
<div class="highlight highlight-source-shell"><pre>uvx cookiecutter gh:simonw/datasette-plugin

  [1/8] <span class="pl-en">plugin_name</span> (): os-info
  [2/8] <span class="pl-en">description</span> (): Information about the current OS
  [3/8] hyphenated (os-info): 
  [4/8] underscored (os_info): 
  [5/8] <span class="pl-en">github_username</span> (): datasette
  [6/8] <span class="pl-en">author_name</span> (): Simon Willison
  [7/8] <span class="pl-en">include_static_directory</span> (): 
  [8/8] <span class="pl-en">include_templates_directory</span> (): </pre></div>
<p>This created a <code>datasette-os-info</code> directory with the initial <code>pyproject.toml</code> and <code>tests/</code> and <code>datasette_os_info/__init__.py</code> files. Here's an example of <a href="https://github.com/simonw/datasette-plugin-template-demo">that starter template</a>.</p>
<p>I created a <code>uv</code> virtual environment for it, installed the initial test dependencies and ran <code>pytest</code> to check that worked:</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span> datasette-os-info
uv venv
uv sync --extra <span class="pl-c1">test</span>
uv run pytest</pre></div>
<p>Then I fired up <a href="https://www.claude.com/product/claude-code">Claude Code</a> in that directory in YOLO mode:</p>
<div class="highlight highlight-source-shell"><pre>claude --dangerously-skip-permissions</pre></div>
<p>(I actually used my <code>claude-yolo</code> shortcut which runs the above.)</p>
<p>Then, in Claude, I told it how to run the tests:</p>
<blockquote>
<p><code>Run uv run pytest</code></p>
</blockquote>
<p>When that worked, I told it to build the plugin:</p>
<blockquote>
<p><code>This is a Datasette plugin which should add a new page /-/os which returns pretty-printed JSON about the current operating system - implement it. I want to pick up as many details as possible across as many OS as possible, including if possible figuring out the base image if it is in a docker container - otherwise the Debian OS release name and suchlike would be good</code></p>
</blockquote>
<p>... and that was it! Claude <a href="https://github.com/datasette/datasette-os-info/blob/0.1/datasette_os_info/__init__.py">implemented the plugin</a> using Datasette's <a href="https://docs.datasette.io/en/stable/plugin_hooks.html#register-routes-datasette">register_routes() plugin hook</a> to add the <code>/-/os</code> page,and then without me prompting it to do so <a href="https://github.com/datasette/datasette-os-info/blob/0.1/tests/test_os_info.py">built this basic test as well</a>.</p>
<p>It ran the new test, spotted a bug (it had guessed a non-existent <code>Response(..., default_repr=)</code> parameter), fixed the bug and declared itself done.</p>
<p>I built myself a wheel:</p>
<div class="highlight highlight-source-shell"><pre>uv pip install build
uv run python -m build</pre></div>
<p>Then uploaded that to an S3 bucket and deployed it to test it out using <code>datasette publish ... --install URL-to-wheel</code>.  It did exactly what I had hoped - here's what that <code>/-/os</code> page looked like:</p>
<div class="highlight highlight-source-json"><pre>{
  <span class="pl-ent">"platform"</span>: {
    <span class="pl-ent">"system"</span>: <span class="pl-s"><span class="pl-pds">"</span>Linux<span class="pl-pds">"</span></span>,
    <span class="pl-ent">"release"</span>: <span class="pl-s"><span class="pl-pds">"</span>4.4.0<span class="pl-pds">"</span></span>,
    <span class="pl-ent">"version"</span>: <span class="pl-s"><span class="pl-pds">"</span>#1 SMP Sun Jan 10 15:06:54 PST 2016<span class="pl-pds">"</span></span>,
    <span class="pl-ent">"machine"</span>: <span class="pl-s"><span class="pl-pds">"</span>x86_64<span class="pl-pds">"</span></span>,
    <span class="pl-ent">"processor"</span>: <span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>,
    <span class="pl-ent">"architecture"</span>: [
      <span class="pl-s"><span class="pl-pds">"</span>64bit<span class="pl-pds">"</span></span>,
      <span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>
    ],
    <span class="pl-ent">"platform"</span>: <span class="pl-s"><span class="pl-pds">"</span>Linux-4.4.0-x86_64-with-glibc2.41<span class="pl-pds">"</span></span>,
    <span class="pl-ent">"python_version"</span>: <span class="pl-s"><span class="pl-pds">"</span>3.14.0<span class="pl-pds">"</span></span>,
    <span class="pl-ent">"python_implementation"</span>: <span class="pl-s"><span class="pl-pds">"</span>CPython<span class="pl-pds">"</span></span>
  },
  <span class="pl-ent">"hostname"</span>: <span class="pl-s"><span class="pl-pds">"</span>localhost<span class="pl-pds">"</span></span>,
  <span class="pl-ent">"cpu_count"</span>: <span class="pl-c1">2</span>,
  <span class="pl-ent">"linux"</span>: {
    <span class="pl-ent">"os_release"</span>: {
      <span class="pl-ent">"PRETTY_NAME"</span>: <span class="pl-s"><span class="pl-pds">"</span>Debian GNU/Linux 13 (trixie)<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"NAME"</span>: <span class="pl-s"><span class="pl-pds">"</span>Debian GNU/Linux<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"VERSION_ID"</span>: <span class="pl-s"><span class="pl-pds">"</span>13<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"VERSION"</span>: <span class="pl-s"><span class="pl-pds">"</span>13 (trixie)<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"VERSION_CODENAME"</span>: <span class="pl-s"><span class="pl-pds">"</span>trixie<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"DEBIAN_VERSION_FULL"</span>: <span class="pl-s"><span class="pl-pds">"</span>13.1<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"ID"</span>: <span class="pl-s"><span class="pl-pds">"</span>debian<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"HOME_URL"</span>: <span class="pl-s"><span class="pl-pds">"</span>https://www.debian.org/<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"SUPPORT_URL"</span>: <span class="pl-s"><span class="pl-pds">"</span>https://www.debian.org/support<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"BUG_REPORT_URL"</span>: <span class="pl-s"><span class="pl-pds">"</span>https://bugs.debian.org/<span class="pl-pds">"</span></span>
    },
    <span class="pl-ent">"debian_version"</span>: <span class="pl-s"><span class="pl-pds">"</span>13.1<span class="pl-pds">"</span></span>,
    <span class="pl-ent">"kernel"</span>: {
      <span class="pl-ent">"sysname"</span>: <span class="pl-s"><span class="pl-pds">"</span>Linux<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"nodename"</span>: <span class="pl-s"><span class="pl-pds">"</span>localhost<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"release"</span>: <span class="pl-s"><span class="pl-pds">"</span>4.4.0<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"version"</span>: <span class="pl-s"><span class="pl-pds">"</span>#1 SMP Sun Jan 10 15:06:54 PST 2016<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"machine"</span>: <span class="pl-s"><span class="pl-pds">"</span>x86_64<span class="pl-pds">"</span></span>
    },
    <span class="pl-ent">"cpu_model"</span>: <span class="pl-s"><span class="pl-pds">"</span>unknown<span class="pl-pds">"</span></span>,
    <span class="pl-ent">"memory"</span>: {
      <span class="pl-ent">"MemTotal"</span>: <span class="pl-s"><span class="pl-pds">"</span>1048576 kB<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"MemFree"</span>: <span class="pl-s"><span class="pl-pds">"</span>958244 kB<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"MemAvailable"</span>: <span class="pl-s"><span class="pl-pds">"</span>958244 kB<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"SwapTotal"</span>: <span class="pl-s"><span class="pl-pds">"</span>0 kB<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"SwapFree"</span>: <span class="pl-s"><span class="pl-pds">"</span>0 kB<span class="pl-pds">"</span></span>
    }
  },
  <span class="pl-ent">"environment"</span>: {
    <span class="pl-ent">"HOME"</span>: <span class="pl-s"><span class="pl-pds">"</span>/root<span class="pl-pds">"</span></span>,
    <span class="pl-ent">"PATH"</span>: <span class="pl-s"><span class="pl-pds">"</span>/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin<span class="pl-pds">"</span></span>
  },
  <span class="pl-ent">"python_executable"</span>: <span class="pl-s"><span class="pl-pds">"</span>/usr/local/bin/python3.14<span class="pl-pds">"</span></span>
}</pre></div>
<p>I dread to think how long it would have taken me to figure out all of those different useful OS details!</p>
<p>To finish up, I had Claude fill out the README via a couple of extra prompts:</p>
<blockquote>
<p><code>Add usage instructions to the README (about the /-/os page)</code></p>
</blockquote>
<p>Then had a brief conversation about how safe the output was:</p>
<blockquote>
<p><code>Is there anything potentially sensitive in that output?</code></p>
</blockquote>
<p>Claude said:</p>
<blockquote>
<p>Yes, there are several potentially sensitive items in the output:</p>
<p>Most sensitive:</p>
<ul>
<li>hostname - Could reveal internal naming schemes or organization details</li>
<li>Environment variables:
<ul>
<li>HOME - Reveals user's home directory path</li>
<li>USER - Reveals the username</li>
<li>PATH - Reveals filesystem structure and software installation locations</li>
<li>VIRTUAL_ENV - Could reveal project paths and directory structure</li>
</ul>
</li>
<li>python_executable - Reveals filesystem paths and user directories</li>
</ul>
<p>Moderately sensitive:</p>
<ul>
<li>CPU count and memory info - Could reveal infrastructure capacity</li>
<li>Kernel version details - Could potentially be used to identify specific vulnerabilities</li>
</ul>
<p>Considerations:</p>
<ol>
<li>This endpoint is accessible without authentication by default (at /-/os)</li>
<li>If you're running Datasette on a public server, this information would be visible to anyone</li>
<li>The information could help attackers understand your infrastructure setup</li>
</ol>
<p>Suggestions:</p>
<p>You might want to:</p>
<ol>
<li>Add authentication requirements to this endpoint</li>
<li>Make certain fields optional/configurable</li>
<li>Add a warning in the README that this exposes system information</li>
<li>Consider filtering out or redacting sensitive paths (replace usernames with ***, etc.)</li>
</ol>
<p>Would you like me to add any of these protections to the plugin?</p>
</blockquote>
<p>In this case I'm fine with the current behavior provided people understand the risks, so I told it:</p>
<blockquote>
<p><code>Add the warning to the README</code></p>
</blockquote>
<p>And then (because the warning it added was quite long):</p>
<blockquote>
<p><code>Drop the bit about configuring the permission system, just have a short warning telling people to review what it exposes</code></p>
</blockquote>
<p>And requested an extra README note:</p>
<blockquote>
<p><code>Add a note that you can also see the output by running: datasette --get /-/os</code></p>
</blockquote>
<p>Three last prompts:</p>
<ul>
<li><code>Add uv.lock to gitignore</code></li>
<li><code>Drop Python 3.9 and add Python 3.14 - to the GitHub workflows, also min version in pyproject.toml</code></li>
<li><code>Bump to setup-python@v6</code></li>
</ul>
<p>... and that was the project finished. I pushed it <a href="https://github.com/datasette/datasette-os-info">to GitHub</a>, <a href="https://til.simonwillison.net/pypi/pypi-releases-from-github">configured Trusted Publishing</a> for it on PyPI and posted <a href="https://github.com/datasette/datasette-os-info/releases/tag/0.1">the 0.1 release</a>, which ran <a href="https://github.com/datasette/datasette-os-info/blob/0.1/.github/workflows/publish.yml">this GitHub Actions publish.yml</a> and deployed that release <a href="https://pypi.org/project/datasette-os-info/">to datasette-os-info on PyPI</a>.</p>
<p>Now that it's live you can try it out without even installing Datasette using a <code>uv</code> one-liner like this:</p>
<div class="highlight highlight-source-shell"><pre>uv run --isolated \
  --with datasette-os-info \
  datasette --get /-/os</pre></div>
<p>That's using the <code>--get PATH</code> CLI option to show what that path in the Datasette instance would return, as <a href="https://docs.datasette.io/en/stable/cli-reference.html#datasette-get">described in the Datasette documentation</a>.</p>
<p>I've shared <a href="https://gist.github.com/simonw/85fd7a76589dc01950e71d8e606cd5dd">my full Claude Code transcript</a> in a Gist.</p>
<p>A year ago I'd have been <em>very</em> impressed by this. Today I wasn't even particularly surprised that this worked - the coding agent pattern implemented by Claude Code is spectacularly effective when you combine it with pre-existing templates, and Datasette has been aroung for long enough now that plenty of examples of plugins have made it into the training data for the leading models.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/plugins">plugins</a>, <a href="https://simonwillison.net/tags/projects">projects</a>, <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/datasette">datasette</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/uv">uv</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a></p>]]></description><pubDate>Wed, 8 Oct 2025 23:43:43 +0000</pubDate></item><item><title>Python 3.14 Is Here. How Fast Is It?</title><link>https://simonwillison.net/2025/Oct/8/python-314-is-here-how-fast-is-it/#atom-everything</link><description><![CDATA[<p><strong><a href="https://blog.miguelgrinberg.com/post/python-3-14-is-here-how-fast-is-it">Python 3.14 Is Here. How Fast Is It?</a></strong></p>
Miguel Grinberg uses some basic benchmarks (like <code>fib(40)</code>) to test the new Python 3.14 on Linux and macOS and finds some substantial speedups over Python 3.13 - around 27% faster.</p>
<p>The optional JIT didn't make a meaningful difference to his benchmarks. On a threaded benchmark he got 3.09x speedup with 4 threads using the free threading build - for Python 3.13 the free threading build only provided a 2.2x improvement.

    <p><small></small>Via <a href="https://lobste.rs/s/p0iw9e/python_3_14_is_here_how_fast_is_it">lobste.rs</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/gil">gil</a>, <a href="https://simonwillison.net/tags/performance">performance</a>, <a href="https://simonwillison.net/tags/python">python</a></p>]]></description><pubDate>Wed, 8 Oct 2025 18:36:33 +0000</pubDate></item><item><title>Quoting Simon Højberg</title><link>https://simonwillison.net/2025/Oct/8/simon-hojberg/#atom-everything</link><description><![CDATA[<blockquote cite="https://hojberg.xyz/the-programmer-identity-crisis/"><p>The cognitive debt of LLM-laden coding extends beyond disengagement of our craft. We’ve all heard the stories. Hyped up, vibed up, slop-jockeys with attention spans shorter than the framework-hopping JavaScript devs of the early 2010s, sling their sludge in pull requests and design docs, discouraging collaboration and disrupting teams. Code reviewing coworkers are rapidly losing their minds as they come to the crushing realization that they are now the first layer of quality control instead of one of the last. Asked to review; forced to pick apart. Calling out freshly added functions that are never called, hallucinated library additions, and obvious runtime or compilation errors. All while the author—who clearly only skimmed their “own” code—is taking no responsibility, going “whoopsie, Claude wrote that. Silly AI, ha-ha.”</p></blockquote>
<p class="cite">&mdash; <a href="https://hojberg.xyz/the-programmer-identity-crisis/">Simon Højberg</a>, The Programmer Identity Crisis</p>

    <p>Tags: <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/code-review">code-review</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a></p>]]></description><pubDate>Wed, 8 Oct 2025 18:08:32 +0000</pubDate></item><item><title>Why NetNewsWire Is Not a Web App</title><link>https://simonwillison.net/2025/Oct/8/why-netnewswire-is-not-a-web-app/#atom-everything</link><description><![CDATA[<p><strong><a href="https://inessential.com/2025/10/04/why-netnewswire-is-not-web-app.html">Why NetNewsWire Is Not a Web App</a></strong></p>
In the wake of Apple <a href="https://daringfireball.net/2025/10/iceblock_removed_from_app_store">removing ICEBlock from the App Store</a>, Brent Simmons talks about why he still thinks his veteran (and actively maintained) <a href="https://netnewswire.com/">NetNewsWire</a> feed reader app should remain a native application.</p>
<p>Part of the reason is cost - NetNewsWire is free these days (<a href="">MIT licensed in fact</a>) and the cost to Brent is an annual Apple developer subscription:</p>
<blockquote>
<p>If it were a web app instead, I could drop the developer membership, but I’d have to pay way more money for web and database hosting. [...] I could charge for NetNewsWire, but that would go against my political goal of making sure there’s a good and <em>free</em> RSS reader available to everyone.</p>
</blockquote>
<p>A bigger reason is around privacy and protecting users:</p>
<blockquote>
<p>Second issue. Right now, if law enforcement comes to me and demands I turn over a given user’s subscriptions list, I can’t. Literally can’t. I don’t have an encrypted version, even — I have nothing at all. The list lives on their machine (iOS or macOS).</p>
</blockquote>
<p>And finally it's about the principle of what a personal computing device should mean:</p>
<blockquote>
<p>My computer is <em>not</em> a terminal. It’s a world I get to control, and I can use — and, especially, <em>make</em> — whatever I want. I’m not stuck using just what’s provided to me on some other machines elsewhere: I’m not dialing into a mainframe or doing the modern equivalent of using only websites that other people control.</p>
</blockquote>


    <p>Tags: <a href="https://simonwillison.net/tags/apple">apple</a>, <a href="https://simonwillison.net/tags/brent-simmons">brent-simmons</a>, <a href="https://simonwillison.net/tags/macos">macos</a>, <a href="https://simonwillison.net/tags/netnewswire">netnewswire</a>, <a href="https://simonwillison.net/tags/ios">ios</a></p>]]></description><pubDate>Wed, 8 Oct 2025 16:12:14 +0000</pubDate></item><item><title>Python 3.14</title><link>https://simonwillison.net/2025/Oct/8/python-314/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.python.org/downloads/release/python-3140/">Python 3.14</a></strong></p>
This year's major Python version, Python 3.14, just made its first stable release!</p>
<p>As usual the <a href="https://docs.python.org/3.14/whatsnew/3.14.html">what's new in Python 3.14</a> document is the best place to get familiar with the new release:</p>
<blockquote>
<p>The biggest changes include <a href="https://docs.python.org/3.14/whatsnew/3.14.html#whatsnew314-template-string-literals">template string literals</a>, <a href="https://docs.python.org/3.14/whatsnew/3.14.html#whatsnew314-deferred-annotations">deferred evaluation of annotations</a>, and support for <a href="https://docs.python.org/3.14/whatsnew/3.14.html#whatsnew314-multiple-interpreters">subinterpreters</a> in the standard library.</p>
<p>The library changes include significantly improved capabilities for <a href="https://docs.python.org/3.14/whatsnew/3.14.html#whatsnew314-asyncio-introspection">introspection in asyncio</a>, <a href="https://docs.python.org/3.14/whatsnew/3.14.html#whatsnew314-zstandard">support for Zstandard</a> via a new <a href="https://docs.python.org/3.14/library/compression.zstd.html#module-compression.zstd">compression.zstd</a> module, syntax highlighting in the REPL, as well as the usual deprecations and removals, and improvements in user-friendliness and correctness.</p>
</blockquote>
<p>Subinterpreters look particularly interesting as a way to use multiple CPU cores to run Python code despite the continued existence of the GIL. If you're feeling brave and <a href="https://hugovk.github.io/free-threaded-wheels/">your dependencies cooperate</a> you can also use the free-threaded build of Python 3.14 - <a href="https://docs.python.org/3.14/whatsnew/3.14.html#whatsnew314-free-threaded-now-supported">now officially supported</a> - to skip the GIL entirely.</p>
<p>A new major Python release means an older release hits the <a href="https://devguide.python.org/versions/">end of its support lifecycle</a> - in this case that's Python 3.9. If you maintain open source libraries that target every supported Python versions (as I do) this means features introduced in Python 3.10 can now be depended on! <a href="https://docs.python.org/3.14/whatsnew/3.10.html">What's new in Python 3.10</a> lists those - I'm most excited by <a href="https://docs.python.org/3.14/whatsnew/3.10.html#pep-634-structural-pattern-matching">structured pattern matching</a> (the <code>match/case</code> statement) and the <a href="https://docs.python.org/3.14/whatsnew/3.10.html#pep-604-new-type-union-operator">union type operator</a>, allowing <code>int | float | None</code> as a type annotation in place of <code>Optional[Union[int, float]]</code>.</p>
<p>If you use <code>uv</code> you can grab a copy of 3.14 using:</p>
<pre><code>uv self update
uv python upgrade 3.14
uvx python@3.14
</code></pre>
<p>Or for free-threaded Python 3.1;:</p>
<pre><code>uvx python@3.14t
</code></pre>
<p>The <code>uv</code> team wrote <a href="https://astral.sh/blog/python-3.14">about their Python 3.14 highlights</a> in their announcement of Python 3.14's availability via <code>uv</code>.</p>
<p>The GitHub Actions <a href="https://github.com/actions/setup-python">setup-python action</a> includes Python 3.14 now too, so the following YAML snippet in will run tests on all currently supported versions:</p>
<pre><code>strategy:
  matrix:
    python-version: ["3.10", "3.11", "3.12", "3.13", "3.14"]
steps:
- uses: actions/setup-python@v6
  with:
    python-version: ${{ matrix.python-version }}
</code></pre>
<p><a href="https://github.com/simonw/datasette-pretty-traces/blob/3edddecab850d6ac47ed128a400b6a0ff8b0c012/.github/workflows/test.yml">Full example here</a> for one of my many Datasette plugin repos.


    <p>Tags: <a href="https://simonwillison.net/tags/gil">gil</a>, <a href="https://simonwillison.net/tags/open-source">open-source</a>, <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/github-actions">github-actions</a>, <a href="https://simonwillison.net/tags/psf">psf</a>, <a href="https://simonwillison.net/tags/uv">uv</a></p>]]></description><pubDate>Wed, 8 Oct 2025 04:10:06 +0000</pubDate></item><item><title>Retraction</title><link>https://simonwillison.net/2025/Oct/7/gemini-25-computer-use-captchas/#atom-everything</link><description><![CDATA[<p>Google released a new <a href="https://blog.google/technology/google-deepmind/gemini-computer-use-model/">Gemini 2.5 Computer Use model</a> today, specially designed to help operate a GUI interface by interacting with visible elements using a virtual mouse and keyboard.</p>
<p>I tried the demo hosted by Browserbase at <a href="https://gemini.browserbase.com/">gemini.browserbase.com</a> and was delighted and slightly horrified when it appeared to kick things off by first navigating to Google.com and solving their CAPTCHA in order to run a search!</p>

<p>I wrote a post about it and included <a href="https://static.simonwillison.net/static/2025/captcha-solve-gemini-2.jpg">this screenshot</a>, but then learned that Browserbase itself has CAPTCHA solving built in and, as shown <a href="https://static.simonwillison.net/static/2025/browserbase-captcha.mp4">in this longer video</a>, it was Browserbase that solved the CAPTCHA even while Gemini was thinking about doing so itself.</p>

<p>I deeply regret this error. I've deleted various social media posts about the original entry and linked back to this retraction instead.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/gemini">gemini</a></p>]]></description><pubDate>Tue, 7 Oct 2025 22:00:28 +0000</pubDate></item><item><title>Quoting Thomas Klausner</title><link>https://simonwillison.net/2025/Oct/7/thomas-klausner/#atom-everything</link><description><![CDATA[<blockquote cite="https://domm.plix.at/perl/2025_10_braincoded_static_image_gallery.html"><p>For quite some I wanted to write a small static image gallery so I can share my pictures with friends and family. Of course there are a gazillion tools like this, but, well, sometimes I just want to roll my own. [...]</p>
<p>I used the old, well tested technique I call <strong>brain coding</strong>, where you start with an empty vim buffer and type some code (Perl, HTML, CSS) until you're happy with the result. It helps to think a bit (aka use your brain) during this process.</p></blockquote>
<p class="cite">&mdash; <a href="https://domm.plix.at/perl/2025_10_braincoded_static_image_gallery.html">Thomas Klausner</a>, coining "brain coding"</p>

    <p>Tags: <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a>, <a href="https://simonwillison.net/tags/definitions">definitions</a></p>]]></description><pubDate>Tue, 7 Oct 2025 16:03:12 +0000</pubDate></item><item><title>Vibe engineering</title><link>https://simonwillison.net/2025/Oct/7/vibe-engineering/#atom-everything</link><description><![CDATA[<p>I feel like <strong>vibe coding</strong> is <a href="https://simonwillison.net/2025/Mar/19/vibe-coding/">pretty well established now</a> as covering the fast, loose and irresponsible way of building software with AI - entirely prompt-driven, and with no attention paid to how the code actually works. This leaves us with a terminology gap: what should we call the other end of the spectrum, where seasoned professionals accelerate their work with LLMs while staying proudly and confidently accountable for the software they produce?</p>
<p>I propose we call this <strong>vibe engineering</strong>, with my tongue only partially in my cheek.</p>
<p>One of the lesser spoken truths of working productively with LLMs as a software engineer on non-toy-projects is that it's <em>difficult</em>. There's a lot of depth to understanding how to use the tools, there are plenty of traps to avoid, and the pace at which they can churn out working code raises the bar for what the human participant can and should be contributing.</p>
<p>The rise of <strong>coding agents</strong> - tools like <a href="https://www.claude.com/product/claude-code">Claude Code</a> (released February 2025), OpenAI's <a href="https://github.com/openai/codex">Codex CLI</a> (April) and <a href="https://github.com/google-gemini/gemini-cli">Gemini CLI</a> (June) that can iterate on code, actively testing and modifying it until it achieves a specified goal, has dramatically increased the usefulness of LLMs for real-world coding problems.</p>
<p>I'm increasingly hearing from experienced, credible software engineers who are running multiple copies of agents at once, tackling several problems in parallel and expanding the scope of what they can take on. I was skeptical of this at first but <a href="https://simonwillison.net/2025/Oct/5/parallel-coding-agents/">I've started running multiple agents myself now</a> and it's surprisingly effective, if mentally exhausting!</p>
<p>This feels very different from classic vibe coding, where I outsource a simple, low-stakes task to an LLM and accept the result if it appears to work. Most of my <a href="https://tools.simonwillison.net/">tools.simonwillison.net</a> collection (<a href="https://simonwillison.net/2025/Sep/4/highlighted-tools/">previously</a>) were built like that. Iterating with coding agents to produce production-quality code that I'm confident I can maintain in the future feels like a different process entirely.</p>
<p>It's also become clear to me that LLMs actively reward existing top tier software engineering practices:</p>
<ul id="techniques">
<li>
<strong>Automated testing</strong>. If your project has a robust, comprehensive and stable test suite agentic coding tools can <em>fly</em> with it. Without tests? Your agent might claim something works without having actually tested it at all, plus any new change could break an unrelated feature without you realizing it. Test-first development is particularly effective with agents that can iterate in a loop.</li>
<li>
<strong>Planning in advance</strong>. Sitting down to hack something together goes much better if you start with a high level plan. Working with an agent makes this even more important - you can iterate on the plan first, then hand it off to the agent to write the code.</li>
<li>
<strong>Comprehensive documentation</strong>. Just like human programmers, an LLM can only keep a subset of the codebase in its context at once. Being able to feed in relevant documentation lets it use APIs from other areas without reading the code first. Write good documentation first and the model may be able to build the matching implementation from that input alone.</li>
<li>
<strong>Good version control habits</strong>. Being able to undo mistakes and understand when and how something was changed is even more important when a coding agent might have made the changes. LLMs are also fiercely competent at Git - they can navigate the history themselves to track down the origin of bugs, and they're better than most developers at using <a href="https://til.simonwillison.net/git/git-bisect">git bisect</a>. Use that to your advantage.</li>
<li>Having <strong>effective automation</strong> in place. Continuous integration, automated formatting and linting, continuous deployment to a preview environment - all things that agentic coding tools can benefit from too. LLMs make writing quick automation scripts easier as well, which can help them then repeat tasks accurately and consistently next time.</li>
<li>A <strong>culture of code review</strong>. This one explains itself. If you're fast and productive at code review you're going to have a much better time working with LLMs than if you'd rather write code yourself than review the same thing written by someone (or something) else.</li>
<li>A <strong>very weird form of management</strong>. Getting good results out of a coding agent feels uncomfortably close to getting good results out of a human collaborator. You need to provide clear instructions, ensure they have the necessary context and provide actionable feedback on what they produce. It's a <em>lot</em> easier than working with actual people because you don't have to worry about offending or discouraging them - but any existing management experience you have will prove surprisingly useful.</li>
<li>Really good <strong>manual QA (quality assurance)</strong>. Beyond automated tests, you need to be really good at manually testing software, including predicting and digging into edge-cases.</li>
<li>Strong <strong>research skills</strong>. There are dozens of ways to solve any given coding problem. Figuring out the best options and proving an approach has always been important, and remains a blocker on unleashing an agent to write the actual code.</li>
<li>The ability to <strong>ship to a preview environment</strong>. If an agent builds a feature, having a way to safely preview that feature (without deploying it straight to production) makes reviews much more productive and greatly reduces the risk of shipping something broken.</li>
<li>An instinct for <strong>what can be outsourced</strong> to AI and what you need to manually handle yourself. This is constantly evolving as the models and tools become more effective. A big part of working effectively with LLMs is maintaining a strong intuition for when they can best be applied.</li>
<li>An updated <strong>sense of estimation</strong>. Estimating how long a project will take has always been one of the hardest but most important parts of being a senior engineer, especially in organizations where budget and strategy decisions are made based on those estimates. AI-assisted coding makes this <em>even harder</em> - things that used to take a long time are much faster, but estimations now depend on new factors which we're all still trying to figure out.</li>
</ul>
<p>If you're going to really exploit the capabilities of these new tools, you need to be operating <em>at the top of your game</em>. You're not just responsible for writing the code - you're researching approaches, deciding on high-level architecture, writing specifications, defining success criteria, <a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/">designing agentic loops</a>, planning QA, managing a growing army of weird digital interns who will absolutely cheat if you give them a chance, and spending <em>so much time on code review</em>.</p>
<p>Almost all of these are characteristics of senior software engineers already!</p>
<p>AI tools <strong>amplify existing expertise</strong>. The more skills and experience you have as a software engineer the faster and better the results you can get from working with LLMs and coding agents.</p>
<h4 id="-vibe-engineering-really-">"Vibe engineering", really?</h4>
<p>Is this a stupid name? Yeah, probably. "Vibes" as a concept in AI feels a little tired at this point. "Vibe coding" itself is used by a lot of developers in a dismissive way. I'm ready to reclaim vibes for something more constructive.</p>
<p>I've never really liked the artificial distinction between "coders" and "engineers" - that's always smelled to me a bit like gatekeeping. But in this case a bit of gatekeeping is exactly what we need!</p>
<p><strong>Vibe engineering</strong> establishes a clear distinction from vibe coding. It signals that this is a different, harder and more sophisticated way of working with AI tools to build production software.</p>
<p>I like that this is cheeky and likely to be controversial. This whole space is still absurd in all sorts of different ways. We shouldn't take ourselves too seriously while we figure out the most productive ways to apply these new tools.</p>
<p>I've tried in the past to get terms like <strong><a href="https://simonwillison.net/tags/ai-assisted-programming/">AI-assisted programming</a></strong> to stick, with approximately zero success. May as well try rubbing some vibes on it and see what happens.</p>
<p>I also really like the clear mismatch between "vibes" and "engineering". It makes the combined term self-contradictory in a way that I find mischievous and (hopefully) sticky.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/code-review">code-review</a>, <a href="https://simonwillison.net/tags/definitions">definitions</a>, <a href="https://simonwillison.net/tags/software-engineering">software-engineering</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/parallel-agents">parallel-agents</a></p>]]></description><pubDate>Tue, 7 Oct 2025 14:32:25 +0000</pubDate></item></channel></rss>