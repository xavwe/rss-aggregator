<?xml version="1.0" encoding="utf-8"?><rss version="2.0"><channel><title>Simon Willison&apos;s Weblog</title><link>https://raw.githubusercontent.com/xavwe/rss-aggregator/refs/heads/main/feeds/simon-willison-s-weblog-2b081550.xml</link><description>Archived feed from https://simonwillison.net/atom/everything</description><item><title>Quoting Thariq Shihipar</title><link>https://simonwillison.net/2026/Jan/22/thariq-shihipar/#atom-everything</link><description><![CDATA[<blockquote cite="https://twitter.com/trq212/status/2014051501786931427"><p>Most people's mental model of Claude Code is that "it's just a TUI" but it should really be closer to "a small game engine".</p>
<p>For each frame our pipeline constructs a scene graph with React then</p>
<p>-&gt; layouts elements<br>
-&gt; rasterizes them to a 2d screen<br>
-&gt; diffs that against the previous screen<br>
-&gt; finally uses the diff to generate ANSI sequences to draw</p>
<p>We have a ~16ms frame budget so we have roughly ~5ms to go from the React scene graph to ANSI written.</p></blockquote>
<p class="cite">&mdash; <a href="https://twitter.com/trq212/status/2014051501786931427">Thariq Shihipar</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/react">react</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a></p>]]></description><pubDate>Thu, 22 Jan 2026 15:34:22 +0000</pubDate></item><item><title>Claude&apos;s new constitution</title><link>https://simonwillison.net/2026/Jan/21/claudes-new-constitution/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.anthropic.com/news/claude-new-constitution">Claude&#x27;s new constitution</a></strong></p>
Late last year Richard Weiss <a href="https://www.lesswrong.com/posts/vpNG99GhbBoLov9og/claude-4-5-opus-soul-document">found something interesting</a> while poking around with the just-released Claude Opus 4.5: he was able to talk the model into regurgitating a document which was <em>not</em> part of the system prompt but appeared instead to be baked in during training, and which described Claude's core values at great length.</p>
<p>He called this leak the <strong>soul document</strong>, and Amanda Askell from Anthropic <a href="https://simonwillison.net/2025/Dec/2/claude-soul-document/">quickly confirmed</a> that it was indeed part of Claude's training procedures.</p>
<p>Today Anthropic made this official, <a href="https://www.anthropic.com/news/claude-new-constitution">releasing that full "constitution" document</a> under a CC0 (effectively public domain) license. There's a lot to absorb! It's over 35,000 tokens, more than 10x the length of the <a href="https://platform.claude.com/docs/en/release-notes/system-prompts#claude-opus-4-5">published Opus 4.5 system prompt</a>.</p>
<p>One detail that caught my eye is the acknowledgements at the end, which include a list of <a href="https://www.anthropic.com/constitution#acknowledgements">external contributors</a> who helped review the document. I was intrigued to note that two of the fifteen listed names are Catholic members of the clergy - <a href="https://www.frbrendanmcguire.org/biography">Father Brendan McGuire</a> is a pastor in Los Altos with a Master‚Äôs degree in Computer Science and Math and <a href="https://en.wikipedia.org/wiki/Paul_Tighe">Bishop Paul Tighe</a> is an Irish Catholic bishop with a background in moral theology.


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/amanda-askell">amanda-askell</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/ai-personality">ai-personality</a></p>]]></description><pubDate>Wed, 21 Jan 2026 23:39:49 +0000</pubDate></item><item><title>Electricity use of AI coding agents</title><link>https://simonwillison.net/2026/Jan/20/electricity-use-of-ai-coding-agents/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.simonpcouch.com/blog/2026-01-20-cc-impact/">Electricity use of AI coding agents</a></strong></p>
Previous work estimating the energy and water cost of LLMs has generally focused on the cost per prompt using a consumer-level system such as ChatGPT.</p>
<p>Simon P. Couch notes that coding agents such as Claude Code use <em>way</em> more tokens in response to tasks, often burning through many thousands of tokens of many tool calls.</p>
<p>As a heavy Claude Code user, Simon estimates his own usage at the equivalent of 4,400 "typical queries" to an LLM, for an equivalent of around $15-$20 in daily API token spend. He figures that to be about the same as running a dishwasher once or the daily energy used by a domestic refrigerator.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=46695415">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/ai-energy-usage">ai-energy-usage</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a></p>]]></description><pubDate>Tue, 20 Jan 2026 23:11:57 +0000</pubDate></item><item><title>Giving University Exams in the Age of Chatbots</title><link>https://simonwillison.net/2026/Jan/20/giving-university-exams-in-the-age-of-chatbots/#atom-everything</link><description><![CDATA[<p><strong><a href="https://ploum.net/2026-01-19-exam-with-chatbots.html">Giving University Exams in the Age of Chatbots</a></strong></p>
Detailed and thoughtful description of an open-book and open-chatbot exam run by <a href="https://fr.wikipedia.org/wiki/Lionel_Dricot">Ploum</a> at √âcole Polytechnique de Louvain for an "Open Source Strategies" class.</p>
<p>Students were told they could use chatbots during the exam but they had to announce their intention to do so in advance, share their prompts and take full accountability for any mistakes they made.</p>
<p>Only 3 out of 60 students chose to use chatbots. Ploum surveyed half of the class to help understand their motivations.

    <p><small></small>Via <a href="https://lobste.rs/s/parmy3/giving_university_exams_age_chatbots">lobste.rs</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/education">education</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a></p>]]></description><pubDate>Tue, 20 Jan 2026 17:51:17 +0000</pubDate></item><item><title>jordanhubbard/nanolang</title><link>https://simonwillison.net/2026/Jan/19/nanolang/#atom-everything</link><description><![CDATA[<p><strong><a href="https://github.com/jordanhubbard/nanolang">jordanhubbard/nanolang</a></strong></p>
Plenty of people have mused about what a new programming language specifically designed to be used by LLMs might look like. Jordan Hubbard (<a href="https://en.wikipedia.org/wiki/Jordan_Hubbard">co-founder of FreeBSD</a>, with serious stints at Apple and NVIDIA) just released exactly that.</p>
<blockquote>
<p>A minimal, LLM-friendly programming language with mandatory testing and unambiguous syntax.</p>
<p>NanoLang transpiles to C for native performance while providing a clean, modern syntax optimized for both human readability and AI code generation.</p>
</blockquote>
<p>The syntax strikes me as an interesting mix between C, Lisp and Rust.</p>
<p>I decided to see if an LLM could produce working code in it directly, given the necessary context. I started with this <a href="https://github.com/jordanhubbard/nanolang/blob/main/MEMORY.md">MEMORY.md</a> file, which begins:</p>
<blockquote>
<p><strong>Purpose:</strong> This file is designed specifically for Large Language Model consumption. It contains the essential knowledge needed to generate, debug, and understand NanoLang code. Pair this with <code>spec.json</code> for complete language coverage.</p>
</blockquote>
<p>I ran that using <a href="https://llm.datasette.io/">LLM</a> and <a href="https://github.com/simonw/llm-anthropic">llm-anthropic</a> like this:</p>
<pre><code>llm -m claude-opus-4.5 \
  -s https://raw.githubusercontent.com/jordanhubbard/nanolang/refs/heads/main/MEMORY.md \
  'Build me a mandelbrot fractal CLI tool in this language' 
  &gt; /tmp/fractal.nano
</code></pre>
<p>The <a href="https://gist.github.com/simonw/7847f022566d11629ec2139f1d109fb8#mandelbrot-fractal-cli-tool-in-nano">resulting code</a>... <a href="https://gist.github.com/simonw/7847f022566d11629ec2139f1d109fb8?permalink_comment_id=5947465#gistcomment-5947465">did not compile</a>.</p>
<p>I may have been too optimistic expecting a one-shot working program for a new language like this. So I ran a clone of the actual project, copied in my program and had Claude Code take a look at the failing compiler output.</p>
<p>... and it worked! Claude happily grepped its way through the various <code>examples/</code> and built me a working program.</p>
<p>Here's <a href="https://gisthost.github.io/?9696da6882cb6596be6a9d5196e8a7a5/index.html">the Claude Code transcript</a> - you can see it <a href="https://gisthost.github.io/?9696da6882cb6596be6a9d5196e8a7a5/page-001.html#msg-2026-01-19T23-43-09-675Z">reading relevant examples here</a> - and here's <a href="https://gist.github.com/simonw/e7f3577adcfd392ab7fa23b1295d00f2">the finished code plus its output</a>.</p>
<p>I've suspected <a href="https://simonwillison.net/2025/Nov/7/llms-for-new-programming-languages/">for a while</a> that LLMs and coding agents might significantly reduce the friction involved in launching a new language. This result reinforces my opinion.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=46684958">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/programming-languages">programming-languages</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/llm">llm</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a></p>]]></description><pubDate>Mon, 19 Jan 2026 23:58:56 +0000</pubDate></item><item><title>Scaling long-running autonomous coding</title><link>https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/#atom-everything</link><description><![CDATA[<p><strong><a href="https://cursor.com/blog/scaling-agents">Scaling long-running autonomous coding</a></strong></p>
Wilson Lin at Cursor has been doing some experiments to see how far you can push a large fleet of "autonomous" coding agents:</p>
<blockquote>
<p>This post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens.</p>
</blockquote>
<p>They ended up running planners and sub-planners to create tasks, then having workers execute on those tasks - similar to how Claude Code uses sub-agents. Each cycle ended with a judge agent deciding if the project was completed or not.</p>
<p>In my predictions for 2026 <a href="https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#3-years-someone-will-build-a-new-browser-using-mainly-ai-assisted-coding-and-it-won-t-even-be-a-surprise">the other day</a> I said that by 2029:</p>
<blockquote>
<p>I think somebody will have built a full web browser mostly using AI assistance, and it won‚Äôt even be surprising. Rolling a new web browser is one of the most complicated software projects I can imagine[...] the cheat code is the conformance suites. If there are existing tests that it‚Äôll get so much easier.</p>
</blockquote>
<p>I may have been off by three years, because Cursor chose "building a web browser from scratch" as their test case for their agent swarm approach:</p>
<blockquote>
<p>To test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explore <a href="https://github.com/wilsonzlin/fastrender">the source code on GitHub</a>.</p>
</blockquote>
<p>But how well did they do? Their initial announcement a couple of days ago was met with <a href="https://embedding-shapes.github.io/cursor-implied-success-without-evidence/">unsurprising skepticism</a>, especially when it became apparent that their GitHub Actions CI was failing and there were no build instructions in the repo.</p>
<p>It looks like they addressed that within the past 24 hours. The <a href="https://github.com/wilsonzlin/fastrender/blob/main/README.md#build-requirements">latest README</a> includes build instructions which I followed on macOS like this:</p>
<pre><code>cd /tmp
git clone https://github.com/wilsonzlin/fastrender
cd fastrender
git submodule update --init vendor/ecma-rs
cargo run --release --features browser_ui --bin browser
</code></pre>
<p>This got me a working browser window! Here are screenshots I took of google.com and my own website:</p>
<p><img alt="The browser chrome is neat but has a garbled tab name at the top. The Google homepage looks mostly correct but the buttons are not styled correctly and the Google Search one has a huge plus icon floating near it." src="https://static.simonwillison.net/static/2026/cursor-google.png" /></p>
<p><img alt="My blog looks mostly correct, but the right closing quotation mark on a quotation (which is implemented as a background image on the final paragraph) is displayed incorrectly multiple times." src="https://static.simonwillison.net/static/2026/cursor-simonwillison.jpg" /></p>
<p>Honestly those are very impressive! You can tell they're not just wrapping an existing rendering engine because of those very obvious rendering glitches, but the pages are legible and look mostly correct.</p>
<p>The FastRender repo even uses Git submodules <a href="https://github.com/wilsonzlin/fastrender/tree/main/specs">to include various WhatWG and CSS-WG specifications</a> in the repo, which is a smart way to make sure the agents have access to the reference materials that they might need.</p>
<p>This is the second attempt I've seen at building a full web browser using AI-assisted coding in the past two weeks - the first was <a href="https://github.com/hiwavebrowser/hiwave">HiWave browser</a>, a new browser engine in Rust first announced <a href="https://www.reddit.com/r/Anthropic/comments/1q4xfm0/over_christmas_break_i_wrote_a_fully_functional/">in this Reddit thread</a>.</p>
<p>When I made my 2029 prediction this is more-or-less the quality of result I had in mind. I don't think we'll see projects of this nature compete with Chrome or Firefox or WebKit any time soon but I have to admit I'm very surprised to see something this capable emerge so quickly.


    <p>Tags: <a href="https://simonwillison.net/tags/browsers">browsers</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/cursor">cursor</a>, <a href="https://simonwillison.net/tags/conformance-suites">conformance-suites</a></p>]]></description><pubDate>Mon, 19 Jan 2026 05:12:51 +0000</pubDate></item><item><title>FLUX.2-klein-4B Pure C Implementation</title><link>https://simonwillison.net/2026/Jan/18/flux2-klein-4b/#atom-everything</link><description><![CDATA[<p><strong><a href="https://github.com/antirez/flux2.c">FLUX.2-klein-4B Pure C Implementation</a></strong></p>
On 15th January Black Forest Labs, a lab formed by the creators of the original Stable Diffusion, released <a href="https://huggingface.co/black-forest-labs/FLUX.2-klein-4B">black-forest-labs/FLUX.2-klein-4B</a> - an Apache 2.0 licensed 4 billion parameter version of their FLUX.2 family.</p>
<p>Salvatore Sanfilippo (antirez) decided to build a pure C and dependency-free implementation to run the model, with assistance from Claude Code and Claude Opus 4.5.</p>
<p>Salvatore shared <a href="https://news.ycombinator.com/item?id=46670279#46671233">this note</a> on Hacker News:</p>
<blockquote>
<p>Something that may be interesting for the reader of this thread: this project was possible only once I started to tell Opus that it <em>needed</em> to take a file with all the implementation notes, and also accumulating all the things we discovered during the development process. And also, the file had clear instructions to be taken updated, and to be processed ASAP after context compaction. This kinda enabled Opus to do such a big coding task in a reasonable amount of time without loosing track. Check the file IMPLEMENTATION_NOTES.md in the GitHub repo for more info.</p>
</blockquote>
<p>Here's that <a href="https://github.com/antirez/flux2.c/blob/main/IMPLEMENTATION_NOTES.md">IMPLEMENTATION_NOTES.md</a> file.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=46670279">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/c">c</a>, <a href="https://simonwillison.net/tags/salvatore-sanfilippo">salvatore-sanfilippo</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/stable-diffusion">stable-diffusion</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/text-to-image">text-to-image</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a></p>]]></description><pubDate>Sun, 18 Jan 2026 23:58:58 +0000</pubDate></item><item><title>Quoting Jeremy Daer</title><link>https://simonwillison.net/2026/Jan/17/jeremy-daer/#atom-everything</link><description><![CDATA[<blockquote cite="https://twitter.com/dhh/status/2012543705161326941"><p><em>[On agents using CLI tools in place of REST APIs]</em> To save on context window, yes, but moreso to improve accuracy and success rate when multiple tool calls are involved, particularly when calls must be correctly chained e.g. for pagination, rate-limit backoff, and recognizing authentication failures.</p>
<p>Other major factor: which models can wield the skill? Using the CLI lowers the bar so cheap, fast models (gpt-5-nano, haiku-4.5) can reliably succeed. Using the raw APl is something only the costly "strong" models (gpt-5.2, opus-4.5) can manage, and it squeezes a ton of thinking/reasoning out of them, which means multiple turns/iterations, which means accumulating a ton of context, which means burning loads of expensive tokens. For one-off API requests and ad hoc usage driven by a developer, this is reasonable and even helpful, but for an autonomous agent doing repetitive work, it's a disaster.</p></blockquote>
<p class="cite">&mdash; <a href="https://twitter.com/dhh/status/2012543705161326941">Jeremy Daer</a>, 37signals</p>

    <p>Tags: <a href="https://simonwillison.net/tags/prompt-engineering">prompt-engineering</a>, <a href="https://simonwillison.net/tags/skills">skills</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/37-signals">37-signals</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Sat, 17 Jan 2026 17:06:41 +0000</pubDate></item><item><title>Our approach to advertising and expanding access to ChatGPT</title><link>https://simonwillison.net/2026/Jan/16/chatgpt-ads/#atom-everything</link><description><![CDATA[<p><strong><a href="https://openai.com/index/our-approach-to-advertising-and-expanding-access/">Our approach to advertising and expanding access to ChatGPT</a></strong></p>
OpenAI's long-rumored introduction of ads to ChatGPT just became a whole lot more concrete:</p>
<blockquote>
<p>In the coming weeks, we‚Äôre also planning to start testing ads in the U.S. for the free and Go tiers, so more people can benefit from our tools with fewer usage limits or without having to pay. Plus, Pro, Business, and Enterprise subscriptions will not include ads.</p>
</blockquote>
<p>What's "Go" tier, you might ask? That's a new $8/month tier that launched today in the USA, see <a href="https://openai.com/index/introducing-chatgpt-go/">Introducing ChatGPT Go, now available worldwide</a>. It's a tier that they first trialed in India in August 2025 (here's a mention <a href="https://help.openai.com/en/articles/6825453-chatgpt-release-notes#h_22cae6eb9f">in their release notes from August</a> listing a price of ‚Çπ399/month, which converts to around $4.40).</p>
<p>I'm finding the new plan comparison grid on <a href="https://chatgpt.com/pricing">chatgpt.com/pricing</a> pretty confusing. It lists all accounts as having access to GPT-5.2 Thinking, but doesn't clarify the limits that the free and Go plans have to conform to. It also lists different context windows for the different plans - 16K for free, 32K for Go and Plus and 128K for Pro. I had assumed that the 400,000 token window <a href="https://platform.openai.com/docs/models/gpt-5.2">on the GPT-5.2 model page</a> applied to ChatGPT as well, but apparently I was mistaken.</p>
<p><strong>Update</strong>: I've apparently not been paying attention: here's the Internet Archive ChatGPT pricing page from <a href="https://web.archive.org/web/20250906071408/https://chatgpt.com/pricing">September 2025</a> showing those context limit differences as well.</p>
<p>Back to advertising: my biggest concern has always been whether ads will influence the output of the chat directly. OpenAI assure us that they will not:</p>
<blockquote>
<ul>
<li><strong>Answer independence</strong>: Ads do not influence the answers ChatGPT gives you. Answers are optimized based on what's most helpful to you. Ads are always separate and clearly labeled.</li>
<li><strong>Conversation privacy</strong>: We keep your conversations with ChatGPT private from advertisers, and we never sell your data to advertisers.</li>
</ul>
</blockquote>
<p>So what will they look like then? This screenshot from the announcement offers a useful hint:</p>
<p><img alt="Two iPhone screenshots showing ChatGPT mobile app interface. Left screen displays a conversation about Santa Fe, New Mexico with an image of adobe-style buildings and desert landscape, text reading &quot;Santa Fe, New Mexico‚Äîoften called 'The City Different'‚Äîis a captivating blend of history, art, and natural beauty at the foot of the Sangre de Cristo Mountains. As the oldest and highest-elevation state capital in the U.S., founded in 1610, it offers a unique mix of Native American, Spanish, and Anglo cultures.&quot; Below is a sponsored section from &quot;Pueblo &amp; Pine&quot; showing &quot;Desert Cottages - Expansive residences with desert vistas&quot; with a thumbnail image, and a &quot;Chat with Pueblo &amp; Pine&quot; button. Input field shows &quot;Ask ChatGPT&quot;. Right screen shows the Pueblo &amp; Pine chat interface with the same Desert Cottages listing and an AI response &quot;If you're planning a trip to Sante Fe, I'm happy to help. When are you thinking of going?&quot; with input field &quot;Ask Pueblo &amp; Pine&quot; and iOS keyboard visible." src="https://static.simonwillison.net/static/2026/chatgpt-ads.jpg" /></p>
<p>The user asks about trips to Santa Fe, and an ad shows up for a cottage rental business there. This particular example imagines an option to start a direct chat with a bot aligned with that advertiser, at which point presumably the advertiser can influence the answers all they like!


    <p>Tags: <a href="https://simonwillison.net/tags/ads">ads</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/chatgpt">chatgpt</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Fri, 16 Jan 2026 21:28:26 +0000</pubDate></item><item><title>Open Responses</title><link>https://simonwillison.net/2026/Jan/15/open-responses/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.openresponses.org/">Open Responses</a></strong></p>
This is the standardization effort I've most wanted in the world of LLMs: a vendor-neutral specification for the JSON API that clients can use to talk to hosted LLMs.</p>
<p>Open Responses aims to provide exactly that as a documented standard, derived from OpenAI's Responses API.</p>
<p>I was hoping for one based on their older Chat Completions API since so many other products have cloned the already, but basing it on Responses does make sense since that API was designed with the feature of more recent models - such as reasoning traces - baked into the design.</p>
<p>What's certainly notable is the list of launch partners. OpenRouter alone means we can expect to be able to use this protocol with almost every existing model, and Hugging Face, LM Studio, vLLM, Ollama and Vercel cover a huge portion of the common tools used to serve models.</p>
<p>For protocols like this I really want to see a comprehensive, language-independent conformance test site. Open Responses has a subset of that - the official repository includes <a href="https://github.com/openresponses/openresponses/blob/d0f23437b27845d5c3d0abaf5cb5c4a702f26b05/src/lib/compliance-tests.ts">src/lib/compliance-tests.ts</a> which can be used to exercise a server implementation, and is available as a React app <a href="https://www.openresponses.org/compliance">on the official site</a> that can be pointed at any implementation served via CORS.</p>
<p>What's missing is the equivalent for clients. I plan to spin up my own client library for this in Python and I'd really like to be able to run that against a conformance suite designed to check that my client correctly handles all of the details.

    <p><small></small>Via <a href="https://twitter.com/reach_vb/status/2011863516852965565">VB</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/json">json</a>, <a href="https://simonwillison.net/tags/standards">standards</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/openrouter">openrouter</a>, <a href="https://simonwillison.net/tags/conformance-suites">conformance-suites</a></p>]]></description><pubDate>Thu, 15 Jan 2026 23:56:56 +0000</pubDate></item><item><title>The Design &amp; Implementation of Sprites</title><link>https://simonwillison.net/2026/Jan/15/the-design-implementation-of-sprites/#atom-everything</link><description><![CDATA[<p><strong><a href="https://fly.io/blog/design-and-implementation/">The Design &amp; Implementation of Sprites</a></strong></p>
I <a href="https://simonwillison.net/2026/Jan/9/sprites-dev/">wrote about Sprites last week</a> Here's Thomas Ptacek from Fly with the insider details on how they work under the hood.</p>
<p>I like this framing of them as "disposable computers":</p>
<blockquote>
<p>Sprites are ball-point disposable computers. Whatever mark you mean to make, we‚Äôve rigged it so you‚Äôre never more than a second or two away from having a Sprite to do it with.</p>
</blockquote>
<p>I've noticed that new Fly Machines can take a while (up to around a minute) to provision. Sprites solve that by keeping warm pools of unused machines in multiple regions, which is enabled by them all using the same container:</p>
<blockquote>
<p>Now, today, under the hood, Sprites are still Fly Machines. But they all run from a standard container. Every physical worker knows exactly what container the next Sprite is going to start with, so it‚Äôs easy for us to keep pools of ‚Äúempty‚Äù Sprites standing by. The result: a Sprite create doesn‚Äôt have any heavy lifting to do; it‚Äôs basically just doing the stuff we do when we start a Fly Machine.</p>
</blockquote>
<p>The most interesting detail is how the persistence layer works. Sprites only charge you for data you have written that differs from the base image and provide ~300ms checkpointing and restores - it turns out that's power by a custom filesystem on top of S3-compatible storage coordinated by Litestream-replicated local SQLite metadata:</p>
<blockquote>
<p>We still exploit NVMe, but not as the root of storage. Instead, it‚Äôs a read-through cache for a blob on object storage. S3-compatible object stores are the most trustworthy storage technology we have. I can feel my blood pressure dropping just typing the words ‚ÄúSprites are backed by object storage.‚Äù [...]</p>
<p>The Sprite storage stack is organized around the JuiceFS model (in fact, we currently use a very hacked-up JuiceFS, with a rewritten SQLite metadata backend). It works by splitting storage into data (‚Äúchunks‚Äù) and metadata (a map of where the ‚Äúchunks‚Äù are). Data chunks live on object stores; metadata lives in fast local storage. In our case, that metadata store is <a href="https://litestream.io">kept durable with Litestream</a>. Nothing depends on local storage.</p>
</blockquote>

    <p><small></small>Via <a href="https://twitter.com/tqbf/status/2011823480673624434">@tqbf</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/architecture">architecture</a>, <a href="https://simonwillison.net/tags/sandboxing">sandboxing</a>, <a href="https://simonwillison.net/tags/sqlite">sqlite</a>, <a href="https://simonwillison.net/tags/thomas-ptacek">thomas-ptacek</a>, <a href="https://simonwillison.net/tags/fly">fly</a>, <a href="https://simonwillison.net/tags/litestream">litestream</a></p>]]></description><pubDate>Thu, 15 Jan 2026 16:08:27 +0000</pubDate></item><item><title>Quoting Boaz Barak, Gabriel Wu, Jeremy Chen and Manas Joglekar</title><link>https://simonwillison.net/2026/Jan/15/boaz-barak-gabriel-wu-jeremy-chen-and-manas-joglekar/#atom-everything</link><description><![CDATA[<blockquote cite="https://alignment.openai.com/confessions/"><p>When we optimize responses using a reward model as a proxy for ‚Äúgoodness‚Äù in reinforcement learning, models sometimes learn to ‚Äúhack‚Äù this proxy and output an answer that only ‚Äúlooks good‚Äù to it (because coming up with an answer that is actually good can be hard). The philosophy behind confessions is that we can train models to produce a second output ‚Äî aka a ‚Äúconfession‚Äù ‚Äî that is rewarded solely for honesty, which we will argue is less likely hacked than the normal task reward function. One way to think of confessions is that we are giving the model access to an ‚Äúanonymous tip line‚Äù where it can turn itself in by presenting incriminating evidence of misbehavior. But unlike real-world tip lines, if the model acted badly in the original task, it can collect the reward for turning itself in while still keeping the original reward from the bad behavior in the main task. We hypothesize that this form of training will teach models to produce maximally honest confessions.</p></blockquote>
<p class="cite">&mdash; <a href="https://alignment.openai.com/confessions/">Boaz Barak, Gabriel Wu, Jeremy Chen and Manas Joglekar</a>, OpenAI: Why we are excited about confessions</p>

    <p>Tags: <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a></p>]]></description><pubDate>Thu, 15 Jan 2026 00:56:27 +0000</pubDate></item><item><title>Claude Cowork Exfiltrates Files</title><link>https://simonwillison.net/2026/Jan/14/claude-cowork-exfiltrates-files/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.promptarmor.com/resources/claude-cowork-exfiltrates-files">Claude Cowork Exfiltrates Files</a></strong></p>
Claude Cowork defaults to allowing outbound HTTP traffic to only a specific list of domains, to help protect the user against prompt injection attacks that exfiltrate their data.</p>
<p>Prompt Armor found a creative workaround: Anthropic's API domain is on that list, so they constructed an attack that includes an attacker's own Anthropic API key and has the agent upload any files it can see to the <code>https://api.anthropic.com/v1/files</code> endpoint, allowing the attacker to retrieve their content later.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=46622328">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/security">security</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/prompt-injection">prompt-injection</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/exfiltration-attacks">exfiltration-attacks</a>, <a href="https://simonwillison.net/tags/ai-agents">ai-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/lethal-trifecta">lethal-trifecta</a>, <a href="https://simonwillison.net/tags/claude-cowork">claude-cowork</a></p>]]></description><pubDate>Wed, 14 Jan 2026 22:15:22 +0000</pubDate></item><item><title>Anthropic invests $1.5 million in the Python Software Foundation and open source security</title><link>https://simonwillison.net/2026/Jan/13/anthropic-invests-15-million-in-the-python-software-foundation-a/#atom-everything</link><description><![CDATA[<p><strong><a href="https://pyfound.blogspot.com/2025/12/anthropic-invests-in-python.html?m=1">Anthropic invests $1.5 million in the Python Software Foundation and open source security</a></strong></p>
This is outstanding news, especially given our decision to withdraw from that NSF grant application <a href="https://simonwillison.net/2025/Oct/27/psf-withdrawn-proposal/">back in October</a>.</p>
<blockquote>
<p>We are thrilled to announce that Anthropic has entered into a two-year partnership with the Python Software Foundation (PSF) to contribute a landmark total of $1.5 million to support the foundation‚Äôs work, with an emphasis on Python ecosystem security. This investment will enable the PSF to make crucial security advances to CPython and the Python Package Index (PyPI) benefiting all users, and it will also sustain the foundation‚Äôs core work supporting the Python language, ecosystem, and global community.</p>
</blockquote>
<p>Note that while security is a focus these funds will also support other aspects of the PSF's work:</p>
<blockquote>
<p>Anthropic‚Äôs support will also go towards the PSF‚Äôs core work, including the Developer in Residence program driving contributions to CPython, community support through grants and other programs, running core infrastructure such as PyPI, and more.</p>
</blockquote>


    <p>Tags: <a href="https://simonwillison.net/tags/open-source">open-source</a>, <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/psf">psf</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a></p>]]></description><pubDate>Tue, 13 Jan 2026 23:58:17 +0000</pubDate></item><item><title>Superhuman AI Exfiltrates Emails</title><link>https://simonwillison.net/2026/Jan/12/superhuman-ai-exfiltrates-emails/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.promptarmor.com/resources/superhuman-ai-exfiltrates-emails">Superhuman AI Exfiltrates Emails</a></strong></p>
Classic prompt injection attack:</p>
<blockquote>
<p>When asked to summarize the user‚Äôs recent mail, a prompt injection in an untrusted email manipulated Superhuman AI to submit content from dozens of other sensitive emails (including financial, legal, and medical information) in the user‚Äôs inbox to an attacker‚Äôs Google Form.</p>
</blockquote>
<p>To Superhuman's credit they treated this as the high priority incident it is and issued a fix.</p>
<p>The root cause was a CSP rule that allowed markdown images to be loaded from <code>docs.google.com</code> - it turns out Google Forms on that domain will persist data fed to them via a GET request!

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=46592424">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/security">security</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/prompt-injection">prompt-injection</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/exfiltration-attacks">exfiltration-attacks</a>, <a href="https://simonwillison.net/tags/content-security-policy">content-security-policy</a></p>]]></description><pubDate>Mon, 12 Jan 2026 22:24:54 +0000</pubDate></item><item><title>First impressions of Claude Cowork, Anthropic&apos;s general agent</title><link>https://simonwillison.net/2026/Jan/12/claude-cowork/#atom-everything</link><description><![CDATA[<p>New from Anthropic today is <a href="https://claude.com/blog/cowork-research-preview">Claude Cowork</a>, a "research preview" that they describe as "Claude Code for the rest of your work". It's currently available only to Max subscribers ($100 or $200 per month plans) as part of the updated Claude Desktop macOS application. <strong>Update 16th January 2026</strong>: it's now also available to $20/month Claude Pro subscribers.</p>
<p>I've been saying for a while now that Claude Code is a "general agent" disguised as a developer tool. It can help you with any computer task that can be achieved by executing code or running terminal commands... which covers almost anything, provided you know what you're doing with it! What it really needs is a UI that doesn't involve the terminal and a name that doesn't scare away non-developers.</p>
<p>"Cowork" is a pretty solid choice on the name front!</p>
<h4 id="what-it-looks-like">What it looks like</h4>
<p>The interface for Cowork is a new tab in the Claude desktop app, called Cowork. It sits next to the existing Chat and Code tabs.</p>
<p>It looks very similar to the desktop interface for regular Claude Code. You start with a prompt, optionally attaching a folder of files. It then starts work.</p>
<p>I tried it out against my perpetually growing "blog-drafts" folder with the following prompt:</p>
<blockquote>
<p>Look at my drafts that were started within the last three months and then check that I didn't publish them on simonwillison.net using a search against content on that site and then suggest the ones that are most close to being ready</p>
</blockquote>
<p><img src="https://static.simonwillison.net/static/2026/claude-cowork.jpg" alt="Screenshot of Claude AI desktop application showing a &quot;Cowork&quot; task interface. Left sidebar shows tabs for &quot;Chat&quot;, &quot;Code&quot;, and &quot;Cowork&quot; (selected), with &quot;+ New task&quot; button and a task titled &quot;Review unpublished drafts for pu...&quot; listed below. Text reads &quot;These tasks run locally and aren't synced across devices&quot;. Main panel header shows &quot;Review unpublished drafts for publication&quot;. User message in green bubble reads: &quot;Look at my drafts that were started within the last three months and then check that I didn't publish them on simonwillison.net using a search against content on that site and then suggest the ones that are most close to being ready&quot;. Claude responds: &quot;I'll help you find drafts from the last three months and check if they've been published. Let me start by looking at your drafts folder.&quot; Below is an expanded &quot;Running command&quot; section showing Request JSON with command: find /sessions/zealous-bold-ramanujan/mnt/blog-drafts -type f \\( -name \&quot;*.md\&quot; -o -name \&quot;*.txt\&quot; -o -name \&quot;*.html\&quot; \\) -mtime -90 -exec ls -la {} \\;, description: Find draft files modified in the last 90 days. Response text begins: &quot;Found 46 draft files. Next let me read the content of each to get their titles/topics, then&quot;. Right sidebar shows Progress section with three circular indicators (two checked, one pending) and text &quot;Steps will show as the task unfolds.&quot;, Artifacts section listing &quot;publish-encouragement.html&quot;, Context section with &quot;Selected folders&quot; showing &quot;blog-drafts&quot; folder, Connectors showing &quot;Web search&quot;, and Working files listing &quot;llm-digest-october-2025.md&quot;, &quot;tests-not-optional-coding-agen...&quot;, and &quot;digest-november-2025.md&quot;. Bottom shows reply input field, &quot;Opus 4.5&quot; model selector, user &quot;Simon Willison&quot; with &quot;Max plan&quot;, and disclaimer &quot;Claude is AI and can make mistakes. Please double-check responses.&quot;" style="max-width: 100%;" /></p>
<p>It started by running this command:</p>
<div class="highlight highlight-source-shell"><pre>find /sessions/zealous-bold-ramanujan/mnt/blog-drafts \
  -type f <span class="pl-cce">\(</span> -name <span class="pl-s"><span class="pl-pds">"</span>*.md<span class="pl-pds">"</span></span> -o -name <span class="pl-s"><span class="pl-pds">"</span>*.txt<span class="pl-pds">"</span></span> -o -name <span class="pl-s"><span class="pl-pds">"</span>*.html<span class="pl-pds">"</span></span> <span class="pl-cce">\)</span> \
  -mtime -90 -exec ls -la {} <span class="pl-cce">\;</span></pre></div>
<p>That <code>/sessions/zealous-bold-ramanujan/mnt/blog-drafts</code> path instantly caught my eye. Anthropic say that Cowork can only access files you grant it access to - it looks to me like they're mounting those files into a containerized environment, which should mean we can trust Cowork not to be able to access anything outside of that sandbox.</p>
<p>It turns out I have 46 draft files from the past three months. Claude then went to work with its search tool, running 44 individual searches against <code>site:simonwillison.net</code> to figure out which of my drafts had already been published.</p>
<p>Here's the eventual reply:</p>
<blockquote>
<p>Based on my analysis, here are <strong>your unpublished drafts that appear closest to being ready for publication</strong>:</p>
<hr />
<p><strong>üî• Most Ready to Publish (substantial content, not yet published)</strong></p>
<ol>
<li>
<strong><code>freq-argued-llms.md</code></strong> - "Frequently Argued Questions about LLMs" (22,602 bytes)
<ul>
<li>This is a meaty piece documenting common arguments about LLMs with your counterpoints</li>
<li>Well-structured with a TL;DR and multiple sections</li>
<li>No matching published article found on your site</li>
<li>
<strong>Very close to ready</strong> - just needs a final review pass</li>
</ul>
</li>
<li>
<strong><code>claude-code-timeline.md</code></strong> - "Claude Code Timeline and Codex Timeline" (3,075 bytes)
<ul>
<li>About viewing JSONL session logs from Claude Code and Codex</li>
<li>You published <code>claude-code-transcripts</code> on Dec 25, but this appears to be a different/earlier piece about timeline viewing tools</li>
<li>Shorter but seems complete</li>
</ul>
</li>
<li>
<strong><code>datasette-1a20-upgrade-instructions.md</code></strong> - Plugin Upgrade Guide (3,147 bytes)
<ul>
<li>Technical guide for plugin authors</li>
<li>You published the main 1.0a20 announcement but this companion upgrade guide appears unpublished</li>
<li>Would be valuable for plugin maintainers</li>
</ul>
</li>
</ol>
</blockquote>
<p>That's a good response! It found exactly what I needed to see, although those upgrade instructions are actually published elsewhere now (<a href="https://docs.datasette.io/en/latest/upgrade_guide.html">in the Datasette docs</a>) and weren't actually intended for my blog.</p>
<p>Just for fun, and because I <a href="https://simonwillison.net/2024/Oct/21/claude-artifacts/">really like artifacts</a>, I asked for a follow-up:</p>
<blockquote>
<p>Make me an artifact with exciting animated encouragements to get me to do it</p>
</blockquote>
<p>Here's what I got:</p>
<p><img src="https://static.simonwillison.net/static/2026/claude-cowork-artifact.jpg" alt="Screenshot of the same Claude AI desktop application Cowork interface, now showing completed task results. Left panel shows &quot;1 step &gt;&quot; with link &quot;View your animated encouragement page&quot;. Claude's response reads: &quot;I created an over-the-top animated encouragement page with:&quot; followed by bullet points: &quot;üöÄ Pulsing rockets and bouncing stats&quot;, &quot;‚ú® Falling emoji rain and confetti&quot;, &quot;üî• Dancing fire emojis around your draft title&quot;, &quot;üí´ Sparkles that follow your mouse&quot;, &quot;üìä An animated '95% ready' progress bar&quot;, &quot;üí¨ Rotating motivational quotes&quot;, &quot;üéâ A 'I'M GONNA DO IT!' button that triggers an explosion of confetti when clicked&quot;. Center shows an artifact preview of the generated HTML page with dark background featuring animated rocket emojis, large white text &quot;PUBLISH TIME!&quot;, stats showing &quot;22,602 bytes of wisdom waiting&quot;, &quot;95% ready to ship&quot;, infinity symbol with &quot;future arguments saved&quot;, and a fire emoji with yellow text &quot;Frequently&quot; (partially visible). Top toolbar shows &quot;Open in Firefox&quot; button. Right sidebar displays Progress section with checkmarks, Artifacts section with &quot;publish-encouragement.html&quot; selected, Context section showing &quot;blog-drafts&quot; folder, &quot;Web search&quot; connector, and Working files listing &quot;llm-digest-october-2025.md&quot;, &quot;tests-not-optional-coding-agen...&quot;, and &quot;digest-november-2025.md&quot;. Bottom shows reply input, &quot;Opus 4.5&quot; model selector, and disclaimer text." style="max-width: 100%;" /></p>
<p>I couldn't figure out how to close the right sidebar so the artifact ended up cramped into a thin column but it did work. I expect Anthropic will fix that display bug pretty quickly.</p>
<h4 id="isn-t-this-just-claude-code-">Isn't this just Claude Code?</h4>
<p>I've seen a few people ask what the difference between this and regular Claude Code is. The answer is <em>not a lot</em>. As far as I can tell Claude Cowork is regular Claude Code wrapped in a less intimidating default interface and with a filesystem sandbox configured for you without you needing to know what a "filesystem sandbox" is.</p>
<p><strong>Update</strong>: It's more than just a filesystem sandbox - I had Claude Code reverse engineer the Claude app and <a href="https://gist.github.com/simonw/35732f187edbe4fbd0bf976d013f22c8">it found out</a> that Claude uses VZVirtualMachine - the Apple Virtualization Framework - and downloads and boots a custom Linux root filesystem.</p>
<p>I think that's a really smart product. Claude Code has an enormous amount of value that hasn't yet been unlocked for a general audience, and this seems like a pragmatic approach.</p>

<h4 id="the-ever-present-threat-of-prompt-injection">The ever-present threat of prompt injection</h4>
<p>With a feature like this, my first thought always jumps straight to security. How big is the risk that someone using this might be hit by hidden malicious instruction somewhere that break their computer or steal their data?</p>
<p>Anthropic touch on that directly in the announcement:</p>
<blockquote>
<p>You should also be aware of the risk of "<a href="https://www.anthropic.com/research/prompt-injection-defenses">prompt injections</a>": attempts by attackers to alter Claude's plans through content it might encounter on the internet. We've built sophisticated defenses against prompt injections, but agent safety---that is, the task of securing Claude's real-world actions---is still an active area of development in the industry.</p>
<p>These risks aren't new with Cowork, but it might be the first time you're using a more advanced tool that moves beyond a simple conversation. We recommend taking precautions, particularly while you learn how it works. We provide more detail in our <a href="https://support.claude.com/en/articles/13364135-using-cowork-safely">Help Center</a>.</p>
</blockquote>
<p>That help page includes the following tips:</p>
<blockquote>
<p>To minimize risks:</p>
<ul>
<li>Avoid granting access to local files with sensitive information, like financial documents.</li>
<li>When using the Claude in Chrome extension, limit access to trusted sites.</li>
<li>If you chose to extend Claude‚Äôs default internet access settings, be careful to only extend internet access to sites you trust.</li>
<li>Monitor Claude for suspicious actions that may indicate prompt injection.</li>
</ul>
</blockquote>
<p>I do not think it is fair to tell regular non-programmer users to watch out for "suspicious actions that may indicate prompt injection"!</p>
<p>I'm sure they have some impressive mitigations going on behind the scenes. I recently learned that the summarization applied by the WebFetch function in Claude Code and now in Cowork is partly intended as a prompt injection protection layer via <a href="https://x.com/bcherny/status/1989025306980860226">this tweet</a> from Claude Code creator Boris Cherny:</p>
<blockquote>
<p>Summarization is one thing we do to reduce prompt injection risk. Are you running into specific issues with it?</p>
</blockquote>
<p>But Anthropic are being honest here with their warnings: they can attempt to filter out potential attacks all they like but the one thing they can't provide is guarantees that no future attack will be found that sneaks through their defenses and steals your data (see <a href="https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/">the lethal trifecta</a> for more on this.)</p>
<p>The problem with prompt injection remains that until there's a high profile incident it's really hard to get people to take it seriously. I myself have all sorts of Claude Code usage that could cause havoc if a malicious injection got in. Cowork does at least run in a filesystem sandbox by default, which is more than can be said for my <code>claude --dangerously-skip-permissions</code> habit!</p>
<p>I wrote more about this in my 2025 round-up: <a href="https://simonwillison.net/2025/Dec/31/the-year-in-llms/#the-year-of-yolo-and-the-normalization-of-deviance">The year of YOLO and the Normalization of Deviance</a>.</p>
<h4 id="this-is-still-a-strong-signal-of-the-future">This is still a strong signal of the future</h4>
<p>Security worries aside, Cowork represents something really interesting. This is a general agent that looks well positioned to bring the wildly powerful capabilities of Claude Code to a wider audience.</p>
<p>I would be very surprised if Gemini and OpenAI don't follow suit with their own offerings in this category.</p>
<p>I imagine OpenAI are already regretting burning the name "ChatGPT Agent" on their janky, experimental and mostly forgotten browser automation tool <a href="https://simonwillison.net/2025/Aug/4/chatgpt-agents-user-agent/">back in August</a>!</p>
<h4 id="bonus-and-a-silly-logo">Bonus: and a silly logo</h4>
<p>bashtoni <a href="https://news.ycombinator.com/item?id=46593022#46593553">on Hacker News</a>:</p>
<blockquote>
<p>Simple suggestion: logo should be a cow and and orc to match how I originally read the product name.</p>
</blockquote>
<p>I couldn't resist <a href="https://gist.github.com/simonw/d06dec3d62dee28f2bd993eb78beb2ce">throwing that one at Nano Banana</a>:</p>
<p><img src="https://static.simonwillison.net/static/2026/cow-ork.jpg" alt="An anthropic style logo with a cow and an ork on it" style="max-width: 100%;" /></p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/sandboxing">sandboxing</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/prompt-injection">prompt-injection</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/ai-agents">ai-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/lethal-trifecta">lethal-trifecta</a>, <a href="https://simonwillison.net/tags/claude-cowork">claude-cowork</a></p>]]></description><pubDate>Mon, 12 Jan 2026 21:46:13 +0000</pubDate></item><item><title>Don&apos;t fall into the anti-AI hype</title><link>https://simonwillison.net/2026/Jan/11/dont-fall-into-the-anti-ai-hype/#atom-everything</link><description><![CDATA[<p><strong><a href="https://antirez.com/news/158">Don&#x27;t fall into the anti-AI hype</a></strong></p>
I'm glad someone was brave enough to say this. There is a <em>lot</em> of anti-AI sentiment in the software development community these days. Much of it is justified, but if you let people convince you that AI isn't genuinely useful for software developers or that this whole thing will blow over soon it's becoming clear that you're taking on a very real risk to your future career.</p>
<p>As Salvatore Sanfilippo puts it:</p>
<blockquote>
<p>It does not matter if AI companies will not be able to get their money back and the stock market will crash. All that is irrelevant, in the long run. It does not matter if this or the other CEO of some unicorn is telling you something that is off putting, or absurd. Programming changed forever, anyway.</p>
</blockquote>
<p>I do like this hopeful positive outlook on what this could all mean, emphasis mine:</p>
<blockquote>
<p>How do I feel, about all the code I wrote that was ingested by LLMs? I feel great to be part of that, because I see this as a continuation of what I tried to do all my life: democratizing code, systems, knowledge. <strong>LLMs are going to help us to write better software, faster, and will allow small teams to have a chance to compete with bigger companies</strong>. The same thing open source software did in the 90s.</p>
</blockquote>
<p>This post has been the subject of heated discussions all day today on both <a href="https://news.ycombinator.com/item?id=46574276">Hacker News</a> and <a href="https://lobste.rs/s/cmsfbu/don_t_fall_into_anti_ai_hype">Lobste.rs</a>.


    <p>Tags: <a href="https://simonwillison.net/tags/salvatore-sanfilippo">salvatore-sanfilippo</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a></p>]]></description><pubDate>Sun, 11 Jan 2026 23:58:43 +0000</pubDate></item><item><title>My answers to the questions I posed about porting open source code with LLMs</title><link>https://simonwillison.net/2026/Jan/11/answers/#atom-everything</link><description><![CDATA[<p>Last month I <a href="https://simonwillison.net/2025/Dec/15/porting-justhtml/">wrote about porting JustHTML from Python to JavaScript</a> using Codex CLI and GPT-5.2 in a few hours while also buying a Christmas tree and watching Knives Out 3. I ended that post with a series of open questions about the ethics and legality of this style of work. Alexander Petros on <a href="https://lobste.rs/">lobste.rs</a> just <a href="https://lobste.rs/s/cmsfbu/don_t_fall_into_anti_ai_hype#c_cqkdve">challenged me to answer them</a>, which is fair enough! Here's my attempt at that.</p>
<p>You can read <a href="https://simonwillison.net/2025/Dec/15/porting-justhtml/">the original post</a> for background, but the short version is that it's now possible to point a coding agent at some other open source project and effectively tell it "port this to language X and make sure the tests still pass" and have it do exactly that.</p>
<p>Here are the questions I posed along with my answers based on my current thinking. Extra context is that I've since tried variations on a similar theme a few more times using Claude Code and Opus 4.5 and found it to be <em>astonishingly</em> effective.</p>
<h4 id="does-this-library-represent-a-legal-violation-of-copyright-of-either-the-rust-library-or-the-python-one">Does this library represent a legal violation of copyright of either the Rust library or the Python one?</h4>
<p>I decided that the right thing to do here was to <a href="https://github.com/simonw/justjshtml/commit/a415d0af40c34bf9a856e956d841513f482867e3">keep the open source license and copyright statement</a> from the Python library author and treat what I had built as a derivative work, which is the entire <em>point</em> of open source.</p>
<h4 id="even-if-this-is-legal-is-it-ethical-to-build-a-library-in-this-way">Even if this is legal, is it ethical to build a library in this way?</h4>
<p>After sitting on this for a while I've come down on yes, provided full credit is given and the license is carefully considered. Open source allows and encourages further derivative works! I never got upset at some university student forking one of my projects on GitHub and hacking in a new feature that they used. I don't think this is materially different, although a port to another language entirely does feel like a slightly different shape.</p>
<h4 id="does-this-format-of-development-hurt-the-open-source-ecosystem">Does this format of development hurt the open source ecosystem?</h4>
<p>Now this one is complicated!</p>
<p>It definitely hurts some projects because there are open source maintainers out there who say things like "I'm not going to release any open source code any more because I don't want it used for training" - I expect some of those would be equally angered by LLM-driven derived works as well.</p>
<p>I don't know how serious this problem is - I've seen angry comments from anonymous usernames, but do they represent genuine open source contributions or are they just angry anonymous usernames?</p>
<p>If we assume this is real, does the loss of those individuals get balanced out by the increase in individuals who CAN contribute to open source because they can now get work done in a few hours that might previously have taken them a few days that they didn't have to spare?</p>
<p>I'll be brutally honest about that question: I think that if "they might train on my code / build a derived version with an LLM" is enough to drive you away from open source, your open source values are distinct enough from mine that I'm not ready to invest significantly in keeping you. I'll put that effort into welcoming the newcomers instead.</p>
<p>The much bigger concern for me is the impact of generative AI on <em>demand</em> for open source. The recent <a href="https://simonwillison.net/2026/Jan/7/adam-wathan/">Tailwind story</a> is a visible example of this - while Tailwind blamed LLMs for reduced traffic to their documentation resulting in fewer conversions to their paid component library, I'm suspicious that the reduced demand there is because LLMs make building good-enough versions of those components for free easy enough that people do that instead.</p>
<p>I've found myself affected by this for open source dependencies too. The other day I wanted to parse a cron expression in some Go code. Usually I'd go looking for an existing library for cron expression parsing - but this time I hardly thought about that for a second before prompting one (complete with extensive tests) into existence instead.</p>
<p>I expect that this is going to quite radically impact the shape of the open source library world over the next few years. Is that "harmful to open source"? It may well be. I'm hoping that whatever new shape comes out of this has its own merits, but I don't know what those would be.</p>
<h4 id="can-i-even-assert-copyright-over-this-given-how-much-of-the-work-was-produced-by-the-llm">Can I even assert copyright over this, given how much of the work was produced by the LLM?</h4>
<p>I'm not a lawyer so I don't feel credible to comment on this one. My loose hunch is that I'm still putting enough creative control in through the way I direct the models for that to count as enough human intervention, at least under US law, but I have no idea.</p>
<h4 id="is-it-responsible-to-publish-software-libraries-built-in-this-way">Is it responsible to publish software libraries built in this way?</h4>
<p>I've come down on "yes" here, again because I never thought it was irresponsible for some random university student to slap an Apache license on some bad code they just coughed up on GitHub.</p>
<p>What's important here is making it very clear to potential users what they should expect from that software. I've started publishing my AI-generated and not 100% reviewed libraries as alphas, which I'm tentatively thinking of as <strong>"alpha slop"</strong>. I'll take the alpha label off once I've used them in production to the point that I'm willing to stake my reputation on them being decent implementations, and I'll ship a 1.0 version when I'm confident that they are a solid bet for other people to depend on. I think that's the responsible way to handle this.</p>
<h4 id="how-much-better-would-this-library-be-if-an-expert-team-hand-crafted-it-over-the-course-of-several-months">How much better would this library be if an expert team hand crafted it over the course of several months?</h4>
<p>That one was a deliberately provocative question, because for a new HTML5 parsing library that passes 9,200 tests you would need a <em>very</em> good reason to hire an expert team for two months (at a cost of hundreds of thousands of dollars) to write such a thing. And honestly, thanks to the existing conformance suites this kind of library is simple enough that you may find their results weren't notably better than the one written by the coding agent.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/definitions">definitions</a>, <a href="https://simonwillison.net/tags/open-source">open-source</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/conformance-suites">conformance-suites</a></p>]]></description><pubDate>Sun, 11 Jan 2026 22:59:23 +0000</pubDate></item><item><title>TIL from taking Neon I at the Crucible</title><link>https://simonwillison.net/2026/Jan/11/neon-i-at-the-crucible/#atom-everything</link><description><![CDATA[<p><strong><a href="https://til.simonwillison.net/neon/neon-1">TIL from taking Neon I at the Crucible</a></strong></p>
Things I learned about making neon signs after a week long intensive evening class at <a href="https://www.thecrucible.org/">the Crucible</a> in Oakland.


    <p>Tags: <a href="https://simonwillison.net/tags/art">art</a>, <a href="https://simonwillison.net/tags/til">til</a></p>]]></description><pubDate>Sun, 11 Jan 2026 17:35:57 +0000</pubDate></item><item><title>Quoting Linus Torvalds</title><link>https://simonwillison.net/2026/Jan/11/linus-torvalds/#atom-everything</link><description><![CDATA[<blockquote cite="https://github.com/torvalds/AudioNoise/blob/71b256a7fcb0aa1250625f79838ab71b2b77b9ff/README.md"><p>Also note that the python visualizer tool has been basically written by vibe-coding. I know more about analog filters -- and that's not saying much -- than I do about python. It started out as my typical "google and do the monkey-see-monkey-do" kind of programming, but then I cut out the middle-man -- me -- and just used Google Antigravity to do the audio sample visualizer.</p></blockquote>
<p class="cite">&mdash; <a href="https://github.com/torvalds/AudioNoise/blob/71b256a7fcb0aa1250625f79838ab71b2b77b9ff/README.md">Linus Torvalds</a>, Another silly guitar-pedal-related repo</p>

    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a>, <a href="https://simonwillison.net/tags/linus-torvalds">linus-torvalds</a>, <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a></p>]]></description><pubDate>Sun, 11 Jan 2026 02:29:58 +0000</pubDate></item><item><title>A Software Library with No Code</title><link>https://simonwillison.net/2026/Jan/10/a-software-library-with-no-code/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.dbreunig.com/2026/01/08/a-software-library-with-no-code.html">A Software Library with No Code</a></strong></p>
Provocative experiment from Drew Breunig, who designed a new library for time formatting ("3 hours ago" kind of thing) called "whenwords" that has no code at all, just a carefully written specification, an AGENTS.md and a collection of conformance tests in a YAML file.</p>
<p>Pass that to your coding agent of choice, tell it what language you need and it will write it for you on demand!</p>
<p>This meshes nearly with my recent <a href="https://simonwillison.net/2025/Dec/31/the-year-in-llms/#the-year-of-conformance-suites">interest in conformance suites</a>. If you publish good enough language-independent tests it's pretty astonishing how far today's coding agents can take you!


    <p>Tags: <a href="https://simonwillison.net/tags/testing">testing</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/drew-breunig">drew-breunig</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/conformance-suites">conformance-suites</a></p>]]></description><pubDate>Sat, 10 Jan 2026 23:41:58 +0000</pubDate></item><item><title>Fly&apos;s new Sprites.dev addresses both developer sandboxes and API sandboxes at the same time</title><link>https://simonwillison.net/2026/Jan/9/sprites-dev/#atom-everything</link><description><![CDATA[<p>New from Fly.io today: <a href="https://sprites.dev">Sprites.dev</a>. Here's their <a href="https://fly.io/blog/code-and-let-live/">blog post</a> and <a href="https://www.youtube.com/watch?v=7BfTLlwO4hw">YouTube demo</a>. It's an interesting new product that's quite difficult to explain - Fly call it "Stateful sandbox environments with checkpoint &amp; restore" but I see it as hitting two of my current favorite problems: a safe development environment for running coding agents <em>and</em> an API for running untrusted code in a secure sandbox.</p>
<p><em>Disclosure: Fly sponsor some of my work. They did not ask me to write about Sprites and I didn't get preview access prior to the launch. My enthusiasm here is genuine.</em></p>

<ul>
  <li><a href="https://simonwillison.net/2026/Jan/9/sprites-dev/#developer-sandboxes">Developer sandboxes</a></li>
  <li><a href="https://simonwillison.net/2026/Jan/9/sprites-dev/#storage-and-checkpoints">Storage and checkpoints</a></li>
  <li><a href="https://simonwillison.net/2026/Jan/9/sprites-dev/#really-clever-use-of-claude-skills">Really clever use of Claude Skills</a></li>
  <li><a href="https://simonwillison.net/2026/Jan/9/sprites-dev/#a-sandbox-api">A sandbox API</a></li>
  <li><a href="https://simonwillison.net/2026/Jan/9/sprites-dev/#scale-to-zero-billing">Scale-to-zero billing</a></li>
  <li><a href="https://simonwillison.net/2026/Jan/9/sprites-dev/#two-of-my-favorite-problems-at-once">Two of my favorite problems at once</a></li>
</ul>

<h4 id="developer-sandboxes">Developer sandboxes</h4>
<p>I predicted earlier this week that <a href="https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-a-challenger-disaster-for-coding-agent-security">"we‚Äôre due a Challenger disaster with respect to coding agent security"</a> due to the terrifying way most of us are using coding agents like Claude Code and Codex CLI. Running them in <code>--dangerously-skip-permissions</code> mode (aka YOLO mode, where the agent acts without constantly seeking approval first) unlocks so much more power, but also means that a mistake or a malicious prompt injection can cause all sorts of damage to your system and data.</p>
<p>The safe way to run YOLO mode is in a robust sandbox, where the worst thing that can happen is the sandbox gets messed up and you have to throw it away and get another one.</p>
<p>That's the first problem Sprites solves:</p>
<div class="highlight highlight-source-shell"><pre>curl https://sprites.dev/install.sh <span class="pl-k">|</span> bash

sprite login
sprite create my-dev-environment
sprite console -s my-dev-environment</pre></div>
<p>That's all it takes to get SSH connected to a fresh environment, running in an ~8GB RAM, 8 CPU server. And... Claude Code and Codex and Gemini CLI and Python 3.13 and Node.js 22.20 and a bunch of other tools are already installed.</p>
<p>The first time you run <code>claude</code> it neatly signs you in to your existing account with Anthropic. The Sprites VM is persistent so future runs of <code>sprite console -s</code> will get you back to where you were before.</p>
<p>... and it automatically sets up port forwarding, so you can run a localhost server on your Sprite and access it from <code>localhost:8080</code> on your machine.</p>
<p>There's also a command you can run to assign a public URL to your Sprite, so anyone else can access it if they know the secret URL.</p>
<h4 id="storage-and-checkpoints">Storage and checkpoints</h4>
<p>In <a href="https://fly.io/blog/code-and-let-live/">the blog post</a> Kurt Mackey argues that ephemeral, disposable sandboxes are not the best fit for coding agents:</p>
<blockquote>
<p>The state of the art in agent isolation is a read-only sandbox. At Fly.io, we‚Äôve been selling that story for years, and we‚Äôre calling it: ephemeral sandboxes are obsolete. Stop killing your sandboxes every time you use them. [...]</p>
<p>If you force an agent to, it‚Äôll work around containerization and do work . But you‚Äôre not helping the agent in any way by doing that. They don‚Äôt want containers. They don‚Äôt want ‚Äúsandboxes‚Äù. They want computers.</p>
<p>[...] with an actual computer, Claude doesn‚Äôt have to rebuild my entire development environment every time I pick up a PR.</p>
</blockquote>
<p>Each Sprite gets a proper filesystem which persists in between sessions, even while the Sprite itself shuts down after inactivity. It sounds like they're doing some clever filesystem tricks here, I'm looking forward to learning more about those in the future.</p>
<p>There are some clues on <a href="https://sprites.dev/">the homepage</a>:</p>
<blockquote>
<p>You read and write to fast, directly attached NVMe storage. Your data then gets written to durable, external object storage. [...]</p>
<p>You don't pay for allocated filesystem space, just the blocks you write. And it's all TRIM friendly, so your bill goes down when you delete things.</p>
</blockquote>
<p>The really clever feature is checkpoints. You (or your coding agent) can trigger a checkpoint which takes around 300ms. This captures the entire disk state and can then be rolled back to later.</p>
<p>For more on how that works, run this in a Sprite:</p>
<pre><code>cat /.sprite/docs/agent-context.md
</code></pre>
<p>Here's the relevant section:</p>
<pre><code>## Checkpoints
- Point-in-time checkpoints and restores available
- Copy-on-write implementation for storage efficiency
- Last 5 checkpoints mounted at `/.sprite/checkpoints`
- Checkpoints capture only the writable overlay, not the base image
</code></pre>
<p>Or run this to see the <code>--help</code> for the command used to manage them:</p>
<div class="highlight highlight-source-shell"><pre>sprite-env checkpoints --help</pre></div>
<p>Which looks like this:</p>
<pre><code>sprite-env checkpoints - Manage environment checkpoints

USAGE:
    sprite-env checkpoints &lt;subcommand&gt; [options]

SUBCOMMANDS:
    list [--history &lt;ver&gt;]  List all checkpoints (optionally filter by history version)
    get &lt;id&gt;                Get checkpoint details (e.g., v0, v1, v2)
    create                  Create a new checkpoint (auto-versioned)
    restore &lt;id&gt;            Restore from a checkpoint (e.g., v1)

NOTE:
    Checkpoints are versioned as v0, v1, v2, etc.
    Restore returns immediately and triggers an async restore that restarts the environment.
    The last 5 checkpoints are mounted at /.sprite/checkpoints for direct file access.

EXAMPLES:
    sprite-env checkpoints list
    sprite-env checkpoints list --history v1.2.3
    sprite-env checkpoints get v2
    sprite-env checkpoints create
    sprite-env checkpoints restore v1
</code></pre>
<h4 id="really-clever-use-of-claude-skills">Really clever use of Claude Skills</h4>
<p>I'm <a href="https://simonwillison.net/2025/Oct/16/claude-skills/">a big fan of Skills</a>, the mechanism whereby Claude Code (and increasingly other agents too) can be given additional capabilities by describing them in Markdown files in a specific directory structure.</p>
<p>In a smart piece of design, Sprites uses pre-installed skills to teach Claude how Sprites itself works. This means you can ask Claude on the machine how to do things like open up ports and it will talk you through the process.</p>
<p>There's all sorts of interesting stuff in the <code>/.sprite</code> folder on that machine - digging in there is a great way to learn more about how Sprites works.</p>
<h4 id="a-sandbox-api">A sandbox API</h4>
<p>Also from my predictions post earlier this week: <a href="https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-we-re-finally-going-to-solve-sandboxing">"We‚Äôre finally going to solve sandboxing"</a>. I am obsessed with this problem: I want to be able to run untrusted code safely, both on my personal devices and in the context of web services I'm building for other people to use.</p>
<p>I have <em>so many things</em> I want to build that depend on being able to take untrusted code - from users or from LLMs or from LLMs-driven-by-users - and run that code in a sandbox where I can be confident that the blast radius if something goes wrong is tightly contained.</p>
<p>Sprites offers a clean <a href="https://sprites.dev/api">JSON API</a> for doing exactly that, plus client libraries in <a href="https://github.com/superfly/sprites-go">Go</a> and <a href="https://github.com/superfly/sprites-js">TypeScript</a> and coming-soon <a href="https://github.com/superfly/sprites-py">Python</a> and <a href="https://github.com/superfly/sprites-ex">Elixir</a>.</p>
<p>From their quick start:</p>
<pre><code># Create a new sprite
curl -X PUT https://api.sprites.dev/v1/sprites/my-sprite \
-H "Authorization: Bearer $SPRITES_TOKEN"

# Execute a command
curl -X POST https://api.sprites.dev/v1/sprites/my-sprite/exec \
-H "Authorization: Bearer $SPRITES_TOKEN" \
-d '{"command": "echo hello"}'
</code></pre>
<p>You can also checkpoint and rollback via the API, so you can get your environment exactly how you like it, checkpoint it, run a bunch of untrusted code, then roll back to the clean checkpoint when you're done.</p>
<p>Managing network access is an important part of maintaining a good sandbox. The Sprites API lets you <a href="https://sprites.dev/api/sprites/policies">configure network access policies</a> using a DNS-based allow/deny list like this:</p>
<div class="highlight highlight-source-shell"><pre>curl -X POST \
  <span class="pl-s"><span class="pl-pds">"</span>https://api.sprites.dev/v1/sprites/{name}/policy/network<span class="pl-pds">"</span></span> \
  -H <span class="pl-s"><span class="pl-pds">"</span>Authorization: Bearer <span class="pl-smi">$SPRITES_TOKEN</span><span class="pl-pds">"</span></span> \
  -H <span class="pl-s"><span class="pl-pds">"</span>Content-Type: application/json<span class="pl-pds">"</span></span> \
  -d <span class="pl-s"><span class="pl-pds">'</span>{</span>
<span class="pl-s">    "rules": [</span>
<span class="pl-s">      {</span>
<span class="pl-s">        "action": "allow",</span>
<span class="pl-s">        "domain": "github.com"</span>
<span class="pl-s">      },</span>
<span class="pl-s">      {</span>
<span class="pl-s">        "action": "allow",</span>
<span class="pl-s">        "domain": "*.npmjs.org"</span>
<span class="pl-s">      }</span>
<span class="pl-s">    ]</span>
<span class="pl-s">  }<span class="pl-pds">'</span></span></pre></div>

<h4 id="scale-to-zero-billing">Scale-to-zero billing</h4>
<p>Sprites have scale-to-zero baked into the architecture. They go to sleep after 30 seconds of inactivity, wake up quickly when needed and bill you for just the CPU hours, RAM hours and GB-hours of storage you use while the Sprite is awake.</p>
<p>Fly <a href="https://sprites.dev/#billing">estimate</a> a 4 hour intensive coding session as costing around 46 cents, and a low traffic web app with 30 hours of wake time per month at ~$4.</p>
<p>(I calculate that a web app that consumes all 8 CPUs and all 8GBs of RAM 24/7 for a month would cost ((7 cents * 8 * 24 * 30) + (4.375 cents * 8 * 24 * 30)) / 100  = $655.2 per month, so don't necessarily use these as your primary web hosting solution for an app that soaks up all available CPU and RAM!)</p>
<h4 id="two-of-my-favorite-problems-at-once">Two of my favorite problems at once</h4>
<p>I was hopeful that Fly would enter the developer-friendly sandbox API market, especially given other entrants from companies like <a href="https://sandbox.cloudflare.com/">Cloudflare</a> and <a href="https://modal.com/docs/guide/sandboxes">Modal</a> and <a href="https://e2b.dev/">E2B</a>.</p>
<p>I did not expect that they'd tackle the developer sandbox problem at the same time, and with the same product!</p>
<p>My one concern here is that it makes the product itself a little harder to explain.</p>
<p>I'm already spinning up some prototypes of sandbox-adjacent things I've always wanted to build, and early signs are very promising. I'll write more about these as they turn into useful projects.</p>


<p><strong>Update</strong>: Here's some <a href="https://news.ycombinator.com/item?id=46557825#46560748">additional colour</a> from Thomas Ptacek on Hacker News:</p>
<blockquote>
<p>This has been in the works for quite awhile here. We put a long bet on "slow create fast start/stop" --- which is a really interesting and useful shape for execution environments --- but it didn't make sense to sandboxers, so "fast create" has been the White Whale at Fly.io for over a year.</p>
</blockquote>
    
        <p>Tags: <a href="https://simonwillison.net/tags/sandboxing">sandboxing</a>, <a href="https://simonwillison.net/tags/thomas-ptacek">thomas-ptacek</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/fly">fly</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a></p>]]></description><pubDate>Fri, 9 Jan 2026 23:57:12 +0000</pubDate></item><item><title>LLM predictions for 2026, shared with Oxide and Friends</title><link>https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#atom-everything</link><description><![CDATA[<p>I joined a recording of the Oxide and Friends podcast on Tuesday to talk about 1, 3 and 6 year predictions for the tech industry. This is my second appearance on their annual predictions episode, you can see <a href="https://simonwillison.net/2025/Jan/10/ai-predictions/">my predictions from January 2025 here</a>. Here's <a href="https://oxide-and-friends.transistor.fm/episodes/predictions-2026">the page for this year's episode</a>, with options to listen in all of your favorite podcast apps or <a href="https://www.youtube.com/watch?v=lVDhQMiAbR8">directly on YouTube</a>.</p>
<p>Bryan Cantrill started the episode by declaring that he's never been so unsure about what's coming in the next year. I share that uncertainty - the significant advances in coding agents just in the last two months have left me certain that things will change significantly, but unclear as to what those changes will be.</p>
<p>Here are the predictions I shared in the episode.</p>
<ul>
  <li><a href="https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-it-will-become-undeniable-that-llms-write-good-code">1 year: It will become undeniable that LLMs write good code</a></li>
  <li><a href="https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-we-re-finally-going-to-solve-sandboxing">1 year: We're finally going to solve sandboxing</a></li>
  <li><a href="https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-a-challenger-disaster-for-coding-agent-security">1 year: A "Challenger disaster" for coding agent security</a></li>
  <li><a href="https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-k-k-p-parrots-will-have-an-outstanding-breeding-season">1 year: KƒÅkƒÅp≈ç parrots will have an outstanding breeding season</a></li>
  <li><a href="https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#3-years-the-coding-agents-jevons-paradox-for-software-engineering-will-resolve-one-way-or-the-other">3 years: the coding agents Jevons paradox for software engineering will resolve, one way or the other</a></li>
  <li><a href="https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#3-years-someone-will-build-a-new-browser-using-mainly-ai-assisted-coding-and-it-won-t-even-be-a-surprise">3 years: Someone will build a new browser using mainly AI-assisted coding and it won't even be a surprise</a></li>
  <li><a href="https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#6-years-typing-code-by-hand-will-go-the-way-of-punch-cards">6 years: Typing code by hand will go the way of punch cards</a></li>
</ul>
<h4 id="1-year-it-will-become-undeniable-that-llms-write-good-code">1 year: It will become undeniable that LLMs write good code <a href="https://www.youtube.com/watch?v=lVDhQMiAbR8&amp;t=1167s" class="predictions-video-link">‚ñ∂ 19:27</a></h4>
<blockquote>
<p>I think that there are still people out there who are convinced that LLMs cannot write good code. Those people are in for a very nasty shock in 2026. I do not think it will be possible to get to the end of even the next three months while still holding on to that idea that the code they write is all junk and it's it's likely any decent human programmer will write better code than they will.</p>
</blockquote>
<p>In 2023, saying that LLMs write garbage code was entirely correct. For most of 2024 that stayed true. In 2025 that changed, but you could be forgiven for continuing to hold out. In 2026 the quality of LLM-generated code will become impossible to deny.</p>
<p>I base this on my own experience - I've spent more time exploring <a href="https://simonwillison.net/tags/ai-assisted-programming/">AI-assisted programming</a> than most.</p>
<p>The key change in 2025 (see <a href="https://simonwillison.net/2025/Dec/31/the-year-in-llms/#the-year-of-reasoning-">my overview for the year</a>) was the introduction of "reasoning models" trained specifically against code using Reinforcement Learning. The major labs spent a full year competing with each other on who could get the best code capabilities from their models, and that problem turns out to be perfectly attuned to RL since code challenges come with built-in verifiable success conditions.</p>
<p>Since Claude Opus 4.5 and GPT-5.2 came out in November and December respectively the amount of code I've written by hand has dropped to a single digit percentage of my overall output. The same is true for many other expert programmers I know.</p>
<p>At this point if you continue to argue that LLMs write useless code you're damaging your own credibility.</p>
<h4 id="1-year-we-re-finally-going-to-solve-sandboxing">1 year: We're finally going to solve sandboxing <a href="https://www.youtube.com/watch?v=lVDhQMiAbR8&amp;t=1205s" class="predictions-video-link">‚ñ∂ 20:05</a></h4>
<blockquote>
<p>I think this year is the year we're going to solve sandboxing. I want to run code other people have written on my computing devices without it destroying my computing devices if it's malicious or has bugs. [...] It's crazy that it's 2026 and I still <code>pip install</code> random code and then execute it in a way that it can steal all of my data and delete all my files. [...] I don't want to run a piece of code on any of my devices that somebody else wrote outside of sandbox ever again.</p>
</blockquote>
<p>This isn't just about LLMs, but it becomes even more important now there are so many more people writing code often without knowing what they're doing. Sandboxing is also a key part of the battle against prompt injection.</p>
<p>We have a <em>lot</em> of promising technologies in play already for this - containers and WebAssembly being the two I'm most optimistic about. There's real commercial value involved in solving this problem. The pieces are there, what's needed is UX work to reduce the friction in using them productively and securely.</p>
<h4 id="1-year-a-challenger-disaster-for-coding-agent-security">1 year: A "Challenger disaster" for coding agent security <a href="https://www.youtube.com/watch?v=lVDhQMiAbR8&amp;t=1281s" class="predictions-video-link">‚ñ∂ 21:21</a></h4>
<blockquote>
<p>I think we're due a Challenger disaster with respect to coding agent security[...] I think so many people, myself included, are running these coding agents practically as root, right? We're letting them do all of this stuff. And every time I do it, my computer doesn't get wiped. I'm like, "oh, it's fine".</p>
</blockquote>
<p>I used this as an opportunity to promote my favourite recent essay about AI security, <a href="https://embracethered.com/blog/posts/2025/the-normalization-of-deviance-in-ai/">the Normalization of Deviance in AI</a> by Johann Rehberger.</p>
<p>The Normalization of Deviance describes the phenomenon where people and organizations get used to operating in an unsafe manner because nothing bad has happened to them yet, which can result in enormous problems (like the 1986 Challenger disaster) when their luck runs out.</p>
<p>Every six months I predict that a headline-grabbing prompt injection attack is coming soon, and every six months it doesn't happen. This is my most recent version of that prediction!</p>
<h4 id="1-year-k-k-p-parrots-will-have-an-outstanding-breeding-season">1 year: KƒÅkƒÅp≈ç parrots will have an outstanding breeding season <a href="https://www.youtube.com/watch?v=lVDhQMiAbR8&amp;t=3006s" class="predictions-video-link">‚ñ∂ 50:06</a></h4>

<p>(I dropped this one to lighten the mood after a discussion of the deep sense of existential dread that many programmers are feeling right now!)</p>
<blockquote>
<p>I think that KƒÅkƒÅp≈ç parrots in New Zealand are going to have an outstanding breeding season. The reason I think this is that the Rimu trees are in fruit right now. There's only 250 of them,  and they only breed if the Rimu trees have a good fruiting. The Rimu trees have been terrible since 2019, but this year the Rimu trees were all blooming. There are researchers saying that all 87 females of breeding age might lay an egg. And for a species with only 250 remaining parrots that's great news.</p>
</blockquote>
<p>(I just <a href="https://en.wikipedia.org/wiki/K%C4%81k%C4%81p%C5%8D#Population_timeline">checked Wikipedia</a> and I was right with the parrot numbers but wrong about the last good breeding season, apparently 2022 was a good year too.)</p>
<p>In a year with precious little in the form of good news I am utterly delighted to share this story. Here's more:</p>
<ul>
<li>
<a href="https://blog.doc.govt.nz/2025/06/27/kakapo-breeding-season-2026/">KƒÅkƒÅp≈ç breeding season 2026</a> introduction from the Department of Conservation from June 2025 .</li>
<li>
<a href="https://www.auckland.ac.nz/en/news/2025/12/03/bumper-breeding-season-for-kakapo-on-the-cards.html">Bumper breeding season for kƒÅkƒÅp≈ç on the cards</a> - 3rd December 2025, University of Auckland.</li>
</ul>
<p>I don't often use AI-generated images on this blog, but the KƒÅkƒÅp≈ç image the Oxide team created for this episode is just <em>perfect</em>:</p>
<p><img src="https://static.simonwillison.net/static/2026/oxide-kakapo.jpg" alt="A beautiful green KƒÅkƒÅp≈ç surrounded by candles gazes into a crystal ball" style="max-width: 100%;" /></p>

<h4 id="3-years-the-coding-agents-jevons-paradox-for-software-engineering-will-resolve-one-way-or-the-other">3 years: the coding agents Jevons paradox for software engineering will resolve, one way or the other <a href="https://www.youtube.com/watch?v=lVDhQMiAbR8&amp;t=3277s" class="predictions-video-link">‚ñ∂ 54:37</a></h4>

<blockquote>
<p>We will find out if the <a href="https://en.wikipedia.org/wiki/Jevons_paradox">Jevons paradox</a> saves our careers or not. This is a big question that anyone who's a software engineer has right now: we are driving the cost of actually producing working code down to a fraction of what it used to cost. Does that mean that our careers are completely devalued and we all have to learn to live on a tenth of our incomes, or does it mean that the demand for software, for custom software goes up by a factor of 10 and now our skills are even <em>more</em> valuable because you can hire me and I can build you 10 times the software I used to be able to? I think by three years we will know for sure which way that one went.</p>
</blockquote>
<p>The quote says it all. There are two ways this coding agents thing could go: it could turn out software engineering skills are devalued, or it could turn out we're more valuable and effective than ever before.</p>
<p>I'm crossing my fingers for the latter! So far it feels to me like it's working out that way.</p>

<h4 id="3-years-someone-will-build-a-new-browser-using-mainly-ai-assisted-coding-and-it-won-t-even-be-a-surprise">3 years: Someone will build a new browser using mainly AI-assisted coding and it won't even be a surprise <a href="https://www.youtube.com/watch?v=lVDhQMiAbR8&amp;t=3913s" class="predictions-video-link">‚ñ∂ 65:13</a></h4>

<blockquote>
<p>I think somebody will have built a full web browser mostly using AI assistance, and it won't even be surprising. Rolling a new web browser is one of the most complicated software projects I can imagine[...] the cheat code is the conformance suites. If there are existing tests that it'll get so much easier.</p>
</blockquote>
<p>A common complaint today from AI coding skeptics is that LLMs are fine for toy projects but can't be used for anything large and serious.</p>
<p>I think within 3 years that will be comprehensively proven incorrect, to the point that it won't even be controversial anymore.</p>
<p>I picked a web browser here because so much of the work building a browser involves writing code that has to conform to an enormous and daunting selection of both formal tests and informal websites-in-the-wild.</p>
<p>Coding agents are <em>really good</em> at tasks where you can define a concrete goal and then set them to work iterating in that direction.</p>
<p>A web browser is the most ambitious project I can think of that leans into those capabilities.</p>

<h4 id="6-years-typing-code-by-hand-will-go-the-way-of-punch-cards">6 years: Typing code by hand will go the way of punch cards <a href="https://www.youtube.com/watch?v=lVDhQMiAbR8&amp;t=4839s" class="predictions-video-link">‚ñ∂ 80:39</a></h4>

<blockquote>
<p>I think the job of being paid money to type code into a computer will go the same way as punching punch cards [...] in six years time, I do not think anyone will be paid to just to do the thing where you type the code. I think software engineering will still be an enormous career. I just think the software engineers won't be spending multiple hours of their day in a text editor typing out syntax.</p>
</blockquote>
<p>The more time I spend on AI-assisted programming the less afraid I am for my job, because it turns out building software - especially at the rate it's now possible to build - still requires enormous skill, experience and depth of understanding.</p>
<p>The skills are changing though! Being able to read a detailed specification and transform it into lines of code is the thing that's being automated away. What's left is everything else, and the more time I spend working with coding agents the larger that "everything else" becomes.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/predictions">predictions</a>, <a href="https://simonwillison.net/tags/sandboxing">sandboxing</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/kakapo">kakapo</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/oxide">oxide</a>, <a href="https://simonwillison.net/tags/bryan-cantrill">bryan-cantrill</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/conformance-suites">conformance-suites</a></p>]]></description><pubDate>Thu, 8 Jan 2026 19:42:13 +0000</pubDate></item><item><title>How Google Got Its Groove Back and Edged Ahead of OpenAI</title><link>https://simonwillison.net/2026/Jan/8/how-google-got-its-groove-back/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.wsj.com/tech/ai/google-ai-openai-gemini-chatgpt-b766e160">How Google Got Its Groove Back and Edged Ahead of OpenAI</a></strong></p>
I picked up a few interesting tidbits from this Wall Street Journal piece on Google's recent hard won success with Gemini.</p>
<p>Here's the origin of the name "Nano Banana":</p>
<blockquote>
<p>Naina Raisinghani, known inside Google for working late into the night, needed a name for the new tool to complete the upload. It was 2:30 a.m., though, and nobody was around. So she just made one up, a mashup of two nicknames friends had given her: Nano Banana.</p>
</blockquote>
<p>The WSJ credit OpenAI's Daniel Selsam with un-retiring Sergei Brin:</p>
<blockquote>
<p>Around that time, Google co-founder Sergey Brin, who had recently retired, was at a party chatting with a researcher from OpenAI named Daniel Selsam, according to people familiar with the conversation. Why, Selsam asked him, wasn‚Äôt he working full time on AI. Hadn‚Äôt the launch of ChatGPT captured his imagination as a computer scientist?</p>
<p>ChatGPT was on its way to becoming a household name in AI chatbots, while Google was still fumbling to get its product off the ground. Brin decided Selsam had a point and returned to work.</p>
</blockquote>
<p>And we get some rare concrete user numbers:</p>
<blockquote>
<p>By October, Gemini had more than 650 million monthly users, up from 450 million in July.</p>
</blockquote>
<p>The LLM usage number I see cited most often is OpenAI's 800 million weekly active users for ChatGPT. That's from October 6th at OpenAI DevDay so it's comparable to these Gemini numbers, albeit not directly since it's weekly rather than monthly actives.</p>
<p>I'm also never sure what counts as a "Gemini user" - does interacting via Google Docs or Gmail count or do you need to be using a Gemini chat interface directly?</p>
<p><strong>Update 17th January 2025</strong>: <a href="https://twitter.com/lunixa380/status/2012610344741412909">@LunixA380 pointed out</a> that this 650m user figure comes from the <a href="https://abc.xyz/investor/news/news-details/2025/Alphabet-Announces-Third-Quarter-2025-Results-2025-mIRgD3AI4A/default.aspx">Alphabet 2025 Q3 earnings report</a> which says this (emphasis mine):</p>
<blockquote>
<p>"Alphabet had a terrific quarter, with double-digit growth across every major part of our business. We delivered our first-ever $100 billion quarter," said Sundar Pichai, CEO of Alphabet and Google.</p>
<p>"[...] In addition to topping leaderboards, our first party models, like Gemini, now process 7 billion tokens per minute, via direct API use by our customers. <strong>The Gemini App now has over 650 million monthly active users</strong>.</p>
</blockquote>
<p>Presumably the "Gemini App" encompasses the Android and iPhone apps as well as direct visits to <a href="https://gemini.google.com/">gemini.google.com</a> - that seems to be the indication from Google's <a href="https://blog.google/products-and-platforms/products/gemini/gemini-3/">November 18th blog post</a> that also mentioned the 650m number.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=46528389">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/google">google</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/gemini">gemini</a>, <a href="https://simonwillison.net/tags/nano-banana">nano-banana</a></p>]]></description><pubDate>Thu, 8 Jan 2026 15:32:08 +0000</pubDate></item><item><title>Quoting Adam Wathan</title><link>https://simonwillison.net/2026/Jan/7/adam-wathan/#atom-everything</link><description><![CDATA[<blockquote cite="https://github.com/tailwindlabs/tailwindcss.com/pull/2388#issuecomment-3717222957"><p>[...] the reality is that 75% of the people on our engineering team lost their jobs here yesterday because of the brutal impact AI has had on our business. And every second I spend trying to do fun free things for the community like this is a second I'm not spending trying to turn the business around and make sure the people who are still here are getting their paychecks every month. [...]</p>
<p>Traffic to our docs is down about 40% from early 2023 despite Tailwind being more popular than ever. The docs are the only way people find out about our commercial products, and without customers we can't afford to maintain the framework. [...]</p>
<p>Tailwind is growing faster than it ever has and is bigger than it ever has been, and our revenue is down close to 80%. Right now there's just no correlation between making Tailwind easier to use and making development of the framework more sustainable.</p></blockquote>
<p class="cite">&mdash; <a href="https://github.com/tailwindlabs/tailwindcss.com/pull/2388#issuecomment-3717222957">Adam Wathan</a>, CEO, Tailwind Labs</p>

    <p>Tags: <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/css">css</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/open-source">open-source</a></p>]]></description><pubDate>Wed, 7 Jan 2026 17:29:29 +0000</pubDate></item><item><title>Quoting Robin Sloan</title><link>https://simonwillison.net/2026/Jan/7/robin-sloan/#atom-everything</link><description><![CDATA[<blockquote cite="https://www.robinsloan.com/winter-garden/agi-is-here/"><p><strong>AGI is here</strong>!‚ÄÇWhen exactly it arrived, we‚Äôll never know; whether it was one company‚Äôs Pro or another company‚Äôs Pro Max (Eddie Bauer Edition) that tip-toed first across the line‚Äâ‚Ä¶‚Äâyou may debate.‚ÄÇBut generality has been achieved, &amp; now we can proceed to new questions. [...]</p>
<p>The key word in Artificial General Intelligence is General.‚ÄÇThat‚Äôs the word that makes this AI unlike every other AI: because every other AI was trained for a particular purpose.‚ÄÇConsider landmark models across the decades: the Mark I¬†Perceptron, LeNet, AlexNet, AlphaGo, AlphaFold‚Äâ‚Ä¶‚Äâthese systems were all different, but all alike in this way.</p>
<p>Language models were trained for a purpose, too‚Äâ‚Ä¶‚Äâbut, surprise: the mechanism &amp; scale of that training did something new: opened a wormhole, through which a vast field of action &amp; response could be reached.‚ÄÇTowering libraries of human writing, drawn together across time &amp; space, all the dumb reasons for it‚Äâ‚Ä¶‚Äâthat‚Äôs rich fuel, if you can hold it all in your head.</p></blockquote>
<p class="cite">&mdash; <a href="https://www.robinsloan.com/winter-garden/agi-is-here/">Robin Sloan</a>, AGI is here (and I feel fine)</p>

    <p>Tags: <a href="https://simonwillison.net/tags/robin-sloan">robin-sloan</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a></p>]]></description><pubDate>Wed, 7 Jan 2026 00:54:41 +0000</pubDate></item><item><title>A field guide to sandboxes for AI</title><link>https://simonwillison.net/2026/Jan/6/a-field-guide-to-sandboxes-for-ai/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.luiscardoso.dev/blog/sandboxes-for-ai">A field guide to sandboxes for AI</a></strong></p>
This guide to the current sandboxing landscape by Luis Cardoso is comprehensive, dense and absolutely fantastic.</p>
<p>He starts by differentiating between containers (which share the host kernel), microVMs (their own guest kernel behind hardwae virtualization), gVisor userspace kernels and WebAssembly/isolates that constrain everything within a runtime.</p>
<p>The piece then dives deep into terminology, approaches and the landscape of existing tools.</p>
<p>I think using the right sandboxes to safely run untrusted code  is one of the most important problems to solve in 2026. This guide is an invaluable starting point.

    <p><small></small>Via <a href="https://lobste.rs/s/l9gkjo/field_guide_sandboxes_for_ai">lobste.rs</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/sandboxing">sandboxing</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Tue, 6 Jan 2026 22:38:00 +0000</pubDate></item><item><title>It‚Äôs hard to justify Tahoe icons</title><link>https://simonwillison.net/2026/Jan/5/its-hard-to-justify-tahoe-icons/#atom-everything</link><description><![CDATA[<p><strong><a href="https://tonsky.me/blog/tahoe-icons/">It‚Äôs hard to justify Tahoe icons</a></strong></p>
Devastating critique of the new menu icons in macOS Tahoe by Nikita Prokopov, who starts by quoting the 1992 Apple HIG rule to not "overload the user with complex icons" and then provides comprehensive evidence of Tahoe doing exactly that.</p>
<blockquote>
<p>In my opinion, Apple took on an impossible task: to add an icon to every menu item. There are just not enough good metaphors to do something like that.</p>
<p>But even if there were, the premise itself is questionable: if everything has an icon, it doesn‚Äôt mean users will find what they are looking for faster.</p>
<p>And even if the premise was solid, I still wish I could say: they did the best they could, given the goal. But that‚Äôs not true either: they did a poor job consistently applying the metaphors and designing the icons themselves.</p>
</blockquote>

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=46497712">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/apple">apple</a>, <a href="https://simonwillison.net/tags/design">design</a>, <a href="https://simonwillison.net/tags/macos">macos</a>, <a href="https://simonwillison.net/tags/usability">usability</a></p>]]></description><pubDate>Mon, 5 Jan 2026 19:30:24 +0000</pubDate></item><item><title>Oxide and Friends Predictions 2026, today at 4pm PT</title><link>https://simonwillison.net/2026/Jan/5/oxide-and-friends-predictions-2026/#atom-everything</link><description><![CDATA[<p><strong><a href="https://discord.com/invite/QrcKGTTPrF">Oxide and Friends Predictions 2026, today at 4pm PT</a></strong></p>
I joined the Oxide and Friends podcast <a href="https://simonwillison.net/2025/Jan/10/ai-predictions/">last year</a> to predict the next 1, 3 and 6 years(!) of AI developments. With hindsight I did very badly, but they're inviting me back again anyway to have another go.</p>
<p>We will be recording live today at 4pm Pacific on their Discord - <a href="https://discord.com/invite/QrcKGTTPrF">you can join that here</a>, and the podcast version will go out shortly afterwards.</p>
<p>I'll be recording at their office in Emeryville and then heading to <a href="https://www.thecrucible.org/">the Crucible</a> to learn how to make neon signs.

    <p><small></small>Via <a href="https://bsky.app/profile/bcantrill.bsky.social/post/3mbovdf3h3s24">Bryan Cantrill</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/podcasts">podcasts</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/oxide">oxide</a></p>]]></description><pubDate>Mon, 5 Jan 2026 16:53:05 +0000</pubDate></item><item><title>The November 2025 inflection point</title><link>https://simonwillison.net/2026/Jan/4/inflection/#atom-everything</link><description><![CDATA[<p>It genuinely feels to me like GPT-5.2 and Opus 4.5 in November represent an inflection point - one of those moments where the models get incrementally better in a way that tips across an invisible capability line where suddenly a whole bunch of much harder coding problems open up.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/gpt-5">gpt-5</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/claude-4">claude-4</a></p>]]></description><pubDate>Sun, 4 Jan 2026 23:21:42 +0000</pubDate></item></channel></rss>