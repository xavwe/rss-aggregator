<?xml version="1.0" encoding="utf-8"?><rss version="2.0"><channel><title>Simon Willison&apos;s Weblog</title><link>https://raw.githubusercontent.com/xavwe/rss-aggregator/refs/heads/main/feeds/simon-willison-s-weblog-2b081550.xml</link><description>Archived feed from https://simonwillison.net/atom/everything</description><item><title>Quoting Thariq Shihipar</title><link>https://simonwillison.net/2026/Feb/20/thariq-shihipar/#atom-everything</link><description><![CDATA[<blockquote cite="https://twitter.com/trq212/status/2024574133011673516"><p>Long running agentic products like Claude Code are made feasible by prompt caching which allows us to reuse computation from previous roundtrips and significantly decrease latency and cost. [...]</p>
<p>At Claude Code, we build our entire harness around prompt caching. A high prompt cache hit rate decreases costs and helps us create more generous rate limits for our subscription plans, so we run alerts on our prompt cache hit rate and declare SEVs if they're too low.</p></blockquote>
<p class="cite">&mdash; <a href="https://twitter.com/trq212/status/2024574133011673516">Thariq Shihipar</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/prompt-engineering">prompt-engineering</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/ai-agents">ai-agents</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Fri, 20 Feb 2026 07:13:19 +0000</pubDate></item><item><title>Gemini 3.1 Pro</title><link>https://simonwillison.net/2026/Feb/19/gemini-31-pro/#atom-everything</link><description><![CDATA[<p><strong><a href="https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-1-pro/">Gemini 3.1 Pro</a></strong></p>
The first in the Gemini 3.1 series, priced the same as Gemini 3 Pro ($2/million input, $12/million output under 200,000 tokens, $4/$18 for 200,000 to 1,000,000). That's less than half the price of Claude Opus 4.6 with very similar benchmark scores to that model.</p>
<p>They boast about its improved SVG animation performance compared to Gemini 3 Pro in the announcement!</p>
<p>I tried "Generate an SVG of a pelican riding a bicycle" <a href="https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221ugF9fBfLGxnNoe8_rLlluzo9NSPJDWuF%22%5D,%22action%22:%22open%22,%22userId%22:%22106366615678321494423%22,%22resourceKeys%22:%7B%7D%7D&amp;usp=sharing">in Google AI Studio</a> and it thought for 323.9 seconds (<a href="https://gist.github.com/simonw/03a755865021739a3659943a22c125ba#thinking-trace">thinking trace here</a>) before producing this one:</p>
<p><img alt="Whimsical flat-style illustration of a pelican wearing a blue and white baseball cap, riding a red bicycle with yellow-rimmed wheels along a road. The pelican has a large orange bill and a green scarf. A small fish peeks out of a brown basket on the handlebars. The background features a light blue sky with a yellow sun, white clouds, and green hills." src="https://static.simonwillison.net/static/2026/gemini-3.1-pro-pelican.png" /></p>
<p>It's good to see the legs clearly depicted on both sides of the frame (should <a href="https://twitter.com/elonmusk/status/2023833496804839808">satisfy Elon</a>), the fish in the basket is a nice touch and I appreciated this comment in <a href="https://gist.github.com/simonw/03a755865021739a3659943a22c125ba#response">the SVG code</a>:</p>
<pre><code>&lt;!-- Black Flight Feathers on Wing Tip --&gt;
&lt;path d="M 420 175 C 440 182, 460 187, 470 190 C 450 210, 430 208, 410 198 Z" fill="#374151" /&gt;
</code></pre>
<p>I've <a href="https://github.com/simonw/llm-gemini/issues/121">added</a> the two new model IDs <code>gemini-3.1-pro-preview</code> and <code>gemini-3.1-pro-preview-customtools</code> to my <a href="https://github.com/simonw/llm-gemini">llm-gemini plugin</a> for <a href="https://llm.datasette.io/">LLM</a>. That "custom tools" one is <a href="https://ai.google.dev/gemini-api/docs/models/gemini-3.1-pro-preview#gemini-31-pro-preview-customtools">described here</a> - apparently it may provide better tool performance than the default model in some situations.</p>
<p>The model appears to be <em>incredibly</em> slow right now - it took 104s to respond to a simple "hi" and a few of my other tests met "Error: This model is currently experiencing high demand. Spikes in demand are usually temporary. Please try again later." or "Error: Deadline expired before operation could complete" errors. I'm assuming that's just teething problems on launch day.</p>
<p>It sounds like last week's <a href="https://simonwillison.net/2026/Feb/12/gemini-3-deep-think/">Deep Think release</a> was our first exposure to the 3.1 family:</p>
<blockquote>
<p>Last week, we released a major update to Gemini 3 Deep Think to solve modern challenges across science, research and engineering. Today, we’re releasing the upgraded core intelligence that makes those breakthroughs possible: Gemini 3.1 Pro.</p>
</blockquote>
<p><strong>Update</strong>: In <a href="https://simonwillison.net/2025/nov/13/training-for-pelicans-riding-bicycles/">What happens if AI labs train for pelicans riding bicycles?</a> last November I said:</p>
<blockquote>
<p>If a model finally comes out that produces an excellent SVG of a pelican riding a bicycle you can bet I’m going to test it on all manner of creatures riding all sorts of transportation devices.</p>
</blockquote>
<p>Google's Gemini Lead Jeff Dean <a href="https://x.com/JeffDean/status/2024525132266688757">tweeted this video</a> featuring an animated pelican riding a bicycle, plus a frog on a penny-farthing and a giraffe driving a tiny car and an ostrich on roller skates and a turtle kickflipping a skateboard and a dachshund driving a stretch limousine.</p>
<video style="margin-bottom: 1em" poster="https://static.simonwillison.net/static/2026/gemini-animated-pelicans.jpg" muted controls preload="none" style="max-width: 100%">
  <source src="https://static.simonwillison.net/static/2026/gemini-animated-pelicans.mp4" type="video/mp4">
</video>

<p>I've been saying for a while that I wish AI labs would highlight things that their new models can do that their older models could not, so top marks to the Gemini team for this video.</p>
<p><strong>Update 2</strong>: I used <code>llm-gemini</code> to run my <a href="https://simonwillison.net/2025/Nov/18/gemini-3/#and-a-new-pelican-benchmark">more detailed Pelican prompt</a>, with <a href="https://gist.github.com/simonw/a3bdd4ec9476ba9e9ba7aa61b46d8296">this result</a>:</p>
<p><img alt="Flat-style illustration of a brown pelican riding a teal bicycle with dark blue-rimmed wheels against a plain white background. Unlike the previous image's white cartoon pelican, this pelican has realistic brown plumage with detailed feather patterns, a dark maroon head, yellow eye, and a large pink-tinged pouch bill. The bicycle is a simpler design without a basket, and the scene lacks the colorful background elements like the sun, clouds, road, hills, cap, and scarf from the first illustration, giving it a more minimalist feel." src="https://static.simonwillison.net/static/2026/gemini-3.1-pro-pelican-2.png" /></p>
<p>From the SVG comments:</p>
<pre><code>&lt;!-- Pouch Gradient (Breeding Plumage: Red to Olive/Green) --&gt;
...
&lt;!-- Neck Gradient (Breeding Plumage: Chestnut Nape, White/Yellow Front) --&gt;
</code></pre>


    <p>Tags: <a href="https://simonwillison.net/tags/google">google</a>, <a href="https://simonwillison.net/tags/svg">svg</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/llm">llm</a>, <a href="https://simonwillison.net/tags/gemini">gemini</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a></p>]]></description><pubDate>Thu, 19 Feb 2026 17:58:37 +0000</pubDate></item><item><title>Experimenting with sponsorship for my blog and newsletter</title><link>https://simonwillison.net/2026/Feb/19/sponsorship/#atom-everything</link><description><![CDATA[<p>I've long been resistant to the idea of accepting sponsorship for my blog. I value my credibility as an independent voice, and I don't want to risk compromising that reputation.</p>
<p>Then I learned about Troy Hunt's <a href="https://www.troyhunt.com/sponsorship/">approach to sponsorship</a>, which he first wrote about <a href="https://www.troyhunt.com/im-now-offering-sponsorship-of-this-blog/">in 2016</a>. Troy runs with a simple text row in the page banner - no JavaScript, no cookies, unobtrusive while providing value to the sponsor. I can live with that!</p>
<p>Accepting sponsorship in this way helps me maintain my independence while offsetting the opportunity cost of not taking a full-time job.</p>
<p>To start with I'm selling sponsorship by the week. Sponsors get that unobtrusive banner across my blog and also their sponsored message at the top of <a href="https://simonw.substack.com/">my newsletter</a>.</p>
<p><img alt="Screenshot of my blog's homepage. Below the Simon Willison's Weblog heading and list of tags is a new blue page-wide banner reading &quot;Sponsored by: Teleport - Secure, Govern, and Operate Al at Engineering Scale. Learn more&quot;." src="https://static.simonwillison.net/static/2026/sponsor-banner.jpg" /></p>
<p>I <strong>will not write content in exchange for sponsorship</strong>. I hope the sponsors I work with understand that my credibility as an independent voice is a key reason I have an audience, and compromising that trust would be bad for everyone.</p>
<p><a href="https://www.freemanandforrest.com/">Freeman &amp; Forrest</a> helped me set up and sell my first slots. Thanks also to <a href="https://t3.gg/">Theo Browne</a> for helping me think through my approach.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/newsletter">newsletter</a>, <a href="https://simonwillison.net/tags/blogging">blogging</a>, <a href="https://simonwillison.net/tags/troy-hunt">troy-hunt</a></p>]]></description><pubDate>Thu, 19 Feb 2026 05:44:29 +0000</pubDate></item><item><title>SWE-bench February 2026 leaderboard update</title><link>https://simonwillison.net/2026/Feb/19/swe-bench/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.swebench.com/">SWE-bench February 2026 leaderboard update</a></strong></p>
SWE-bench is one of the benchmarks that the labs love to list in their model releases. The official leaderboard is infrequently updated but they just did a full run of it against the current generation of models, which is notable because it's always good to see benchmark results like this that <em>weren't</em> self-reported by the labs.</p>
<p>The fresh results are for their "Bash Only" benchmark, which runs their <a href="https://github.com/SWE-agent/mini-swe-agent">mini-swe-bench</a> agent (~9,000 lines of Python, <a href="https://github.com/SWE-agent/mini-swe-agent/blob/v2.2.1/src/minisweagent/config/benchmarks/swebench.yaml">here are the prompts</a> they use) against the <a href="https://huggingface.co/datasets/princeton-nlp/SWE-bench">SWE-bench</a> dataset of coding problems - 2,294 real-world examples pulled from 12 open source repos: <a href="https://github.com/django/django">django/django</a> (850), <a href="https://github.com/sympy/sympy">sympy/sympy</a> (386), <a href="https://github.com/scikit-learn/scikit-learn">scikit-learn/scikit-learn</a> (229), <a href="https://github.com/sphinx-doc/sphinx">sphinx-doc/sphinx</a> (187), <a href="https://github.com/matplotlib/matplotlib">matplotlib/matplotlib</a> (184), <a href="https://github.com/pytest-dev/pytest">pytest-dev/pytest</a> (119), <a href="https://github.com/pydata/xarray">pydata/xarray</a> (110), <a href="https://github.com/astropy/astropy">astropy/astropy</a> (95), <a href="https://github.com/pylint-dev/pylint">pylint-dev/pylint</a> (57), <a href="https://github.com/psf/requests">psf/requests</a> (44), <a href="https://github.com/mwaskom/seaborn">mwaskom/seaborn</a> (22), <a href="https://github.com/pallets/flask">pallets/flask</a> (11).</p>
<p><strong>Correction</strong>: <em>The Bash only benchmark runs against SWE-bench Verified, not original SWE-bench. Verified is a manually curated subset of 500 samples <a href="https://openai.com/index/introducing-swe-bench-verified/">described here</a>, funded by OpenAI. Here's <a href="https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified">SWE-bench Verified</a> on Hugging Face - since it's just 2.1MB of Parquet it's easy to browse <a href="https://lite.datasette.io/?parquet=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fprinceton-nlp%2FSWE-bench_Verified%2Fresolve%2Fmain%2Fdata%2Ftest-00000-of-00001.parquet#/data/test-00000-of-00001?_facet=repo">using Datasette Lite</a>, which cuts those numbers down to django/django (231), sympy/sympy (75), sphinx-doc/sphinx (44), matplotlib/matplotlib (34), scikit-learn/scikit-learn (32), astropy/astropy (22), pydata/xarray (22), pytest-dev/pytest (19), pylint-dev/pylint (10), psf/requests (8), mwaskom/seaborn (2), pallets/flask (1)</em>.</p>
<p>Here's how the top ten models performed:</p>
<p><img alt="Bar chart showing &quot;% Resolved&quot; by &quot;Model&quot;. Bars in descending order: Claude 4.5 Opus (high reasoning) 76.8%, Gemini 3 Flash (high reasoning) 75.8%, MiniMax M2.5 (high reasoning) 75.8%, Claude Opus 4.6 75.6%, GLM-5 (high reasoning) 72.8%, GPT-5.2 (high reasoning) 72.8%, Claude 4.5 Sonnet (high reasoning) 72.8%, Kimi K2.5 (high reasoning) 71.4%, DeepSeek V3.2 (high reasoning) 70.8%, Claude 4.5 Haiku (high reasoning) 70.0%, and a partially visible final bar at 66.6%." src="https://static.simonwillison.net/static/2026/swbench-feb-2026.jpg" /></p>
<p>It's interesting to see Claude Opus 4.5 beat Opus 4.6, though only by about a percentage point. 4.5 Opus is top, then Gemini 3 Flash, then MiniMax M2.5 - a 229B model released <a href="https://www.minimax.io/news/minimax-m25">last week</a> by Chinese lab MiniMax. GLM-5, Kimi K2.5 and DeepSeek V3.2 are three more Chinese models that make the top ten as well.</p>
<p>OpenAI's GPT-5.2 is their highest performing model at position 6, but it's worth noting that their best coding model, GPT-5.3-Codex, is not represented - maybe because it's not yet available in the OpenAI API.</p>
<p>This benchmark uses the same system prompt for every model, which is important for a fair comparison but does mean that the quality of the different harnesses or optimized prompts is not being measured here.</p>
<p>The chart above is a screenshot from the SWE-bench website, but their charts don't include the actual percentage values visible on the bars. I successfully used Claude for Chrome to add these - <a href="https://claude.ai/share/81a0c519-c727-4caa-b0d4-0d866375d0da">transcript here</a>. My prompt sequence included:</p>
<blockquote>
<p>Use claude in chrome to open https://www.swebench.com/</p>
<p>Click on "Compare results" and then select "Select top 10"</p>
<p>See those bar charts? I want them to display the percentage on each bar so I can take a better screenshot, modify the page like that</p>
</blockquote>
<p>I'm impressed at how well this worked - Claude injected custom JavaScript into the page to draw additional labels on top of the existing chart.</p>
<p><img alt="Screenshot of a Claude AI conversation showing browser automation. A thinking step reads &quot;Pivoted strategy to avoid recursion issues with chart labeling &gt;&quot; followed by the message &quot;Good, the chart is back. Now let me carefully add the labels using an inline plugin on the chart instance to avoid the recursion issue.&quot; A collapsed &quot;Browser_evaluate&quot; section shows a browser_evaluate tool call with JavaScript code using Chart.js canvas context to draw percentage labels on bars: meta.data.forEach((bar, index) =&gt; { const value = dataset.data[index]; if (value !== undefined &amp;&amp; value !== null) { ctx.save(); ctx.textAlign = 'center'; ctx.textBaseline = 'bottom'; ctx.fillStyle = '#333'; ctx.font = 'bold 12px sans-serif'; ctx.fillText(value.toFixed(1) + '%', bar.x, bar.y - 5); A pending step reads &quot;Let me take a screenshot to see if it worked.&quot; followed by a completed &quot;Done&quot; step, and the message &quot;Let me take a screenshot to check the result.&quot;" src="https://static.simonwillison.net/static/2026/claude-chrome-draw-on-chart.jpg" /></p>
<p><strong>Update</strong>: If you look at the transcript Claude claims to have switched to Playwright, which is confusing because I didn't think I had that configured.

    <p><small></small>Via <a href="https://twitter.com/KLieret/status/2024176335782826336">@KLieret</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/benchmarks">benchmarks</a>, <a href="https://simonwillison.net/tags/django">django</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/ai-in-china">ai-in-china</a>, <a href="https://simonwillison.net/tags/minimax">minimax</a></p>]]></description><pubDate>Thu, 19 Feb 2026 04:48:47 +0000</pubDate></item><item><title>LadybirdBrowser/ladybird: Abandon Swift adoption</title><link>https://simonwillison.net/2026/Feb/19/ladybird/#atom-everything</link><description><![CDATA[<p><strong><a href="https://github.com/LadybirdBrowser/ladybird/commit/e87f889e31afbb5fa32c910603c7f5e781c97afd">LadybirdBrowser/ladybird: Abandon Swift adoption</a></strong></p>
Back <a href="https://simonwillison.net/2024/Aug/11/ladybird-set-to-adopt-swift/">in August 2024</a> the Ladybird browser project announced an intention to adopt Swift as their memory-safe language of choice.</p>
<p>As of <a href="https://github.com/LadybirdBrowser/ladybird/commit/e87f889e31afbb5fa32c910603c7f5e781c97afd">this commit</a> it looks like they've changed their mind:</p>
<blockquote>
<p><strong>Everywhere: Abandon Swift adoption</strong></p>
<p>After making no progress on this for a very long time, let's acknowledge it's not going anywhere and remove it from the codebase.</p>
</blockquote>

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=47067678">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ladybird">ladybird</a>, <a href="https://simonwillison.net/tags/swift">swift</a></p>]]></description><pubDate>Thu, 19 Feb 2026 01:25:33 +0000</pubDate></item><item><title>Typing without having to type</title><link>https://simonwillison.net/2026/Feb/18/typing/#atom-everything</link><description><![CDATA[<p>25+ years into my career as a programmer I think I may <em>finally</em> be coming around to preferring type hints or even strong typing. I resisted those in the past because they slowed down the rate at which I could iterate on code, especially in the REPL environments that were key to my productivity. But if a coding agent is doing all that <em>typing</em> for me, the benefits of explicitly defining all of those types are suddenly much more attractive.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/programming">programming</a>, <a href="https://simonwillison.net/tags/programming-languages">programming-languages</a>, <a href="https://simonwillison.net/tags/static-typing">static-typing</a></p>]]></description><pubDate>Wed, 18 Feb 2026 18:56:56 +0000</pubDate></item><item><title>The A.I. Disruption We’ve Been Waiting for Has Arrived</title><link>https://simonwillison.net/2026/Feb/18/the-ai-disruption/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.nytimes.com/2026/02/18/opinion/ai-software.html?unlocked_article_code=1.NFA.UkLv.r-XczfzYRdXJ&amp;smid=url-share">The A.I. Disruption We’ve Been Waiting for Has Arrived</a></strong></p>
New opinion piece from Paul Ford in the New York Times. Unsurprisingly for a piece by Paul it's packed with quoteworthy snippets, but a few stood out for me in particular.</p>
<p>Paul describes the <a href="https://simonwillison.net/2026/Jan/4/inflection/">November moment</a> that so many other programmers have observed, and highlights Claude Code's ability to revive old side projects:</p>
<blockquote>
<p>[Claude Code] was always a helpful coding assistant, but in November it suddenly got much better, and ever since I’ve been knocking off side projects that had sat in folders for a decade or longer. It’s fun to see old ideas come to life, so I keep a steady flow. Maybe it adds up to a half-hour a day of my time, and an hour of Claude’s.</p>
<p>November was, for me and many others in tech, a great surprise. Before, A.I. coding tools were often useful, but halting and clumsy. Now, the bot can run for a full hour and make whole, designed websites and apps that may be flawed, but credible. I spent an entire session of therapy talking about it.</p>
</blockquote>
<p>And as the former CEO of a respected consultancy firm (Postlight) he's well positioned to evaluate the potential impact:</p>
<blockquote>
<p>When you watch a large language model slice through some horrible, expensive problem — like migrating data from an old platform to a modern one — you feel the earth shifting. I was the chief executive of a software services firm, which made me a professional software cost estimator. When I rebooted my messy personal website a few weeks ago, I realized: I would have paid $25,000 for someone else to do this. When a friend asked me to convert a large, thorny data set, I downloaded it, cleaned it up and made it pretty and easy to explore. In the past I would have charged $350,000.</p>
<p>That last price is full 2021 retail — it implies a product manager, a designer, two engineers (one senior) and four to six months of design, coding and testing. Plus maintenance. Bespoke software is joltingly expensive. Today, though, when the stars align and my prompts work out, I can do hundreds of thousands of dollars worth of work for fun (fun for me) over weekends and evenings, for the price of the Claude $200-a-month plan.</p>
</blockquote>
<p>He also neatly captures the inherent community tension involved in exploring this technology:</p>
<blockquote>
<p>All of the people I love hate this stuff, and all the people I hate love it. And yet, likely because of the same personality flaws that drew me to technology in the first place, I am annoyingly excited.</p>
</blockquote>


    <p>Tags: <a href="https://simonwillison.net/tags/new-york-times">new-york-times</a>, <a href="https://simonwillison.net/tags/paul-ford">paul-ford</a>, <a href="https://simonwillison.net/tags/careers">careers</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a></p>]]></description><pubDate>Wed, 18 Feb 2026 17:07:31 +0000</pubDate></item><item><title>Quoting Martin Fowler</title><link>https://simonwillison.net/2026/Feb/18/martin-fowler/#atom-everything</link><description><![CDATA[<blockquote cite="https://martinfowler.com/fragments/2026-02-18.html"><p>LLMs are eating specialty skills. There will be less use of specialist front-end and back-end developers as the LLM-driving skills become more important than the details of platform usage. Will this lead to a greater recognition of the role of <a href="https://martinfowler.com/articles/expert-generalist.html">Expert Generalists</a>? Or will the ability of LLMs to write lots of code mean they code around the silos rather than eliminating them?</p></blockquote>
<p class="cite">&mdash; <a href="https://martinfowler.com/fragments/2026-02-18.html">Martin Fowler</a>, tidbits from the Thoughtworks Future of Software Development Retreat, <a href="https://news.ycombinator.com/item?id=47062534">via HN</a>)</p>

    <p>Tags: <a href="https://simonwillison.net/tags/martin-fowler">martin-fowler</a>, <a href="https://simonwillison.net/tags/careers">careers</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a></p>]]></description><pubDate>Wed, 18 Feb 2026 16:50:07 +0000</pubDate></item><item><title>Introducing Claude Sonnet 4.6</title><link>https://simonwillison.net/2026/Feb/17/claude-sonnet-46/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.anthropic.com/news/claude-sonnet-4-6">Introducing Claude Sonnet 4.6</a></strong></p>
Sonnet 4.6 is out today, and Anthropic claim it offers similar performance to <a href="https://simonwillison.net/2025/Nov/24/claude-opus/">November's Opus 4.5</a> while maintaining the Sonnet pricing of $3/million input and $15/million output tokens (the Opus models are $5/$25). Here's <a href="https://www-cdn.anthropic.com/78073f739564e986ff3e28522761a7a0b4484f84.pdf">the system card PDF</a>.</p>
<p>Sonnet 4.6 has a "reliable knowledge cutoff" of August 2025, compared to Opus 4.6's May 2025 and Haiku 4.5's February 2025. Both Opus and Sonnet default to 200,000 max input tokens but can stretch to 1 million in beta and at a higher cost.</p>
<p>I just released <a href="https://github.com/simonw/llm-anthropic/releases/tag/0.24">llm-anthropic 0.24</a> with support for both Sonnet 4.6 and Opus 4.6. Claude Code <a href="https://github.com/simonw/llm-anthropic/pull/65">did most of the work</a> - the new models had a fiddly amount of extra details around adaptive thinking and no longer supporting prefixes, as described <a href="https://platform.claude.com/docs/en/about-claude/models/migration-guide">in Anthropic's migration guide</a>.</p>
<p>Here's <a href="https://gist.github.com/simonw/b185576a95e9321b441f0a4dfc0e297c">what I got</a> from:</p>
<pre><code>uvx --with llm-anthropic llm 'Generate an SVG of a pelican riding a bicycle' -m claude-sonnet-4.6
</code></pre>
<p><img alt="The pelican has a jaunty top hat with a red band. There is a string between the upper and lower beaks for some reason. The bicycle frame is warped in the wrong way." src="https://static.simonwillison.net/static/2026/pelican-sonnet-4.6.png" /></p>
<p>The SVG comments include:</p>
<pre><code>&lt;!-- Hat (fun accessory) --&gt;
</code></pre>
<p>I tried a second time and also got a top hat. Sonnet 4.6 apparently loves top hats!</p>
<p>For comparison, here's the pelican Opus 4.5 drew me <a href="(https://simonwillison.net/2025/Nov/24/claude-opus/)">in November</a>:</p>
<p><img alt="The pelican is cute and looks pretty good. The bicycle is not great - the frame is wrong and the pelican is facing backwards when the handlebars appear to be forwards.There is also something that looks a bit like an egg on the handlebars." src="https://static.simonwillison.net/static/2025/claude-opus-4.5-pelican.jpg" /></p>
<p>And here's Anthropic's current best pelican, drawn by Opus 4.6 <a href="https://simonwillison.net/2026/Feb/5/two-new-models/">on February 5th</a>:</p>
<p><img alt="Slightly wonky bicycle frame but an excellent pelican, very clear beak and pouch, nice feathers." src="https://static.simonwillison.net/static/2026/opus-4.6-pelican.png" /></p>
<p>Opus 4.6 produces the best pelican beak/pouch. I do think the top hat from Sonnet 4.6 is a nice touch though.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=47050488">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/llm">llm</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/llm-pricing">llm-pricing</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a></p>]]></description><pubDate>Tue, 17 Feb 2026 23:58:58 +0000</pubDate></item><item><title>Rodney v0.4.0</title><link>https://simonwillison.net/2026/Feb/17/rodney/#atom-everything</link><description><![CDATA[<p><strong><a href="https://github.com/simonw/rodney/releases/tag/v0.4.0">Rodney v0.4.0</a></strong></p>
My <a href="https://github.com/simonw/rodney">Rodney</a> CLI tool for browser automation attracted quite the flurry of PRs since I announced it <a href="https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#rodney-cli-browser-automation-designed-to-work-with-showboat">last week</a>. Here are the release notes for the just-released v0.4.0:</p>
<blockquote>
<ul>
<li>Errors now use exit code 2, which means exit code 1 is just for for check failures. <a href="https://github.com/simonw/rodney/pull/15">#15</a></li>
<li>New <code>rodney assert</code> command for running JavaScript tests, exit code 1 if they fail. <a href="https://github.com/simonw/rodney/issues/19">#19</a></li>
<li>New directory-scoped sessions with <code>--local</code>/<code>--global</code> flags. <a href="https://github.com/simonw/rodney/pull/14">#14</a></li>
<li>New <code>reload --hard</code> and <code>clear-cache</code> commands. <a href="https://github.com/simonw/rodney/pull/17">#17</a></li>
<li>New <code>rodney start --show</code> option to make the browser window visible. Thanks, <a href="https://github.com/antocuni">Antonio Cuni</a>. <a href="https://github.com/simonw/rodney/paull/13">#13</a></li>
<li>New <code>rodney connect PORT</code> command to debug an already-running Chrome instance. Thanks, <a href="https://github.com/pnf">Peter Fraenkel</a>. <a href="https://github.com/simonw/rodney/pull/12">#12</a></li>
<li>New <code>RODNEY_HOME</code> environment variable to support custom state directories. Thanks, <a href="https://github.com/senko">Senko Rašić</a>. <a href="https://github.com/simonw/rodney/pull/11">#11</a></li>
<li>New <code>--insecure</code> flag to ignore certificate errors. Thanks, <a href="https://github.com/zgolus">Jakub Zgoliński</a>. <a href="https://github.com/simonw/rodney/pull/10">#10</a></li>
<li>Windows support: avoid <code>Setsid</code> on Windows via build-tag helpers. Thanks, <a href="https://github.com/adm1neca">adm1neca</a>. <a href="https://github.com/simonw/rodney/pull/18">#18</a></li>
<li>Tests now run on <code>windows-latest</code> and <code>macos-latest</code> in addition to Linux.</li>
</ul>
</blockquote>
<p>I've been using <a href="https://github.com/simonw/showboat">Showboat</a> to create demos of new features - here those are for <a href="https://github.com/simonw/rodney/tree/v0.4.0/notes/assert-command-demo">rodney assert</a>, <a href="https://github.com/simonw/rodney/tree/v0.4.0/notes/clear-cache-demo">rodney reload --hard</a>, <a href="https://github.com/simonw/rodney/tree/v0.4.0/notes/error-codes-demo">rodney exit codes</a>, and <a href="https://github.com/simonw/rodney/tree/v0.4.0/notes/local-sessions-demo">rodney start --local</a>.</p>
<p>The <code>rodney assert</code> command is pretty neat: you can now Rodney to test a web app through multiple steps in a shell script that looks something like this (adapted from <a href="https://github.com/simonw/rodney/blob/v0.4.0/README.md#combining-checks-in-a-shell-script">the README</a>):</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#!</span>/bin/bash</span>
<span class="pl-c1">set</span> -euo pipefail

FAIL=0

<span class="pl-en">check</span>() {
    <span class="pl-k">if</span> <span class="pl-k">!</span> <span class="pl-s"><span class="pl-pds">"</span><span class="pl-smi">$@</span><span class="pl-pds">"</span></span><span class="pl-k">;</span> <span class="pl-k">then</span>
        <span class="pl-c1">echo</span> <span class="pl-s"><span class="pl-pds">"</span>FAIL: <span class="pl-smi">$*</span><span class="pl-pds">"</span></span>
        FAIL=1
    <span class="pl-k">fi</span>
}

rodney start
rodney open <span class="pl-s"><span class="pl-pds">"</span>https://example.com<span class="pl-pds">"</span></span>
rodney waitstable

<span class="pl-c"><span class="pl-c">#</span> Assert elements exist</span>
check rodney exists <span class="pl-s"><span class="pl-pds">"</span>h1<span class="pl-pds">"</span></span>

<span class="pl-c"><span class="pl-c">#</span> Assert key elements are visible</span>
check rodney visible <span class="pl-s"><span class="pl-pds">"</span>h1<span class="pl-pds">"</span></span>
check rodney visible <span class="pl-s"><span class="pl-pds">"</span>#main-content<span class="pl-pds">"</span></span>

<span class="pl-c"><span class="pl-c">#</span> Assert JS expressions</span>
check rodney assert <span class="pl-s"><span class="pl-pds">'</span>document.title<span class="pl-pds">'</span></span> <span class="pl-s"><span class="pl-pds">'</span>Example Domain<span class="pl-pds">'</span></span>
check rodney assert <span class="pl-s"><span class="pl-pds">'</span>document.querySelectorAll("p").length<span class="pl-pds">'</span></span> <span class="pl-s"><span class="pl-pds">'</span>2<span class="pl-pds">'</span></span>

<span class="pl-c"><span class="pl-c">#</span> Assert accessibility requirements</span>
check rodney ax-find --role navigation

rodney stop

<span class="pl-k">if</span> [ <span class="pl-s"><span class="pl-pds">"</span><span class="pl-smi">$FAIL</span><span class="pl-pds">"</span></span> <span class="pl-k">-ne</span> 0 ]<span class="pl-k">;</span> <span class="pl-k">then</span>
    <span class="pl-c1">echo</span> <span class="pl-s"><span class="pl-pds">"</span>Some checks failed<span class="pl-pds">"</span></span>
    <span class="pl-c1">exit</span> 1
<span class="pl-k">fi</span>
<span class="pl-c1">echo</span> <span class="pl-s"><span class="pl-pds">"</span>All checks passed<span class="pl-pds">"</span></span></pre></div>


    <p>Tags: <a href="https://simonwillison.net/tags/browsers">browsers</a>, <a href="https://simonwillison.net/tags/projects">projects</a>, <a href="https://simonwillison.net/tags/testing">testing</a>, <a href="https://simonwillison.net/tags/annotated-release-notes">annotated-release-notes</a>, <a href="https://simonwillison.net/tags/rodney">rodney</a></p>]]></description><pubDate>Tue, 17 Feb 2026 23:02:33 +0000</pubDate></item><item><title>Quoting ROUGH DRAFT 8/2/66</title><link>https://simonwillison.net/2026/Feb/17/rough-draft-8266/#atom-everything</link><description><![CDATA[<blockquote cite="https://www.neatorama.com/2026/02/11/The-Original-Drafts-for-Star-Treks-Opening-Narration/"><p>This is the story of the United Space Ship Enterprise. Assigned a five year patrol of our galaxy, the giant starship visits Earth colonies, regulates commerce, and explores strange new worlds and civilizations. These are its voyages... and its adventures.</p></blockquote>
<p class="cite">&mdash; <a href="https://www.neatorama.com/2026/02/11/The-Original-Drafts-for-Star-Treks-Opening-Narration/">ROUGH DRAFT 8/2/66</a>, before the Star Trek opening narration reached its final form</p>

    <p>Tags: <a href="https://simonwillison.net/tags/screen-writing">screen-writing</a>, <a href="https://simonwillison.net/tags/science-fiction">science-fiction</a></p>]]></description><pubDate>Tue, 17 Feb 2026 14:49:04 +0000</pubDate></item><item><title>First kākāpō chick in four years hatches on Valentine&apos;s Day</title><link>https://simonwillison.net/2026/Feb/17/first-kakapo-chick-in-four-years/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.doc.govt.nz/news/media-releases/2026-media-releases/first-kakapo-chick-in-four-years-hatches-on-valentines-day/">First kākāpō chick in four years hatches on Valentine&#x27;s Day</a></strong></p>
First chick of <a href="https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-k-k-p-parrots-will-have-an-outstanding-breeding-season">the 2026 breeding season</a>!</p>
<blockquote>
<p>Kākāpō Yasmine hatched an egg fostered from kākāpō Tīwhiri on Valentine's Day, bringing the total number of kākāpō to 237 – though it won’t be officially added to the population until it fledges.</p>
</blockquote>
<p>Here's why the egg was fostered:</p>
<blockquote>
<p>"Kākāpō mums typically have the best outcomes when raising a maximum of two chicks. Biological mum Tīwhiri has four fertile eggs this season already, while Yasmine, an experienced foster mum, had no fertile eggs."</p>
</blockquote>
<p>And an <a href="https://bsky.app/profile/digs.bsky.social/post/3mf25glzt2c2b">update from conservation biologist Andrew Digby</a> - a second chick hatched this morning!</p>
<blockquote>
<p>The second #kakapo chick of the #kakapo2026 breeding season hatched this morning: Hine Taumai-A1-2026 on Ako's nest on Te Kākahu. We transferred the egg from Anchor two nights ago. This is Ako's first-ever chick, which is just a few hours old in this video.</p>
</blockquote>
<p>That post <a href="https://bsky.app/profile/digs.bsky.social/post/3mf25glzt2c2b">has a video</a> of mother and chick.</p>
<p><img alt="A beautiful charismatic green Kākāp feeding a little grey chick" src="https://static.simonwillison.net/static/2026/kakapo-plus-chick.jpg" />

    <p><small></small>Via <a href="https://www.metafilter.com/212231/Happy-Valen-Kkp-Tines">MetaFilter</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/kakapo">kakapo</a></p>]]></description><pubDate>Tue, 17 Feb 2026 14:09:43 +0000</pubDate></item><item><title>Quoting Dimitris Papailiopoulos</title><link>https://simonwillison.net/2026/Feb/17/dimitris-papailiopoulos/#atom-everything</link><description><![CDATA[<blockquote cite="https://twitter.com/dimitrispapail/status/2023080289828831349"><p>But the intellectually interesting part for me is something else. <strong>I now have something close to a magic box where I throw in a question and a first answer comes back basically for free, in terms of human effort</strong>. Before this, the way I'd explore a new idea is to either clumsily put something together myself or ask a student to run something short for signal, and if it's there, we’d go deeper. That quick signal step, i.e., finding out if a question has any meat to it, is what I can now do without taking up anyone else's time. It’s now between just me, Claude Code, and a few days of GPU time.</p>
<p>I don’t know what this means for how we do research long term. I don’t think anyone does yet. But <strong>the distance between a question and a first answer just got very small</strong>.</p></blockquote>
<p class="cite">&mdash; <a href="https://twitter.com/dimitrispapail/status/2023080289828831349">Dimitris Papailiopoulos</a>, on running research questions though Claude Code</p>

    <p>Tags: <a href="https://simonwillison.net/tags/research">research</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Tue, 17 Feb 2026 14:04:44 +0000</pubDate></item><item><title>Nano Banana Pro diff to webcomic</title><link>https://simonwillison.net/2026/Feb/17/release-notes-webcomic/#atom-everything</link><description><![CDATA[<p>Given the threat of <a href="https://simonwillison.net/tags/cognitive-debt/">cognitive debt</a> brought on by AI-accelerated software development leading to more projects and less deep understanding of how they work and what they actually do, it's interesting to consider artifacts that might be able to help.</p>
<p>Nathan Baschez <a href="https://twitter.com/nbaschez/status/2023501535343509871">on Twitter</a>:</p>
<blockquote>
<p>my current favorite trick for reducing "cognitive debt" (h/t @simonw
) is to ask the LLM to write two versions of the plan:</p>
<ol>
<li>The version for it (highly technical and detailed)</li>
<li>The version for me (an entertaining essay designed to build my intuition)</li>
</ol>
<p>Works great</p>
</blockquote>
<p>This inspired me to try something new. I generated <a href="https://github.com/simonw/showboat/compare/v0.5.0...v0.6.0.diff">the diff</a> between v0.5.0 and v0.6.0 of my Showboat project - which introduced <a href="https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#showboat-remote-publishing">the remote publishing feature</a> - and dumped that into Nano Banana Pro with the prompt:</p>
<blockquote>
<p>Create a webcomic that explains the new feature as clearly and entertainingly as possible</p>
</blockquote>
<p>Here's <a href="https://gemini.google.com/share/cce6da8e5083">what it produced</a>:</p>
<p><img alt="A six-panel comic strip illustrating a tool called &quot;Showboat&quot; for live-streaming document building. Panel 1, titled &quot;THE OLD WAY: Building docs was a lonely voyage. You finished it all before anyone saw it.&quot;, shows a sad bearded man on a wooden boat labeled &quot;THE LOCALHOST&quot; holding papers and saying &quot;Almost done... then I have to export and email the HTML...&quot;. Panel 2, titled &quot;THE UPGRADE: Just set the environment variable!&quot;, shows the same man excitedly plugging in a device with a speech bubble reading &quot;ENV VAR: SHOWBOAT_REMOTE_URL&quot; and the sound effect &quot;*KA-CHUNK!*&quot;. Panel 3, titled &quot;init establishes the uplink and generates a unique UUID beacon.&quot;, shows the man typing at a keyboard with a terminal reading &quot;$ showboat init 'Live Demo'&quot;, a satellite dish transmitting to a floating label &quot;UUID: 550e84...&quot;, and a monitor reading &quot;WAITING FOR STREAM...&quot;. Panel 4, titled &quot;Every note and exec is instantly beamed to the remote viewer!&quot;, shows the man coding with sound effects &quot;*HAMMER!*&quot;, &quot;ZAP!&quot;, &quot;ZAP!&quot;, &quot;BANG!&quot; as red laser beams shoot from a satellite dish to a remote screen displaying &quot;NOTE: Step 1...&quot; and &quot;SUCCESS&quot;. Panel 5, titled &quot;Even image files are teleported in real-time!&quot;, shows a satellite dish firing a cyan beam with the sound effect &quot;*FOOMP!*&quot; toward a monitor displaying a bar chart. Panel 6, titled &quot;You just build. The audience gets the show live.&quot;, shows the man happily working at his boat while a crowd of cheering people watches a projected screen reading &quot;SHOWBOAT LIVE STREAM: Live Demo&quot;, with a label &quot;UUID: 550e84...&quot; and one person in the foreground eating popcorn." src="https://static.simonwillison.net/static/2026/nano-banana-diff.jpg" /></p>
<p>Good enough to publish with the release notes? I don't think so. I'm sharing it here purely to demonstrate the idea. Creating assets like this as a personal tool for thinking about novel ways to explain a feature feels worth exploring further.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/nano-banana">nano-banana</a>, <a href="https://simonwillison.net/tags/gemini">gemini</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/cognitive-debt">cognitive-debt</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/text-to-image">text-to-image</a>, <a href="https://simonwillison.net/tags/showboat">showboat</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a></p>]]></description><pubDate>Tue, 17 Feb 2026 04:51:58 +0000</pubDate></item><item><title>Qwen3.5: Towards Native Multimodal Agents</title><link>https://simonwillison.net/2026/Feb/17/qwen35/#atom-everything</link><description><![CDATA[<p><strong><a href="https://qwen.ai/blog?id=qwen3.5">Qwen3.5: Towards Native Multimodal Agents</a></strong></p>
Alibaba's Qwen just released the first two models in the Qwen 3.5 series - one open weights, one proprietary. Both are multi-modal for vision input.</p>
<p>The open weight one is a Mixture of Experts model called Qwen3.5-397B-A17B. Interesting to see Qwen call out serving efficiency as a benefit of that architecture:</p>
<blockquote>
<p>Built on an innovative hybrid architecture that fuses linear attention (via Gated Delta Networks) with a sparse mixture-of-experts, the model attains remarkable inference efficiency: although it comprises 397 billion total parameters, just 17 billion are activated per forward pass, optimizing both speed and cost without sacrificing capability.</p>
</blockquote>
<p>It's <a href="https://huggingface.co/Qwen/Qwen3.5-397B-A17B">807GB on Hugging Face</a>, and Unsloth have a <a href="https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF">collection of smaller GGUFs</a> ranging in size from 94.2GB 1-bit to 462GB Q8_K_XL.</p>
<p>I got this <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle/">pelican</a> from the <a href="https://openrouter.ai/qwen/qwen3.5-397b-a17b">OpenRouter hosted model</a> (<a href="https://gist.github.com/simonw/625546cf6b371f9c0040e64492943b82">transcript</a>):</p>
<p><img alt="Pelican is quite good although the neck lacks an outline for some reason. Bicycle is very basic with an incomplete frame" src="https://static.simonwillison.net/static/2026/qwen3.5-397b-a17b.png" /></p>
<p>The proprietary hosted model is called Qwen3.5 Plus 2026-02-15, and is a little confusing. Qwen researcher <a href="https://twitter.com/JustinLin610/status/2023340126479569140">Junyang Lin  says</a>:</p>
<blockquote>
<p>Qwen3-Plus is a hosted API version of 397B. As the model natively supports 256K tokens, Qwen3.5-Plus supports 1M token context length. Additionally it supports search and code interpreter, which you can use on Qwen Chat with Auto mode.</p>
</blockquote>
<p>Here's <a href="https://gist.github.com/simonw/9507dd47483f78dc1195117735273e20">its pelican</a>, which is similar in quality to the open weights model:</p>
<p><img alt="Similar quality pelican. The bicycle is taller and has a better frame shape. They are visually quite similar." src="https://static.simonwillison.net/static/2026/qwen3.5-plus-02-15.png" />


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/vision-llms">vision-llms</a>, <a href="https://simonwillison.net/tags/qwen">qwen</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a>, <a href="https://simonwillison.net/tags/openrouter">openrouter</a>, <a href="https://simonwillison.net/tags/ai-in-china">ai-in-china</a></p>]]></description><pubDate>Tue, 17 Feb 2026 04:30:57 +0000</pubDate></item><item><title>Two new Showboat tools: Chartroom and datasette-showboat</title><link>https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#atom-everything</link><description><![CDATA[<p>I <a href="https://simonwillison.net/2026/Feb/10/showboat-and-rodney/">introduced Showboat</a> a week ago - my CLI tool that helps coding agents create Markdown documents that demonstrate the code that they have created. I've been finding new ways to use it on a daily basis, and I've just released two new tools to help get the best out of the Showboat pattern. <a href="https://github.com/simonw/chartroom">Chartroom</a> is a CLI charting tool that works well with Showboat, and <a href="https://github.com/simonw/datasette-showboat">datasette-showboat</a> lets Showboat's new remote publishing feature incrementally push documents to a Datasette instance.</p>

<ul>
  <li><a href="https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#showboat-remote-publishing">Showboat remote publishing</a></li>
  <li><a href="https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#datasette-showboat">datasette-showboat</a></li>
  <li><a href="https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#chartroom">Chartroom</a></li>
  <li><a href="https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#how-i-built-chartroom">How I built Chartroom</a></li>
  <li><a href="https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#the-burgeoning-showboat-ecosystem">The burgeoning Showboat ecosystem</a></li>
</ul>

<h4 id="showboat-remote-publishing">Showboat remote publishing</h4>
<p>I normally use Showboat in Claude Code for web (see <a href="https://simonwillison.net/2026/Feb/16/rodney-claude-code/">note from this morning</a>). I've used it in several different projects in the past few days, each of them with a prompt that looks something like this:</p>
<blockquote>
<p><code>Use "uvx showboat --help" to perform a very thorough investigation of what happens if you use the Python sqlite-chronicle and sqlite-history-json libraries against the same SQLite database table</code></p>
</blockquote>
<p>Here's <a href="https://github.com/simonw/research/blob/main/sqlite-chronicle-vs-history-json/demo.md">the resulting document</a>.</p>
<p>Just telling Claude Code to run <code>uvx showboat --help</code> is enough for it to learn how to use the tool - the <a href="https://github.com/simonw/showboat/blob/main/help.txt">help text</a> is designed to work as a sort of ad-hoc Skill document.</p>
<p>The one catch with this approach is that I can't <em>see</em> the new Showboat document until it's finished. I have to wait for Claude to commit the document plus embedded screenshots and push that to a branch in my GitHub repo - then I can view it through the GitHub interface.</p>
<p>For a while I've been thinking it would be neat to have a remote web server of my own which Claude instances can submit updates to while they are working. Then this morning I realized Showboat might be the ideal mechanism to set that up...</p>
<p>Showboat <a href="https://github.com/simonw/showboat/releases/tag/v0.6.0">v0.6.0</a> adds a new "remote" feature. It's almost invisible to users of the tool itself, instead being configured by an environment variable.</p>
<p>Set a variable like this:</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-k">export</span> SHOWBOAT_REMOTE_URL=https://www.example.com/submit<span class="pl-k">?</span>token=xyz</pre></div>
<p>And every time you run a <code>showboat init</code> or <code>showboat note</code> or <code>showboat exec</code> or <code>showboat image</code> command the resulting document fragments will be POSTed to that API endpoint, in addition to the Showboat Markdown file itself being updated.</p>
<p>There are <a href="https://github.com/simonw/showboat/blob/v0.6.0/README.md#remote-document-streaming">full details in the Showboat README</a> - it's a very simple API format, using regular POST form variables or a multipart form upload for the image attached to <code>showboat image</code>.</p>
<h4 id="datasette-showboat">datasette-showboat</h4>
<p>It's simple enough to build a webapp to receive these updates from Showboat, but I needed one that I could easily deploy and would work well with the rest of my personal ecosystem.</p>
<p>So I had Claude Code write me a Datasette plugin that could act as a Showboat remote endpoint. I actually had this building at the same time as the Showboat remote feature, a neat example of running <a href="https://simonwillison.net/2025/Oct/5/parallel-coding-agents/">parallel agents</a>.</p>
<p><strong><a href="https://github.com/simonw/datasette-showboat">datasette-showboat</a></strong> is a Datasette plugin that adds a <code>/-/showboat</code> endpoint to Datasette for viewing documents and a <code>/-/showboat/receive</code> endpoint for receiving updates from Showboat.</p>
<p>Here's a very quick way to try it out:</p>
<div class="highlight highlight-source-shell"><pre>uvx --with datasette-showboat --prerelease=allow \
  datasette showboat.db --create \
  -s plugins.datasette-showboat.database showboat \
  -s plugins.datasette-showboat.token secret123 \
  --root --secret cookie-secret-123</pre></div>
<p>Click on the sign in as root link that shows up in the console, then navigate to <a href="http://127.0.0.1:8001/-/showboat">http://127.0.0.1:8001/-/showboat</a> to see the interface.</p>
<p>Now set your environment variable to point to this instance:</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-k">export</span> SHOWBOAT_REMOTE_URL=<span class="pl-s"><span class="pl-pds">"</span>http://127.0.0.1:8001/-/showboat/receive?token=secret123<span class="pl-pds">"</span></span></pre></div>
<p>And run Showboat like this:</p>
<div class="highlight highlight-source-shell"><pre>uvx showboat init demo.md <span class="pl-s"><span class="pl-pds">"</span>Showboat Feature Demo<span class="pl-pds">"</span></span></pre></div>
<p>Refresh that page and you should see this:</p>
<p><img src="https://static.simonwillison.net/static/2026/datasette-showboat-documents.jpg" alt="Title: Showboat. Remote viewer for Showboat documents. Showboat Feature Demo 2026-02-17 00:06 · 6 chunks, UUID. To send showboat output to this server, set the SHOWBOAT_REMOTE_URL environment variable: export SHOWBOAT_REMOTE_URL=&quot;http://127.0.0.1:8001/-/showboat/receive?token=your-token&quot;" style="max-width: 100%;" /></p>
<p>Click through to the document, then start Claude Code or Codex or your agent of choice and prompt:</p>
<blockquote>
<p><code>Run 'uvx showboat --help' and then use showboat to add to the existing demo.md document with notes and exec and image to demonstrate the tool - fetch a placekitten for the image demo.</code></p>
</blockquote>
<p>The <code>init</code> command assigns a UUID and title and sends those up to Datasette.</p>
<p><img src="https://static.simonwillison.net/static/2026/datasette-showboat.gif" alt="Animated demo - in the foreground a terminal window runs Claude Code, which executes various Showboat commands. In the background a Firefox window where the Showboat Feature Demo adds notes then some bash commands, then a placekitten image." style="max-width: 100%;" /></p>
<p>The best part of this is that it works in Claude Code for web. Run the plugin on a server somewhere (an exercise left up to the reader - I use <a href="https://fly.io/">Fly.io</a> to host mine) and set that <code>SHOWBOAT_REMOTE_URL</code> environment variable in your Claude environment, then any time you tell it to use Showboat the document it creates will be transmitted to your server and viewable in real time.</p>
<p>I built <a href="https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#rodney-cli-browser-automation-designed-to-work-with-showboat">Rodney</a>, a CLI browser automation tool, specifically to work with Showboat. It makes it easy to have a Showboat document load up web pages, interact with them via clicks or injected JavaScript and captures screenshots to embed in the Showboat document and show the effects.</p>
<p>This is wildly useful for hacking on web interfaces using Claude Code for web, especially when coupled with the new remote publishing feature. I only got this stuff working this morning and I've already had several sessions where Claude Code has published screenshots of its work in progress, which I've then been able to provide feedback on directly in the Claude session while it's still working.</p>
<h3 id="chartroom">Chartroom</h3>
<p>A few days ago I had another idea for a way to extend the Showboat ecosystem: what if Showboat documents could easily include charts?</p>
<p>I sometimes fire up Claude Code for data analysis tasks, often telling it to download a SQLite database and then run queries against it to figure out interesting things from the data.</p>
<p>With a simple CLI tool that produced PNG images I could have Claude use Showboat to build a document with embedded charts to help illustrate its findings.</p>
<p><strong><a href="https://github.com/simonw/chartroom">Chartroom</a></strong> is exactly that. It's effectively a thin wrapper around the excellent <a href="https://matplotlib.org/">matplotlib</a> Python library, designed to be used by coding agents to create charts that can be embedded in Showboat documents.</p>
<p>Here's how to render a simple bar chart:</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-c1">echo</span> <span class="pl-s"><span class="pl-pds">'</span>name,value</span>
<span class="pl-s">Alice,42</span>
<span class="pl-s">Bob,28</span>
<span class="pl-s">Charlie,35</span>
<span class="pl-s">Diana,51</span>
<span class="pl-s">Eve,19<span class="pl-pds">'</span></span> <span class="pl-k">|</span> uvx chartroom bar --csv \
  --title <span class="pl-s"><span class="pl-pds">'</span>Sales by Person<span class="pl-pds">'</span></span> --ylabel <span class="pl-s"><span class="pl-pds">'</span>Sales<span class="pl-pds">'</span></span></pre></div>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/simonw/chartroom/8812afc02e1310e9eddbb56508b06005ff2c0ed5/demo/1f6851ec-2026-02-14.png"><img src="https://raw.githubusercontent.com/simonw/chartroom/8812afc02e1310e9eddbb56508b06005ff2c0ed5/demo/1f6851ec-2026-02-14.png" alt="A chart of those numbers, with a title and y-axis label" style="max-width: 100%;" /></a></p>
<p>It can also do line charts, bar charts, scatter charts, and histograms - as seen in <a href="https://github.com/simonw/chartroom/blob/0.2.1/demo/README.md">this demo document</a> that was built using Showboat.</p>
<p>Chartroom can also generate alt text. If you add <code>-f alt</code> to the above it will output the alt text for the chart instead of the image:</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-c1">echo</span> <span class="pl-s"><span class="pl-pds">'</span>name,value</span>
<span class="pl-s">Alice,42</span>
<span class="pl-s">Bob,28</span>
<span class="pl-s">Charlie,35</span>
<span class="pl-s">Diana,51</span>
<span class="pl-s">Eve,19<span class="pl-pds">'</span></span> <span class="pl-k">|</span> uvx chartroom bar --csv \
  --title <span class="pl-s"><span class="pl-pds">'</span>Sales by Person<span class="pl-pds">'</span></span> --ylabel <span class="pl-s"><span class="pl-pds">'</span>Sales<span class="pl-pds">'</span></span> -f alt</pre></div>
<p>Outputs:</p>
<pre><code>Sales by Person. Bar chart of value by name — Alice: 42, Bob: 28, Charlie: 35, Diana: 51, Eve: 19
</code></pre>
<p>Or you can use <code>-f html</code> or <code>-f markdown</code> to get the image tag with alt text directly:</p>
<div class="highlight highlight-text-md"><pre><span class="pl-s">![</span>Sales by Person. Bar chart of value by name — Alice: 42, Bob: 28, Charlie: 35, Diana: 51, Eve: 19<span class="pl-s">]</span><span class="pl-s">(</span><span class="pl-corl">/Users/simon/chart-7.png</span><span class="pl-s">)</span></pre></div>
<p>I added support for Markdown images with alt text to Showboat in <a href="https://github.com/simonw/showboat/releases/tag/v0.5.0">v0.5.0</a>, to complement this feature of Chartroom.</p>
<p>Finally, Chartroom has support for different <a href="https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html">matplotlib styles</a>. I had Claude build a Showboat document to demonstrate these all in one place - you can see that at <a href="https://github.com/simonw/chartroom/blob/main/demo/styles.md">demo/styles.md</a>.</p>
<h4 id="how-i-built-chartroom">How I built Chartroom</h4>
<p>I started the Chartroom repository with my <a href="https://github.com/simonw/click-app">click-app</a> cookiecutter template, then told a fresh Claude Code for web session:</p>
<blockquote>
<p>We are building a Python CLI tool which uses matplotlib to generate a PNG image containing a chart. It will have multiple sub commands for different chart types, controlled by command line options. Everything you need to know to use it will be available in the single "chartroom --help" output.</p>
<p>It will accept data from files or standard input as CSV or TSV or JSON, similar to how sqlite-utils accepts data - clone simonw/sqlite-utils to /tmp for reference there. Clone matplotlib/matplotlib for reference as well</p>
<p>It will also accept data from --sql path/to/sqlite.db "select ..." which runs in read-only mode</p>
<p>Start by asking clarifying questions - do not use the ask user tool though it is broken - and generate a spec for me to approve</p>
<p>Once approved proceed using red/green TDD running tests with "uv run pytest"</p>
<p>Also while building maintain a demo/README.md document using the "uvx showboat --help" tool - each time you get a new chart type working commit the tests, implementation, root level
README update and a new version of that demo/README.md document with an inline image demo of the new chart type (which should be a UUID image filename managed by the showboat image command and should be stored in the demo/ folder</p>
<p>Make sure "uv build" runs cleanly without complaining about extra directories but also ensure dist/ and uv.lock are in gitignore</p>
</blockquote>
<p>This got most of the work done. You can see the rest <a href="https://github.com/simonw/chartroom/pulls?q=is%3Apr+is%3Aclosed">in the PRs</a> that followed.</p>
<h4 id="the-burgeoning-showboat-ecosystem">The burgeoning Showboat ecosystem</h4>
<p>The Showboat family of tools now consists of <a href="https://github.com/simonw/showboat">Showboat</a> itself, <a href="https://github.com/simonw/rodney">Rodney</a> for browser automation, <a href="https://github.com/simonw/chartroom">Chartroom</a> for charting and <a href="https://github.com/simonw/datasette-showboat">datasette-showboat</a> for streaming remote Showboat documents to Datasette.</p>
<p>I'm enjoying how these tools can operate together based on a very loose set of conventions. If a tool can output a path to an image Showboat can include that image in a document. Any tool that can output text can be used with Showboat.</p>
<p>I'll almost certainly be building more tools that fit this pattern. They're very quick to knock out!</p>
<p>The environment variable mechanism for Showboat's remote streaming is a fun hack too - so far I'm just using it to stream documents somewhere else, but it's effectively a webhook extension mechanism that could likely be used for all sorts of things I haven't thought of yet.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/charting">charting</a>, <a href="https://simonwillison.net/tags/projects">projects</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/datasette">datasette</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/showboat">showboat</a></p>]]></description><pubDate>Tue, 17 Feb 2026 00:43:45 +0000</pubDate></item><item><title>Rodney and Claude Code for Desktop</title><link>https://simonwillison.net/2026/Feb/16/rodney-claude-code/#atom-everything</link><description><![CDATA[<p>I'm a very heavy user of <a href="https://code.claude.com/docs/en/claude-code-on-the-web">Claude Code on the web</a>, Anthropic's excellent but poorly named cloud version of Claude Code where everything runs in a container environment managed by them, greatly reducing the risk of anything bad happening to a computer I care about.</p>
<p>I don't use the web interface at all (hence my dislike of the name) - I access it exclusively through their native iPhone and Mac desktop apps.</p>
<p>Something I particularly appreciate about the desktop app is that it lets you see images that Claude is "viewing" via its <code>Read /path/to/image</code> tool. Here's what that looks like:</p>
<p><img alt="Screenshot of a Claude Code session in Claude Desktop. Claude says: The debug page looks good - all items listed with titles and descriptions. Now let me check the nav
menu -  Analyzed menu image file - Bash uvx rodney open &quot;http://localhost:8765/&quot; 2&gt;&amp;1 &amp;&amp; uvx rodney click &quot;details.nav-menu summary&quot; 2&gt;&amp;1 &amp;% sleep 0.5 &amp;&amp; uvx rodney screenshot /tmp/menu.png 2&gt;&amp;1 Output reads: Datasette: test, Clicked, /tmp/menu.png - then it says Read /tmp/menu.png and reveals a screenshot of the Datasette interface with the nav menu open, showing only &quot;Debug&quot; and &quot;Log out&quot; options. Claude continues: The menu now has just &quot;Debug&quot; and “Log out&quot; — much cleaner. Both pages look good. Let me clean up the server and run the remaining tests." src="https://static.simonwillison.net/static/2026/rodney-claude-desktop.jpg" /></p>
<p>This means you can get a visual preview of what it's working on while it's working, without waiting for it to push code to GitHub for you to try out yourself later on.</p>
<p>The prompt I used to trigger the above screenshot was:</p>
<blockquote>
<p><code>Run "uvx rodney --help" and then use Rodney to manually test the new pages and menu - look at screenshots from it and check you think they look OK</code></p>
</blockquote>
<p>I designed <a href="https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#rodney-cli-browser-automation-designed-to-work-with-showboat">Rodney</a> to have <a href="https://github.com/simonw/rodney/blob/main/help.txt">--help output</a> that provides everything a coding agent needs to know in order to use the tool.</p>
<p>The Claude iPhone app doesn't display opened images yet, so I <a href="https://twitter.com/simonw/status/2023432616066879606">requested it as a feature</a> just now in a thread on Twitter.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/async-coding-agents">async-coding-agents</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/projects">projects</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/rodney">rodney</a></p>]]></description><pubDate>Mon, 16 Feb 2026 16:38:57 +0000</pubDate></item><item><title>The AI Vampire</title><link>https://simonwillison.net/2026/Feb/15/the-ai-vampire/#atom-everything</link><description><![CDATA[<p><strong><a href="https://steve-yegge.medium.com/the-ai-vampire-eda6e4f07163">The AI Vampire</a></strong></p>
Steve Yegge's take on agent fatigue, and its relationship to burnout.</p>
<blockquote>
<p>Let's pretend you're the only person at your company using AI.</p>
<p>In Scenario A, you decide you're going to impress your employer, and work for 8 hours a day at 10x productivity. You knock it out of the park and make everyone else look terrible by comparison.</p>
<p>In that scenario, your employer captures 100% of the value from <em>you</em> adopting AI. You get nothing, or at any rate, it ain't gonna be 9x your salary. And everyone hates you now.</p>
<p>And you're <em>exhausted.</em> You're tired, Boss. You got nothing for it.</p>
<p>Congrats, you were just drained by a company. I've been drained to the point of burnout several times in my career, even at Google once or twice. But now with AI, it's oh, so much easier.</p>
</blockquote>
<p>Steve reports needing more sleep due to the cognitive burden involved in agentic engineering, and notes that four hours of agent work a day is a more realistic pace:</p>
<blockquote>
<p>I’ve argued that AI has turned us all into Jeff Bezos, by automating the easy work, and leaving us with all the difficult decisions, summaries, and problem-solving. I find that I am only really comfortable working at that pace for short bursts of a few hours once or occasionally twice a day, even with lots of practice.</p>
</blockquote>

    <p><small></small>Via <a href="https://cosocial.ca/@timbray/116076167774984883">Tim Bray</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/steve-yegge">steve-yegge</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/cognitive-debt">cognitive-debt</a></p>]]></description><pubDate>Sun, 15 Feb 2026 23:59:36 +0000</pubDate></item><item><title>Em dash</title><link>https://simonwillison.net/2026/Feb/15/em-dashes/#atom-everything</link><description><![CDATA[<p>I'm occasionally accused of using LLMs to write the content on my blog. I don't do that, and I don't think my writing has much of an LLM smell to it... with one notable exception:</p>
<pre>    <span class="pl-c"># Finally, do em dashes</span>
    <span class="pl-s1">s</span> <span class="pl-c1">=</span> <span class="pl-s1">s</span>.<span class="pl-c1">replace</span>(<span class="pl-s">' - '</span>, <span class="pl-s">u'<span class="pl-cce">\u2014</span>'</span>)</pre>

<p>That code to add em dashes to my posts dates back to <a href="https://github.com/simonw/simonwillisonblog/blob/e6d0327b37debdf820b5cfef4fb7d09a9624cea9/blog/templatetags/entry_tags.py#L145-L146">at least 2015</a> when I ported my blog from an older version of Django (in a long-lost Mercurial repository) and started afresh on GitHub.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/typography">typography</a>, <a href="https://simonwillison.net/tags/blogging">blogging</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/python">python</a></p>]]></description><pubDate>Sun, 15 Feb 2026 21:40:46 +0000</pubDate></item><item><title>Deep Blue</title><link>https://simonwillison.net/2026/Feb/15/deep-blue/#atom-everything</link><description><![CDATA[<p>We coined a new term on the <a href="https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/">Oxide and Friends podcast</a> last month (primary credit to Adam Leventhal) covering the sense of psychological ennui leading into existential dread that many software developers are feeling thanks to the encroachment of generative AI into their field of work.</p>
<p>We're calling it <strong>Deep Blue</strong>.</p>
<p>You can listen to it being coined in real time <a href="https://www.youtube.com/watch?v=lVDhQMiAbR8&amp;t=2835s">from 47:15 in the episode</a>. I've included <a href="https://simonwillison.net/2026/Feb/15/deep-blue/#transcript">a transcript below</a>.</p>
<p>Deep Blue is a very real issue.</p>
<p>Becoming a professional software engineer is <em>hard</em>. Getting good enough for people to pay you money to write software takes years of dedicated work. The rewards are significant: this is a well compensated career which opens up a lot of great opportunities.</p>
<p>It's also a career that's mostly free from gatekeepers and expensive prerequisites. You don't need an expensive degree or accreditation. A laptop, an internet connection and a lot of time and curiosity is enough to get you started.</p>
<p>And it rewards the nerds! Spending your teenage years tinkering with computers turned out to be a very smart investment in your future.</p>
<p>The idea that this could all be stripped away by a chatbot is <em>deeply</em> upsetting.</p>
<p>I've seen signs of Deep Blue in most of the online communities I spend time in. I've even faced accusations from my peers that I am actively harming their future careers through my work helping people understand how well AI-assisted programming can work.</p>
<p>I think this is an issue which is causing genuine mental anguish for a lot of people in our community. Giving it a name makes it easier for us to have conversations about it.</p>
<h4 id="my-experiences-of-deep-blue">My experiences of Deep Blue</h4>
<p>I distinctly remember my first experience of Deep Blue. For me it was triggered by ChatGPT Code Interpreter back in early 2023.</p>
<p>My primary project is <a href="https://datasette.io/">Datasette</a>, an ecosystem of open source tools for telling stories with data. I had dedicated myself to the challenge of helping people (initially focusing on journalists) clean up, analyze and find meaning in data, in all sorts of shapes and sizes.</p>
<p>I expected I would need to build a lot of software for this! It felt like a challenge that could keep me happily engaged for many years to come.</p>
<p>Then I tried uploading a CSV file of <a href="https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-2018-to-Present/wg3w-h783/about_data">San Francisco Police Department Incident Reports</a> - hundreds of thousands of rows - to ChatGPT Code Interpreter and... it did every piece of data cleanup and analysis I had on my napkin roadmap for the next few years with a couple of prompts.</p>
<p>It even converted the data into a neatly normalized SQLite database and let me download the result!</p>
<p>I remember having two competing thoughts in parallel.</p>
<p>On the one hand, as somebody who wants journalists to be able to do more with data, this felt like a <em>huge</em> breakthrough. Imagine giving every journalist in the world an on-demand analyst who could help them tackle any data question they could think of!</p>
<p>But on the other hand... <em>what was I even for</em>? My confidence in the value of my own projects took a painful hit. Was the path I'd chosen for myself suddenly a dead end?</p>
<p>I've had some further pangs of Deep Blue just in the past few weeks, thanks to the Claude Opus 4.5/4.6 and GPT-5.2/5.3 coding agent effect. As many other people are also observing, the latest generation of coding agents, given the right prompts, really can churn away for a few minutes to several hours and produce working, documented and fully tested software that exactly matches the criteria they were given.</p>
<p>"The code they write isn't any good" doesn't really cut it any more.</p>
<h4 id="transcript">A lightly edited transcript</h4>
<blockquote>
<p><strong>Bryan</strong>: I think that we're going to see a real problem with AI induced ennui where software engineers in particular get listless because the AI can do anything. Simon, what do you think about that?</p>
<p><strong>Simon</strong>: Definitely. Anyone who's paying close attention to coding agents is feeling some of that already. There's an extent where you sort of get over it when you realize that you're still useful, even though your ability to memorize the syntax of program languages is completely irrelevant now.</p>
<p>Something I see a lot of is people out there who are having existential crises and are very, very unhappy because they're like, "I dedicated my career to learning this thing and now it just does it. What am I even for?". I will very happily try and convince those people that they are for a whole bunch of things and that none of that experience they've accumulated has gone to waste, but psychologically it's a difficult time for software engineers.</p>
<p>[...]</p>
<p><strong>Bryan</strong>: Okay, so I'm going to predict that we name that. Whatever that is, we have a name for that kind of feeling and that kind of, whether you want to call it a blueness or a loss of purpose, and that we're kind of trying to address it collectively in a directed way.</p>
<p><strong>Adam</strong>: Okay, this is your big moment. Pick the name. If you call your shot from here, this is you pointing to the stands. You know, I – Like deep blue, you know.</p>
<p><strong>Bryan</strong>: Yeah, deep blue. I like that. I like deep blue. Deep blue. Oh, did you walk me into that, you bastard? You just blew out the candles on my birthday cake.</p>
<p>It wasn't my big moment at all. That was your big moment. No, that is, Adam, that is very good. That is deep blue.</p>
<p><strong>Simon</strong>: All of the chess players and the Go players went through this a decade ago and they have come out stronger.</p>
</blockquote>
<p>Turns out it was more than a decade ago: <a href="https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov">Deep Blue defeated Garry Kasparov in 1997</a>.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/definitions">definitions</a>, <a href="https://simonwillison.net/tags/careers">careers</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/oxide">oxide</a>, <a href="https://simonwillison.net/tags/bryan-cantrill">bryan-cantrill</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a></p>]]></description><pubDate>Sun, 15 Feb 2026 21:06:44 +0000</pubDate></item><item><title>Gwtar: a static efficient single-file HTML format</title><link>https://simonwillison.net/2026/Feb/15/gwtar/#atom-everything</link><description><![CDATA[<p><strong><a href="https://gwern.net/gwtar">Gwtar: a static efficient single-file HTML format</a></strong></p>
Fascinating new project from Gwern Branwen and Said Achmiz that targets the challenge of combining large numbers of assets into a single archived HTML file without that file being inconvenient to view in a browser.</p>
<p>The key trick it uses is to fire <a href="https://developer.mozilla.org/en-US/docs/Web/API/Window/stop">window.stop()</a> early in the page to prevent the browser from downloading the whole thing, then following that call with inline tar uncompressed content.</p>
<p>It can then make HTTP range requests to fetch content from that tar data on-demand when it is needed by the page.</p>
<p>The JavaScript that has already loaded rewrites asset URLs to point to <code>https://localhost/</code> purely so that they will fail to load. Then it uses a <a href="https://developer.mozilla.org/en-US/docs/Web/API/PerformanceObserver">PerformanceObserver</a> to catch those attempted loads:</p>
<pre><code>let perfObserver = new PerformanceObserver((entryList, observer) =&gt; {
    resourceURLStringsHandler(entryList.getEntries().map(entry =&gt; entry.name));
});
perfObserver.observe({ entryTypes: [ "resource" ] });
</code></pre>
<p>That <code>resourceURLStringsHandler</code> callback finds the resource if it is already loaded or fetches it with an HTTP range request otherwise and then inserts the resource in the right place using a <code>blob:</code> URL.</p>
<p>Here's what the <code>window.stop()</code> portion of the document looks like if you view the source:</p>
<p><img alt="Screenshot of a macOS terminal window titled &quot;gw — more big.html — 123×46&quot; showing the source code of a gwtar (self-extracting HTML archive) file. The visible code includes JavaScript with requestIdleCallback(getMainPageHTML);, a noscript block with warnings: a &quot;js-disabled-warning&quot; stating &quot;This HTML page requires JavaScript to be enabled to render, as it is a self-extracting gwtar HTML file,&quot; a description of gwtar as &quot;a portable self-contained standalone HTML file which is designed to nevertheless support efficient lazy loading of all assets such as large media files,&quot; with a link to https://gwern.net/gwtar, a &quot;local-file-warning&quot; with a shell command perl -ne'print $_ if $x; $x=1 if /&lt;!-- GWTAR END/' &amp;lt; foo.gwtar.html | tar --extract, and a &quot;server-fail-warning&quot; about misconfigured servers. Below the HTML closing tags and &lt;!-- GWTAR END comment is binary tar archive data with the filename 2010-02-brianmoriarty-thesecretofpsalm46.html, showing null-padded tar header fields including ustar^@00root and octal size/permission values. At the bottom, a SingleFile metadata comment shows url: https://web.archive.org/web/20230512001411/http://ludix.com/moriarty/psalm46.html and saved date: Sat Jan 17 2026 19:26:49 GMT-0800 (Pacific Standard Time)." src="https://static.simonwillison.net/static/2026/gwtar.jpg" /></p>
<p>Amusingly for an archive format it doesn't actually work if you open the file directly on your own computer. Here's what you see if you try to do that:</p>
<blockquote>
<p>You are seeing this message, instead of the page you should be seeing, because <code>gwtar</code> files <strong>cannot be opened locally</strong> (due to web browser security restrictions).</p>
<p>To open this page on your computer, use the following shell command:</p>
<p><code>perl -ne'print $_ if $x; $x=1 if /&lt;!-- GWTAR END/' &lt; foo.gwtar.html | tar --extract</code></p>
<p>Then open the file <code>foo.html</code> in any web browser.</p>
</blockquote>

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=47024506">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/archiving">archiving</a>, <a href="https://simonwillison.net/tags/html">html</a>, <a href="https://simonwillison.net/tags/javascript">javascript</a></p>]]></description><pubDate>Sun, 15 Feb 2026 18:26:08 +0000</pubDate></item><item><title>Three months of OpenClaw</title><link>https://simonwillison.net/2026/Feb/15/openclaw/#atom-everything</link><description><![CDATA[<p>It's wild that the first commit to OpenClaw was <a href="https://github.com/openclaw/openclaw/commit/f6dd362d39b8e30bd79ef7560aab9575712ccc11">on November 25th 2025</a>, and less than three months later it's hit 10,000 commits from 600 contributors, attracted 196,000 GitHub stars and sort-of been featured in an extremely vague <a href="https://www.youtube.com/watch?v=n7I-D4YXbzg">Super Bowl commercial for AI.com</a>.</p>
<p>Quoting AI.com founder <a href="https://twitter.com/kris/status/2020663711015514399">Kris Marszalek</a>, purchaser of the <a href="https://www.theregister.com/2026/02/09/70m_aicom_domain_sale/">most expensive domain in history</a> for $70m:</p>
<blockquote>
<p>ai.com is the world’s first easy-to-use and secure implementation of OpenClaw, the open source agent framework that went viral two weeks ago; we made it easy to use without any technical skills, while hardening security to keep your data safe.</p>
</blockquote>
<p>Looks like vaporware to me - all you can do right now is reserve a handle - but it's still remarkable to see an open source project get to <em>that</em> level of hype in such a short space of time.</p>
<p><strong>Update</strong>: OpenClaw creator Peter Steinberger <a href="https://steipete.me/posts/2026/openclaw">just announced</a> that he's joining OpenAI and plans to transfer ownership of OpenClaw to a new independent foundation.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/ai-agents">ai-agents</a>, <a href="https://simonwillison.net/tags/openclaw">openclaw</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/open-source">open-source</a>, <a href="https://simonwillison.net/tags/domains">domains</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/peter-steinberger">peter-steinberger</a></p>]]></description><pubDate>Sun, 15 Feb 2026 17:23:28 +0000</pubDate></item><item><title>Quoting Eric Meyer</title><link>https://simonwillison.net/2026/Feb/15/eric-meyer/#atom-everything</link><description><![CDATA[<blockquote cite="https://mastodon.social/@Meyerweb/116065151451468199"><p>I saw yet another “CSS is a massively bloated mess” whine and I’m like.  My dude.  My brother in Chromium.  It is trying as hard as it can to express the totality of visual presentation and layout design and typography and animation and digital interactivity and a few other things in a human-readable text format.  It’s not bloated, it’s fantastically ambitious.  Its reach is greater than most of us can hope to grasp.  Put some <em>respect</em> on its <em>name</em>.</p></blockquote>
<p class="cite">&mdash; <a href="https://mastodon.social/@Meyerweb/116065151451468199">Eric Meyer</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/css">css</a>, <a href="https://simonwillison.net/tags/web-standards">web-standards</a>, <a href="https://simonwillison.net/tags/eric-meyer">eric-meyer</a></p>]]></description><pubDate>Sun, 15 Feb 2026 13:36:20 +0000</pubDate></item><item><title>How Generative and Agentic AI Shift Concern from Technical Debt to Cognitive Debt</title><link>https://simonwillison.net/2026/Feb/15/cognitive-debt/#atom-everything</link><description><![CDATA[<p><strong><a href="https://margaretstorey.com/blog/2026/02/09/cognitive-debt/">How Generative and Agentic AI Shift Concern from Technical Debt to Cognitive Debt</a></strong></p>
This piece by Margaret-Anne Storey is the best explanation of the term <strong>cognitive debt</strong> I've seen so far.</p>
<blockquote>
<p><em>Cognitive debt</em>, a term gaining <a href="https://www.media.mit.edu/publications/your-brain-on-chatgpt/">traction</a> recently, instead communicates the notion that the debt compounded from going fast lives in the brains of the developers and affects their lived experiences and abilities to “go fast” or to make changes. Even if AI agents produce code that could be easy to understand, the humans involved may have simply lost the plot and may not understand what the program is supposed to do, how their intentions were implemented, or how to possibly change it.</p>
</blockquote>
<p>Margaret-Anne expands on this further with an anecdote about a student team she coached:</p>
<blockquote>
<p>But by weeks 7 or 8, one team hit a wall. They could no longer make even simple changes without breaking something unexpected. When I met with them, the team initially blamed technical debt: messy code, poor architecture, hurried implementations. But as we dug deeper, the real problem emerged: no one on the team could explain why certain design decisions had been made or how different parts of the system were supposed to work together. The code might have been messy, but the bigger issue was that the theory of the system, their shared understanding, had fragmented or disappeared entirely. They had accumulated cognitive debt faster than technical debt, and it paralyzed them.</p>
</blockquote>
<p>I've experienced this myself on some of my more ambitious vibe-code-adjacent projects. I've been experimenting with prompting entire new features into existence without reviewing their implementations and, while it works surprisingly well, I've found myself getting lost in my own projects.</p>
<p>I no longer have a firm mental model of what they can do and how they work, which means each additional feature becomes harder to reason about, eventually leading me to lose the ability to make confident decisions about where to go next.

    <p><small></small>Via <a href="https://martinfowler.com/fragments/2026-02-13.html">Martin Fowler</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/definitions">definitions</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a>, <a href="https://simonwillison.net/tags/cognitive-debt">cognitive-debt</a></p>]]></description><pubDate>Sun, 15 Feb 2026 05:20:11 +0000</pubDate></item><item><title>Launching Interop 2026</title><link>https://simonwillison.net/2026/Feb/15/interop-2026/#atom-everything</link><description><![CDATA[<p><strong><a href="https://hacks.mozilla.org/2026/02/launching-interop-2026/">Launching Interop 2026</a></strong></p>
Jake Archibald reports on Interop 2026, the initiative between Apple, Google, Igalia, Microsoft, and Mozilla to collaborate on ensuring a targeted set of web platform features reach cross-browser parity over the course of the year.</p>
<p>I hadn't realized how influential and successful the Interop series has been. It started back in 2021 as <a href="https://web.dev/blog/compat2021">Compat 2021</a> before being rebranded to Interop <a href="https://blogs.windows.com/msedgedev/2022/03/03/microsoft-edge-and-interop-2022/">in 2022</a>.</p>
<p>The dashboards for each year can be seen here, and they demonstrate how wildly effective the program has been: <a href="https://wpt.fyi/interop-2021">2021</a>, <a href="https://wpt.fyi/interop-2022">2022</a>, <a href="https://wpt.fyi/interop-2023">2023</a>, <a href="https://wpt.fyi/interop-2024">2024</a>, <a href="https://wpt.fyi/interop-2025">2025</a>, <a href="https://wpt.fyi/interop-2026">2026</a>.</p>
<p>Here's the progress chart for 2025, which shows every browser vendor racing towards a 95%+ score by the end of the year:</p>
<p><img alt="Line chart showing Interop 2025 browser compatibility scores over the year (Jan–Dec) for Chrome, Edge, Firefox, Safari, and Interop. Y-axis ranges from 0% to 100%. Chrome (yellow) and Edge (green) lead, starting around 80% and reaching near 100% by Dec. Firefox (orange) starts around 48% and climbs to ~98%. Safari (blue) starts around 45% and reaches ~96%. The Interop line (dark green/black) starts lowest around 29% and rises to ~95% by Dec. All browsers converge near 95–100% by year's end." src="https://static.simonwillison.net/static/2026/interop-2025.jpg" /></p>
<p>The feature I'm most excited about in 2026 is <a href="https://developer.mozilla.org/docs/Web/API/View_Transition_API/Using#basic_mpa_view_transition">Cross-document View Transitions</a>, building on the successful 2025 target of <a href="https://developer.mozilla.org/docs/Web/API/View_Transition_API/Using">Same-Document View Transitions</a>. This will provide fancy SPA-style transitions between pages on websites with no JavaScript at all.</p>
<p>As a keen WebAssembly tinkerer I'm also intrigued by this one:</p>
<blockquote>
<p><a href="https://github.com/WebAssembly/js-promise-integration/blob/main/proposals/js-promise-integration/Overview.md">JavaScript Promise Integration for Wasm</a> allows WebAssembly to asynchronously 'suspend', waiting on the result of an external promise. This simplifies the compilation of languages like C/C++ which expect APIs to run synchronously.</p>
</blockquote>


    <p>Tags: <a href="https://simonwillison.net/tags/browsers">browsers</a>, <a href="https://simonwillison.net/tags/css">css</a>, <a href="https://simonwillison.net/tags/javascript">javascript</a>, <a href="https://simonwillison.net/tags/web-standards">web-standards</a>, <a href="https://simonwillison.net/tags/webassembly">webassembly</a>, <a href="https://simonwillison.net/tags/jake-archibald">jake-archibald</a></p>]]></description><pubDate>Sun, 15 Feb 2026 04:33:22 +0000</pubDate></item><item><title>Quoting Boris Cherny</title><link>https://simonwillison.net/2026/Feb/14/boris/#atom-everything</link><description><![CDATA[<blockquote cite="https://twitter.com/bcherny/status/2022762422302576970"><p>Someone has to prompt the Claudes, talk to customers, coordinate with other teams, decide what to build next. Engineering is changing and great engineers are more important than ever.</p></blockquote>
<p class="cite">&mdash; <a href="https://twitter.com/bcherny/status/2022762422302576970">Boris Cherny</a>, Claude Code creator, on why Anthropic are still hiring developers</p>

    <p>Tags: <a href="https://simonwillison.net/tags/careers">careers</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a></p>]]></description><pubDate>Sat, 14 Feb 2026 23:59:09 +0000</pubDate></item><item><title>Quoting Thoughtworks</title><link>https://simonwillison.net/2026/Feb/14/thoughtworks/#atom-everything</link><description><![CDATA[<blockquote cite="https://www.thoughtworks.com/content/dam/thoughtworks/documents/report/tw_future%20_of_software_development_retreat_%20key_takeaways.pdf"><p>The retreat challenged the narrative that AI eliminates the need for junior developers. Juniors are more profitable than they have ever been. AI tools get them past the awkward initial net-negative phase faster. They serve as a call option on future productivity. And they are better at AI tools than senior engineers, having never developed the habits and assumptions that slow adoption.</p>
<p>The real concern is mid-level engineers who came up during the decade-long hiring boom and may not have developed the fundamentals needed to thrive in the new environment. This population represents the bulk of the industry by volume, and retraining them is genuinely difficult. The retreat discussed whether apprenticeship models, rotation programs and lifelong learning structures could address this gap, but acknowledged that no organization has solved it yet.</p></blockquote>
<p class="cite">&mdash; <a href="https://www.thoughtworks.com/content/dam/thoughtworks/documents/report/tw_future%20_of_software_development_retreat_%20key_takeaways.pdf">Thoughtworks</a>, findings from a retreat concerning "the future of software engineering", conducted under Chatham House rules</p>

    <p>Tags: <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/careers">careers</a>, <a href="https://simonwillison.net/tags/ai">ai</a></p>]]></description><pubDate>Sat, 14 Feb 2026 04:54:41 +0000</pubDate></item><item><title>Anthropic&apos;s public benefit mission</title><link>https://simonwillison.net/2026/Feb/13/anthropic-public-benefit-mission/#atom-everything</link><description><![CDATA[<p>Someone <a href="https://news.ycombinator.com/item?id=47008560#47008978">asked</a> if there was an Anthropic equivalent to <a href="https://simonwillison.net/2026/Feb/13/openai-mission-statement/">OpenAI's IRS mission statements over time</a>.</p>
<p>Anthropic are a "public benefit corporation" but not a non-profit, so they don't have the same requirements to file public documents with the IRS every year.</p>
<p>But when I asked Claude it ran a search and dug up this <a href="https://drive.google.com/drive/folders/1ImqXYv9_H2FTNAujZfu3EPtYFD4xIlHJ">Google Drive folder</a> where Zach Stein-Perlman shared Certificate of Incorporation documents he <a href="https://ailabwatch.substack.com/p/anthropics-certificate-of-incorporation">obtained from the State of Delaware</a>!</p>
<p>Anthropic's are much less interesting that OpenAI's. The earliest document from 2021 states:</p>
<blockquote>
<p>The specific public benefit that the Corporation will promote is to responsibly develop and maintain advanced Al for the cultural, social and technological improvement of humanity.</p>
</blockquote>
<p>Every subsequent document up to 2024 uses an updated version which says:</p>
<blockquote>
<p>The specific public benefit that the Corporation will promote is to responsibly develop and maintain advanced AI for the long term benefit of humanity.</p>
</blockquote>

    <p>Tags: <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/ai">ai</a></p>]]></description><pubDate>Fri, 13 Feb 2026 23:59:51 +0000</pubDate></item><item><title>The evolution of OpenAI&apos;s mission statement</title><link>https://simonwillison.net/2026/Feb/13/openai-mission-statement/#atom-everything</link><description><![CDATA[<p>As a USA <a href="https://en.wikipedia.org/wiki/501(c)(3)_organization">501(c)(3)</a> the OpenAI non-profit has to file a tax return each year with the IRS. One of the required fields on that tax return is to "Briefly describe the organization’s mission or most significant activities" - this has actual legal weight to it as the IRS can use it to evaluate if the organization is sticking to its mission and deserves to maintain its non-profit tax-exempt status.</p>
<p>You can browse OpenAI's <a href="https://projects.propublica.org/nonprofits/organizations/810861541">tax filings by year</a> on ProPublica's excellent <a href="https://projects.propublica.org/nonprofits/">Nonprofit Explorer</a>.</p>
<p>I went through and extracted that mission statement for 2016 through 2024, then had Claude Code <a href="https://gisthost.github.io/?7a569df89f43f390bccc2c5517718b49/index.html">help me</a> fake the commit dates to turn it into a git repository and share that as a Gist - which means that Gist's <a href="https://gist.github.com/simonw/e36f0e5ef4a86881d145083f759bcf25/revisions">revisions page</a> shows every edit they've made since they started filing their taxes!</p>
<p>It's really interesting seeing what they've changed over time.</p>
<p>The original 2016 mission reads as follows (and yes, the apostrophe in "OpenAIs" is missing <a href="https://projects.propublica.org/nonprofits/organizations/810861541/201703459349300445/full">in the original</a>):</p>
<blockquote>
<p>OpenAIs goal is to advance digital intelligence in the way that is most likely to benefit humanity as a whole, unconstrained by a need to generate financial return. We think that artificial intelligence technology will help shape the 21st century, and we want to help the world build safe AI technology and ensure that AI's benefits are as widely and evenly distributed as possible. Were trying to build AI as part of a larger community, and we want to openly share our plans and capabilities along the way.</p>
</blockquote>
<p>In 2018 they dropped the part about "trying to build AI as part of a larger community, and we want to openly share our plans and capabilities along the way."</p>
<p><img src="https://static.simonwillison.net/static/2026/mission-3.jpg" alt="Git diff showing the 2018 revision deleting the final two sentences: &quot;Were trying to build AI as part of a larger community, and we want to openly share our plans and capabilities along the way.&quot;" style="max-width: 100%;" /></p>
<p>In 2020 they dropped the words "as a whole" from "benefit humanity as a whole". They're still "unconstrained by a need to generate financial return" though.</p>
<p><img src="https://static.simonwillison.net/static/2026/mission-5.jpg" alt="Git diff showing the 2020 revision dropping &quot;as a whole&quot; from &quot;benefit humanity as a whole&quot; and changing &quot;We think&quot; to &quot;OpenAI believes&quot;" style="max-width: 100%;" /></p>
<p>Some interesting changes in 2021. They're still unconstrained by a need to generate financial return, but here we have the first reference to "general-purpose artificial intelligence" (replacing "digital intelligence"). They're more confident too: it's not "most likely to benefit humanity", it's just "benefits humanity".</p>
<p>They previously wanted to "help the world build safe AI technology", but now they're going to do that themselves: "the companys goal is to develop and responsibly deploy safe AI technology".</p>
<p><img src="https://static.simonwillison.net/static/2026/mission-6.jpg" alt="Git diff showing the 2021 revision replacing &quot;goal is to advance digital intelligence&quot; with &quot;mission is to build general-purpose artificial intelligence&quot;, changing &quot;most likely to benefit&quot; to just &quot;benefits&quot;, and replacing &quot;help the world build safe AI technology&quot; with &quot;the companys goal is to develop and responsibly deploy safe AI technology&quot;" style="max-width: 100%;" /></p>
<p>2022 only changed one significant word: they added "safely" to "build ... (AI) that safely benefits humanity". They're still unconstrained by those financial returns!</p>
<p><img src="https://static.simonwillison.net/static/2026/mission-7.jpg" alt="Git diff showing the 2022 revision adding &quot;(AI)&quot; and the word &quot;safely&quot; so it now reads &quot;that safely benefits humanity&quot;, and changing &quot;the companys&quot; to &quot;our&quot;" style="max-width: 100%;" /></p>
<p>No changes in 2023... but then in 2024 they deleted almost the entire thing, reducing it to simply:</p>
<blockquote>
<p>OpenAIs mission is to ensure that artificial general intelligence benefits all of humanity.</p>
</blockquote>
<p>They've expanded "humanity" to "all of humanity", but there's no mention of safety any more and I guess they can finally start focusing on that need to generate financial returns!</p>
<p><img src="https://static.simonwillison.net/static/2026/mission-9.jpg" alt="Git diff showing the 2024 revision deleting the entire multi-sentence mission statement and replacing it with just &quot;OpenAIs mission is to ensure that artificial general intelligence benefits all of humanity.&quot;" style="max-width: 100%;" /></p>

<p><strong>Update</strong>: I found loosely equivalent but much less interesting documents <a href="https://simonwillison.net/2026/Feb/13/anthropic-public-benefit-mission/">from Anthropic</a>.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/propublica">propublica</a></p>]]></description><pubDate>Fri, 13 Feb 2026 23:38:29 +0000</pubDate></item><item><title>Introducing GPT‑5.3‑Codex‑Spark</title><link>https://simonwillison.net/2026/Feb/12/codex-spark/#atom-everything</link><description><![CDATA[<p><strong><a href="https://openai.com/index/introducing-gpt-5-3-codex-spark/">Introducing GPT‑5.3‑Codex‑Spark</a></strong></p>
OpenAI announced a partnership with Cerebras <a href="https://openai.com/index/cerebras-partnership/">on January 14th</a>. Four weeks later they're already launching the first integration, "an ultra-fast model for real-time coding in Codex".</p>
<p>Despite being named GPT-5.3-Codex-Spark it's not purely an accelerated alternative to GPT-5.3-Codex - the blog post calls it "a smaller version of GPT‑5.3-Codex" and clarifies that "at launch, Codex-Spark has a 128k context window and is text-only."</p>
<p>I had some preview access to this model and I can confirm that it's significantly faster than their other models.</p>
<p>Here's what that speed looks like running in Codex CLI:</p>
<div style="max-width: 100%;">
    <video 
        controls 
        preload="none"
        poster="https://static.simonwillison.net/static/2026/gpt-5.3-codex-spark-medium-last.jpg"
        style="width: 100%; height: auto;">
        <source src="https://static.simonwillison.net/static/2026/gpt-5.3-codex-spark-medium.mp4" type="video/mp4">
    </video>
</div>

<p>That was the "Generate an SVG of a pelican riding a bicycle" prompt - here's the rendered result:</p>
<p><img alt="Whimsical flat illustration of an orange duck merged with a bicycle, where the duck's body forms the seat and frame area while its head extends forward over the handlebars, set against a simple light blue sky and green grass background." src="https://static.simonwillison.net/static/2026/gpt-5.3-codex-spark-pelican.png" /></p>
<p>Compare that to the speed of regular GPT-5.3 Codex medium:</p>
<div style="max-width: 100%;">
    <video 
        controls 
        preload="none"
        poster="https://static.simonwillison.net/static/2026/gpt-5.3-codex-medium-last.jpg"
        style="width: 100%; height: auto;">
        <source src="https://static.simonwillison.net/static/2026/gpt-5.3-codex-medium.mp4" type="video/mp4">
    </video>
</div>

<p>Significantly slower, but the pelican is a lot better:</p>
<p><img alt="Whimsical flat illustration of a white pelican riding a dark blue bicycle at speed, with motion lines behind it, its long orange beak streaming back in the wind, set against a light blue sky and green grass background." src="https://static.simonwillison.net/static/2026/gpt-5.3-codex-pelican.png" /></p>
<p>What's interesting about this model isn't the quality though, it's the <em>speed</em>. When a model responds this fast you can stay in flow state and iterate with the model much more productively.</p>
<p>I showed a demo of Cerebras running Llama 3.1 70 B at 2,000 tokens/second against Val Town <a href="https://simonwillison.net/2024/Oct/31/cerebras-coder/">back in October 2024</a>. OpenAI claim 1,000 tokens/second for their new model, and I expect it will prove to be a ferociously useful partner for hands-on iterative coding sessions.</p>
<p>It's not yet clear what the pricing will look like for this new model.


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/cerebras">cerebras</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a>, <a href="https://simonwillison.net/tags/codex-cli">codex-cli</a>, <a href="https://simonwillison.net/tags/inference-speed">inference-speed</a></p>]]></description><pubDate>Thu, 12 Feb 2026 21:16:07 +0000</pubDate></item></channel></rss>