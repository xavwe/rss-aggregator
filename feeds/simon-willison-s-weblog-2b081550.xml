<?xml version="1.0" encoding="utf-8"?><rss version="2.0"><channel><title>Simon Willison&apos;s Weblog</title><link>https://raw.githubusercontent.com/xavwe/rss-aggregator/refs/heads/main/feeds/simon-willison-s-weblog-2b081550.xml</link><description>Archived feed from https://simonwillison.net/atom/everything</description><item><title>Datasette 1.0a24</title><link>https://simonwillison.net/2026/Jan/29/datasette-10a24/#atom-everything</link><description><![CDATA[<p><strong><a href="https://docs.datasette.io/en/latest/changelog.html#a24-2026-01-29">Datasette 1.0a24</a></strong></p>
New Datasette alpha this morning. Key new features:</p>
<ul>
<li>Datasette's <code>Request</code> object can now handle <code>multipart/form-data</code> file uploads via the new <a href="https://docs.datasette.io/en/latest/internals.html#internals-formdata">await request.form(files=True)</a>  method. I plan to use this for a <code>datasette-files</code> plugin to support attaching files to rows of data.</li>
<li>The <a href="https://docs.datasette.io/en/latest/contributing.html#setting-up-a-development-environment">recommended development environment</a> for hacking on Datasette itself now uses <a href="https://github.com/astral-sh/uv">uv</a>. Crucially, you can clone Datasette and run <code>uv run pytest</code> to run the tests without needing to manually create a virtual environment or install dependencies first, thanks to the <a href="https://til.simonwillison.net/uv/dependency-groups">dev dependency group pattern</a>.</li>
<li>A new <code>?_extra=render_cell</code> parameter for both table and row JSON pages to return the results of executing the <a href="https://docs.datasette.io/en/latest/plugin_hooks.html#render-cell-row-value-column-table-database-datasette-request">render_cell() plugin hook</a>. This should unlock new JavaScript UI features in the future.</li>
</ul>
<p>More details <a href="https://docs.datasette.io/en/latest/changelog.html#a24-2026-01-29">in the release notes</a>. I also invested a bunch of work in eliminating flaky tests that were intermittently failing in CI - I <em>think</em> those are all handled now.


    <p>Tags: <a href="https://simonwillison.net/tags/projects">projects</a>, <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/datasette">datasette</a>, <a href="https://simonwillison.net/tags/annotated-release-notes">annotated-release-notes</a>, <a href="https://simonwillison.net/tags/uv">uv</a></p>]]></description><pubDate>Thu, 29 Jan 2026 17:21:51 +0000</pubDate></item><item><title>Adding dynamic features to an aggressively cached website</title><link>https://simonwillison.net/2026/Jan/28/dynamic-features-static-site/#atom-everything</link><description><![CDATA[<p>My blog uses aggressive caching: it sits behind Cloudflare with a 15 minute cache header, which guarantees it can survive even the largest traffic spike to any given page. I've recently added a couple of dynamic features that work in spite of that full-page caching. Here's how those work.</p>
<h4 id="edit-links-that-are-visible-only-to-me">Edit links that are visible only to me</h4>
<p>This is a Django site and I manage it through the Django admin.</p>
<p>I have <a href="https://github.com/simonw/simonwillisonblog/blob/b8066f870a94d149f5e8cee6e787d3377c0b9507/blog/models.py#L254-L449">four types of content</a> - entries, link posts (aka blogmarks), quotations and notes. Each of those has a different model and hence a different Django admin area.</p>
<p>I wanted an "edit" link on the public pages that was only visible to me.</p>
<p>The button looks like this:</p>
<p><img src="https://static.simonwillison.net/static/2026/edit-link.jpg" alt="Entry footer - it says Posted 27th January 2026 at 9:44 p.m. followed by a square Edit button with an icon." style="max-width: 100%;" /></p>
<p>I solved conditional display of this button with <code>localStorage</code>. I have a <a href="https://github.com/simonw/simonwillisonblog/blob/b8066f870a94d149f5e8cee6e787d3377c0b9507/templates/base.html#L89-L105">tiny bit of JavaScript</a> which checks to see if the <code>localStorage</code> key <code>ADMIN</code> is set and, if it is, displays an edit link based on a data attribute:</p>
<div class="highlight highlight-source-js"><pre><span class="pl-smi">document</span><span class="pl-kos">.</span><span class="pl-en">addEventListener</span><span class="pl-kos">(</span><span class="pl-s">'DOMContentLoaded'</span><span class="pl-kos">,</span> <span class="pl-kos">(</span><span class="pl-kos">)</span> <span class="pl-c1">=&gt;</span> <span class="pl-kos">{</span>
  <span class="pl-k">if</span> <span class="pl-kos">(</span><span class="pl-smi">window</span><span class="pl-kos">.</span><span class="pl-c1">localStorage</span><span class="pl-kos">.</span><span class="pl-en">getItem</span><span class="pl-kos">(</span><span class="pl-s">'ADMIN'</span><span class="pl-kos">)</span><span class="pl-kos">)</span> <span class="pl-kos">{</span>
    <span class="pl-smi">document</span><span class="pl-kos">.</span><span class="pl-en">querySelectorAll</span><span class="pl-kos">(</span><span class="pl-s">'.edit-page-link'</span><span class="pl-kos">)</span><span class="pl-kos">.</span><span class="pl-en">forEach</span><span class="pl-kos">(</span><span class="pl-s1">el</span> <span class="pl-c1">=&gt;</span> <span class="pl-kos">{</span>
      <span class="pl-k">const</span> <span class="pl-s1">url</span> <span class="pl-c1">=</span> <span class="pl-s1">el</span><span class="pl-kos">.</span><span class="pl-en">getAttribute</span><span class="pl-kos">(</span><span class="pl-s">'data-admin-url'</span><span class="pl-kos">)</span><span class="pl-kos">;</span>
      <span class="pl-k">if</span> <span class="pl-kos">(</span><span class="pl-s1">url</span><span class="pl-kos">)</span> <span class="pl-kos">{</span>
        <span class="pl-k">const</span> <span class="pl-s1">a</span> <span class="pl-c1">=</span> <span class="pl-smi">document</span><span class="pl-kos">.</span><span class="pl-en">createElement</span><span class="pl-kos">(</span><span class="pl-s">'a'</span><span class="pl-kos">)</span><span class="pl-kos">;</span>
        <span class="pl-s1">a</span><span class="pl-kos">.</span><span class="pl-c1">href</span> <span class="pl-c1">=</span> <span class="pl-s1">url</span><span class="pl-kos">;</span>
        <span class="pl-s1">a</span><span class="pl-kos">.</span><span class="pl-c1">className</span> <span class="pl-c1">=</span> <span class="pl-s">'edit-link'</span><span class="pl-kos">;</span>
        <span class="pl-s1">a</span><span class="pl-kos">.</span><span class="pl-c1">innerHTML</span> <span class="pl-c1">=</span> <span class="pl-s">'&lt;svg&gt;...&lt;/svg&gt; Edit'</span><span class="pl-kos">;</span>
        <span class="pl-s1">el</span><span class="pl-kos">.</span><span class="pl-en">appendChild</span><span class="pl-kos">(</span><span class="pl-s1">a</span><span class="pl-kos">)</span><span class="pl-kos">;</span>
        <span class="pl-s1">el</span><span class="pl-kos">.</span><span class="pl-c1">style</span><span class="pl-kos">.</span><span class="pl-c1">display</span> <span class="pl-c1">=</span> <span class="pl-s">'block'</span><span class="pl-kos">;</span>
      <span class="pl-kos">}</span>
    <span class="pl-kos">}</span><span class="pl-kos">)</span><span class="pl-kos">;</span>
  <span class="pl-kos">}</span>
<span class="pl-kos">}</span><span class="pl-kos">)</span><span class="pl-kos">;</span></pre></div>
<p>If you want to see my edit links you can run this snippet of JavaScript:</p>
<div class="highlight highlight-source-js"><pre><span class="pl-s1">localStorage</span><span class="pl-kos">.</span><span class="pl-en">setItem</span><span class="pl-kos">(</span><span class="pl-s">'ADMIN'</span><span class="pl-kos">,</span> <span class="pl-s">'1'</span><span class="pl-kos">)</span><span class="pl-kos">;</span></pre></div>
<p>My Django admin dashboard has <a href="https://github.com/simonw/simonwillisonblog/blob/b8066f870a94d149f5e8cee6e787d3377c0b9507/templates/admin/index.html#L18-L39">a custom checkbox</a> I can click to turn this option on and off in my own browser:</p>
<p><img src="https://static.simonwillison.net/static/2026/edit-toggle.jpg" alt="Screenshot of a Tools settings panel with a teal header reading &quot;Tools&quot; followed by three linked options: &quot;Bulk Tag Tool - Add tags to multiple items at once&quot;, &quot;Merge Tags - Merge multiple tags into one&quot;, &quot;SQL Dashboard - Run SQL queries against the database&quot;, and a checked checkbox labeled &quot;Show &quot;Edit&quot; links on public pages&quot;" style="max-width: 100%;" /></p>
<h4 id="random-navigation-within-a-tag">Random navigation within a tag</h4>
<p>Those admin edit links are a very simple pattern. A more interesting one is a feature I added recently for navigating randomly within a tag.</p>
<p>Here's an animated GIF showing those random tag navigations in action (<a href="https://simonwillison.net/tag/ai-ethics/">try it here</a>):</p>
<p><img src="https://static.simonwillison.net/static/2026/random-by-tag.gif" alt="Animated demo. Starts on the ai-ethics tag page where a new Random button sits next to the feed icon. Clicking that button jumps to a post with that tag and moves the button into the site header - clicking it multiple times jumps to more random items." style="max-width: 100%;" /></p>
<p>On any of my blog's tag pages you can click the "Random" button to bounce to a random post with that tag. That random button then persists in the header of the page and you can click it to continue bouncing to random items in that same tag.</p>
<p>A post can have multiple tags, so there needs to be a little bit of persistent magic to remember which tag you are navigating and display the relevant button in the header.</p>
<p>Once again, this uses <code>localStorage</code>. Any click to a random button records both the tag and the current timestamp to the <code>random_tag</code> key in <code>localStorage</code> before redirecting the user to the <code>/random/name-of-tag/</code> page, which selects a random post and redirects them there.</p>
<p>Any time a new page loads, JavaScript checks if that <code>random_tag</code> key has a value that was recorded within the past 5 seconds. If so, that random button is appended to the header.</p>
<p>This means that, provided the page loads within 5 seconds of the user clicking the button, the random tag navigation will persist on the page.</p>
<p>You can <a href="https://github.com/simonw/simonwillisonblog/blob/b8066f870a94d149f5e8cee6e787d3377c0b9507/templates/base.html#L106-L147">see the code for that here</a>.</p>
<h4 id="and-the-prompts">And the prompts</h4>
<p>I built the random tag feature entirely using Claude Code for web, prompted from my iPhone. I started with the <code>/random/TAG/</code> endpoint (<a href="https://gistpreview.github.io/?2e7de58a779271aa5eb6f4abcd412d72/index.html">full transcript</a>):</p>
<blockquote>
<p>Build /random/TAG/ - a page which picks a random post (could be an entry or blogmark or note or quote) that has that tag and sends a 302 redirect to it, marked as no-cache so Cloudflare does not cache it</p>
<p>Use a union to build a list of every content type (a string representing the table out of the four types) and primary key for every item tagged with that tag, then order by random and return the first one</p>
<p>Then inflate the type and ID into an object and load it and redirect to the URL</p>
<p>Include tests - it should work by setting up a tag with one of each of the content types and then running in a loop calling that endpoint until it has either returned one of each of the four types or it hits 1000 loops at which point fail with an error</p>
</blockquote>
<p>Then:</p>
<blockquote>
<p>I do not like that solution, some of my tags have thousands of items</p>
<p>Can we do something clever with a CTE?</p>
</blockquote>
<p>Here's the <a href="https://github.com/simonw/simonwillisonblog/blob/b8066f870a94d149f5e8cee6e787d3377c0b9507/blog/views.py#L737-L762">something clever with a CTE</a> solution we ended up with.</p>
<p>For the "Random post" button (<a href="https://gistpreview.github.io/?d2d3abe380080ceb9e7fb854fa197bff/index.html">transcript</a>):</p>
<blockquote>
<p>Look at most recent commit, then modify the /tags/xxx/ page to have a "Random post" button which looks good and links to the /random/xxx/ page</p>
</blockquote>
<p>Then:</p>
<blockquote>
<p>Put it before not after the feed icon. It should only display if a tag has more than 5 posts</p>
</blockquote>
<p>And finally, the <code>localStorage</code> implementation that persists a random tag button in the header (<a href="https://gistpreview.github.io/?8405b84f8e53738c8d4377b2e41dcdef/page-001.html">transcript</a>):</p>
<blockquote>
<p>Review the last two commits. Make it so clicking the Random button on a tag page sets a localStorage value for random_tag with that tag and a timestamp. On any other page view that uses the base item template add JS that checks for that localStorage value and makes sure the timestamp is within 5 seconds. If it is within 5 seconds it adds a "Random name-of-tag" button to the little top navigation bar, styled like the original Random button, which bumps the localStorage timestamp and then sends the user to /random/name-of-tag/ when they click it. In this way clicking "Random" on a tag page will send the user into an experience where they can keep clicking to keep surfing randomly in that topic.</p>
</blockquote>
    
        <p>Tags: <a href="https://simonwillison.net/tags/caching">caching</a>, <a href="https://simonwillison.net/tags/django">django</a>, <a href="https://simonwillison.net/tags/javascript">javascript</a>, <a href="https://simonwillison.net/tags/localstorage">localstorage</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/cloudflare">cloudflare</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a></p>]]></description><pubDate>Wed, 28 Jan 2026 22:10:08 +0000</pubDate></item><item><title>The Five Levels: from Spicy Autocomplete to the Dark Factory</title><link>https://simonwillison.net/2026/Jan/28/the-five-levels/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.danshapiro.com/blog/2026/01/the-five-levels-from-spicy-autocomplete-to-the-software-factory/">The Five Levels: from Spicy Autocomplete to the Dark Factory</a></strong></p>
Dan Shapiro proposes a five level model of AI-assisted programming, inspired by the five (or rather six, it's zero-indexed) <a href="https://www.nhtsa.gov/sites/nhtsa.gov/files/2022-05/Level-of-Automation-052522-tag.pdf">levels of driving automation</a>.</p>
<ol start="0">
<li><strong>Spicy autocomplete</strong>, aka original GitHub Copilot or copying and pasting snippets from ChatGPT.</li>
<li>The <strong>coding intern</strong>, writing unimportant snippets and boilerplate with full human review.</li>
<li>The <strong>junior developer</strong>, pair programming with the model but still reviewing every line.</li>
<li>The <strong>developer</strong>. Most code is generated by AI, and you take on the role of full-time code reviewer.</li>
<li>The <strong>engineering team</strong>. You're more of an engineering manager or product/program/project manager. You collaborate on specs and plans, the agents do the work.</li>
<li>The <strong>dark software factory</strong>, like a factory run by robots where the lights are out because robots don't need to see.</li>
</ol>

<p>Dan says about that last category:</p>
<blockquote>
<p>At level 5, it's not really a car any more. You're not really running anybody else's software any more. And your software process isn't really a software process any more. It's a black box that turns specs into software.</p>
<p>Why Dark? Maybe you've heard of the Fanuc Dark Factory, <a href="https://www.organizedergi.com/News/5493/robots-the-maker-of-robots-in-fanuc-s-dark-factory">the robot factory staffed by robots</a>. It's dark, because it's a place where humans are neither needed nor welcome.</p>
<p>I know a handful of people who are doing this. They're small teams, less than five people. And what they're doing is nearly unbelievable -- and it will likely be our future.</p>
</blockquote>
<p>I've talked to one team that's doing the pattern hinted at here. It was <em>fascinating</em>. The key characteristics:</p>
<ul>
<li>Nobody reviews AI-produced code, ever. They don't even look at it.</li>
<li>The goal of the system is to prove that the system works. A huge amount of the coding agent work goes into testing and tooling and simulating related systems and running demos.</li>
<li>The role of the humans is to design that system - to find new patterns that can help the agents work more effectively and demonstrate that the software they are building is robust and effective.</li>
</ul>
<p>It was a tiny team and they stuff they had built in just a few months looked very convincing to me. Some of them had 20+ years of experience as software developers working on systems with high reliability requirements, so they were not approaching this from a naive perspective.</p>
<p>I'm hoping they come out of stealth soon because I can't really share more details than this.


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a></p>]]></description><pubDate>Wed, 28 Jan 2026 21:44:29 +0000</pubDate></item><item><title>One Human + One Agent = One Browser From Scratch</title><link>https://simonwillison.net/2026/Jan/27/one-human-one-agent-one-browser/#atom-everything</link><description><![CDATA[<p><strong><a href="https://emsh.cat/one-human-one-agent-one-browser/">One Human + One Agent = One Browser From Scratch</a></strong></p>
embedding-shapes was <a href="https://emsh.cat/cursor-implied-success-without-evidence/">so infuriated</a> by the hype around Cursor's <a href="https://simonwillison.net/2026/Jan/23/fastrender/">FastRender browser project</a> - thousands of parallel agents producing ~1.6 million lines of Rust - that they were inspired to take a go at building a web browser using coding agents themselves.</p>
<p>The result is <a href="https://github.com/embedding-shapes/one-agent-one-browser">one-agent-one-browser</a> and it's <em>really</em> impressive. Over three days they drove a single Codex CLI agent to build 20,000 lines of Rust that successfully renders HTML+CSS with no Rust crate dependencies at all - though it does (reasonably) use Windows, macOS and Linux system frameworks for image and text rendering.</p>
<p>I installed the <a href="https://github.com/embedding-shapes/one-agent-one-browser/releases/tag/0.1.0">1MB macOS binary release</a> and ran it against my blog:</p>
<pre><code>chmod 755 ~/Downloads/one-agent-one-browser-macOS-ARM64 
~/Downloads/one-agent-one-browser-macOS-ARM64 https://simonwillison.net/
</code></pre>
<p>Here's the result:</p>
<p><img alt="My blog rendered in a window. Everything is in the right place, the CSS gradients look good, the feed subscribe SVG icon is rendered correctly but there's a missing PNG image." src="https://static.simonwillison.net/static/2026/one-agent-simonwillison.jpg" /></p>
<p>It even rendered my SVG feed subscription icon! A PNG image is missing from the page, which looks like an intermittent bug (there's code to render PNGs).</p>
<p>The code is pretty readable too - here's <a href="https://github.com/embedding-shapes/one-agent-one-browser/blob/0.1.0/src/layout/flex.rs">the flexbox implementation</a>.</p>
<p>I had thought that "build a web browser" was the ideal prompt to really stretch the capabilities of coding agents - and that it would take sophisticated multi-agent harnesses (as seen in the Cursor project) and millions of lines of code to achieve.</p>
<p>Turns out one agent driven by a talented engineer, three days and 20,000 lines of Rust is enough to get a very solid basic renderer working!</p>
<p>I'm going to upgrade my <a href="https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#3-years-someone-will-build-a-new-browser-using-mainly-ai-assisted-coding-and-it-won-t-even-be-a-surprise">prediction for 2029</a>: I think we're going to get a <em>production-grade</em> web browser built by a small team using AI assistance by then.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=46779522">Show Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/browsers">browsers</a>, <a href="https://simonwillison.net/tags/predictions">predictions</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/rust">rust</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/codex-cli">codex-cli</a>, <a href="https://simonwillison.net/tags/browser-challenge">browser-challenge</a></p>]]></description><pubDate>Tue, 27 Jan 2026 16:58:08 +0000</pubDate></item><item><title>Kimi K2.5: Visual Agentic Intelligence</title><link>https://simonwillison.net/2026/Jan/27/kimi-k25/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.kimi.com/blog/kimi-k2-5.html">Kimi K2.5: Visual Agentic Intelligence</a></strong></p>
Kimi K2 landed <a href="https://simonwillison.net/2025/Jul/11/kimi-k2/">in July</a> as a 1 trillion parameter open weight LLM. It was joined by Kimi K2 Thinking <a href="https://simonwillison.net/2025/Nov/6/kimi-k2-thinking/">in November</a> which added reasoning capabilities. Now they've made it multi-modal: the K2 models were text-only, but the new 2.5 can handle image inputs as well:</p>
<blockquote>
<p>Kimi K2.5 builds on Kimi K2 with continued pretraining over approximately 15T mixed visual and text tokens. Built as a native multimodal model, K2.5 delivers state-of-the-art coding and vision capabilities and a self-directed agent swarm paradigm.</p>
</blockquote>
<p>The "self-directed agent swarm paradigm" claim there means improved long-sequence tool calling and training on how to break down tasks for multiple agents to work on at once:</p>
<blockquote>
<p>For complex tasks, Kimi K2.5 can self-direct an agent swarm with up to 100 sub-agents, executing parallel workflows across up to 1,500 tool calls. Compared with a single-agent setup, this reduces execution time by up to 4.5x. The agent swarm is automatically created and orchestrated by Kimi K2.5 without any predefined subagents or workflow.</p>
</blockquote>
<p>I used the <a href="https://openrouter.ai/moonshotai/kimi-k2.5">OpenRouter Chat UI</a> to have it "Generate an SVG of a pelican riding a bicycle", and it did <a href="https://gist.github.com/simonw/32a85e337fbc6ee935d10d89726c0476">quite well</a>:</p>
<p><img alt="Cartoon illustration of a white pelican with a large orange beak and yellow throat pouch riding a green bicycle with yellow feet on the pedals, set against a light blue sky with soft bokeh circles and a green grassy hill. The bicycle frame is a little questionable. The pelican is quite good. The feet do not quite align with the pedals, which are floating clear of the frame." src="https://static.simonwillison.net/static/2026/kimi-k2.5-pelican.png" /></p>
<p>As a more interesting test, I decided to exercise the claims around multi-agent planning with this prompt:</p>
<blockquote>
<p>I want to build a Datasette plugin that offers a UI to upload files to an S3 bucket and stores information about them in a SQLite table. Break this down into ten tasks suitable for execution by parallel coding agents.</p>
</blockquote>
<p>Here's <a href="https://gist.github.com/simonw/ee2583b2eb5706400a4737f56d57c456">the full response</a>. It produced ten realistic tasks and reasoned through the dependencies between them. For comparison here's the same prompt <a href="https://claude.ai/share/df9258e7-97ba-4362-83da-76d31d96196f">against Claude Opus 4.5</a> and <a href="https://chatgpt.com/share/6978d48c-3f20-8006-9c77-81161f899104">against GPT-5.2 Thinking</a>.</p>
<p>The <a href="https://huggingface.co/moonshotai/Kimi-K2.5">Hugging Face repository</a> is 595GB. The model uses Kimi's janky "modified MIT" license, which adds the following clause:</p>
<blockquote>
<p>Our only modification part is that, if the Software (or any derivative works thereof) is used for any of your commercial products or services that have more than 100 million monthly active users, or more than 20 million US dollars (or equivalent in other currencies) in monthly revenue, you shall prominently display "Kimi K2.5" on the user interface of such product or service.</p>
</blockquote>
<p>Given the model's size, I expect one way to run it locally would be with MLX and a pair of $10,000 512GB RAM M3 Ultra Mac Studios. That setup has <a href="https://twitter.com/awnihannun/status/1943723599971443134">been demonstrated to work</a> with previous trillion parameter K2 models.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=46775961">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/hugging-face">hugging-face</a>, <a href="https://simonwillison.net/tags/vision-llms">vision-llms</a>, <a href="https://simonwillison.net/tags/llm-tool-use">llm-tool-use</a>, <a href="https://simonwillison.net/tags/ai-agents">ai-agents</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a>, <a href="https://simonwillison.net/tags/ai-in-china">ai-in-china</a>, <a href="https://simonwillison.net/tags/moonshot">moonshot</a>, <a href="https://simonwillison.net/tags/parallel-agents">parallel-agents</a>, <a href="https://simonwillison.net/tags/kimi">kimi</a>, <a href="https://simonwillison.net/tags/janky-licenses">janky-licenses</a></p>]]></description><pubDate>Tue, 27 Jan 2026 15:07:41 +0000</pubDate></item><item><title>Tips for getting coding agents to write good Python tests</title><link>https://simonwillison.net/2026/Jan/26/tests/#atom-everything</link><description><![CDATA[<p>Someone <a href="https://news.ycombinator.com/item?id=46765460#46765823">asked</a> on Hacker News if I had any tips for getting coding agents to write decent quality tests. Here's what I said:</p>
<hr />
<p>I work in Python which helps a lot because there are a TON of good examples of pytest tests floating around in the training data, including things like usage of fixture libraries for mocking external HTTP APIs and snapshot testing and other neat patterns.</p>
<p>Or I can say "use pytest-httpx to mock the endpoints" and Claude knows what I mean.</p>
<p>Keeping an eye on the tests is important. The most common anti-pattern I see is large amounts of duplicated test setup code - which isn't a huge deal, I'm much more more tolerant of duplicated logic in tests than I am in implementation, but it's still worth pushing back on.</p>
<p>"Refactor those tests to use pytest.mark.parametrize" and "extract the common setup into a pytest fixture" work really well there.</p>
<p>Generally though the best way to get good tests out of a coding agent is to make sure it's working in a project with an existing test suite that uses good patterns. Coding agents pick the existing patterns up without needing any extra prompting at all.</p>
<p>I find that once a project has clean basic tests the new tests added by the agents tend to match them in quality. It's similar to how working on large projects with a team of other developers work - keeping the code clean means when people look for examples of how to write a test they'll be pointed in the right direction.</p>
<p>One last tip I use a lot is this:</p>
<pre><code>Clone datasette/datasette-enrichments
from GitHub to /tmp and imitate the
testing patterns it uses
</code></pre>
<p>I do this all the time with different existing projects I've written - the quickest way to show an agent how you like something to be done is to have it look at an example.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/testing">testing</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/hacker-news">hacker-news</a>, <a href="https://simonwillison.net/tags/pytest">pytest</a></p>]]></description><pubDate>Mon, 26 Jan 2026 23:55:29 +0000</pubDate></item><item><title>ChatGPT Containers can now run bash, pip/npm install packages, and download files</title><link>https://simonwillison.net/2026/Jan/26/chatgpt-containers/#atom-everything</link><description><![CDATA[<p>One of my favourite features of ChatGPT is its ability to write and execute code in a container. This feature launched as ChatGPT Code Interpreter <a href="https://simonwillison.net/2023/Apr/12/code-interpreter/">nearly three years ago</a>, was half-heartedly rebranded to "Advanced Data Analysis" at some point and is generally really difficult to find detailed documentation about. Case in point: it appears to have had a <em>massive</em> upgrade at some point in the past few months, and I can't find documentation about the new capabilities anywhere!</p>
<p>Here are the most notable new features:</p>
<ol>
<li>ChatGPT can <strong>directly run Bash commands</strong> now. Previously it was limited to Python code only, although it could run shell commands via the Python <code>subprocess</code> module.</li>
<li>
<strong>It has Node.js</strong> and can run JavaScript directly in addition to Python. I also got it to run "hello world" in <strong>Ruby, Perl, PHP, Go, Java, Swift, Kotlin, C and C++</strong>. No Rust yet though!</li>
<li>While the container still can't make outbound network requests, <strong><code>pip install package</code> and <code>npm install package</code> both work</strong> now via a custom proxy mechanism.</li>
<li>ChatGPT can locate the URL for a file on the web and use a <code>container.download</code> tool to <strong>download that file and save it to a path</strong> within the sandboxed container.</li>
</ol>
<p>This is a substantial upgrade! ChatGPT can now write and then test code in 10 new languages (11 if you count Bash), can find files online and download them into the container, and can install additional packages via <code>pip</code> and <code>npm</code> to help it solve problems.</p>
<p>(OpenAI <em>really</em> need to develop better habits at <a href="https://help.openai.com/en/articles/6825453-chatgpt-release-notes">keeping their release notes up-to-date</a>!)</p>
<p>I was initially suspicious that maybe I'd stumbled into a new preview feature that wasn't available to everyone, but I <a href="https://chatgpt.com/share/6977aa7c-7bd8-8006-8129-8c9e25126fed">tried some experiments</a> in a free ChatGPT account and confirmed that the new features are available there as well.</p>

<ul>
  <li><a href="https://simonwillison.net/2026/Jan/26/chatgpt-containers/#container-download">container.download</a></li>
  <li><a href="https://simonwillison.net/2026/Jan/26/chatgpt-containers/#is-container-download-a-data-exfiltration-vulnerability-">Is container.download a data exfiltration vulnerability?</a></li>
  <li><a href="https://simonwillison.net/2026/Jan/26/chatgpt-containers/#bash-and-other-languages">Bash and other languages</a></li>
  <li><a href="https://simonwillison.net/2026/Jan/26/chatgpt-containers/#installing-packages-from-pip-and-npm">Installing packages from pip and npm</a></li>
  <li><a href="https://simonwillison.net/2026/Jan/26/chatgpt-containers/#adding-it-all-together">Adding it all together</a></li>
</ul>

<h4 id="container-download">container.download</h4>
<p>My first clue to the new features came the other day when I got curious about Los Angeles air quality, in particular has the growing number of electric vehicles there had a measurable impact?</p>
<p>I prompted a fresh GPT-5.2 Thinking session with:</p>
<blockquote>
<p>Show me Los Angeles air quality over time for last 20 years</p>
</blockquote>
<p>Here's the <a href="https://chatgpt.com/share/6977a80d-e6e4-8006-b4fe-ed8478fe99d8">shared transcript</a>. Annoyingly that doesn't make the thinking trace available, so here's a screenshot:</p>
<p><img src="https://static.simonwillison.net/static/2026/container-download.jpg" alt="Exploring methods to fetch AQI data and reports Downloading AQI data for Los Angeles  I found an xlsx file titled &quot;Air Quality Statistics by City, 2000-2023&quot;, which should include data for Los Angeles. While user requested data for Using provided URL to download file  The URL from the error message leads to an Excel file containing air quality trends. I can use the container.download tool to fetch it. Let‚Äôs proceed with this approach and see if it works out for downloading the file directly! Analyzed Parsing and analyzing the data for Los Angeles  Now that we've downloaded the data, I'll parse it using Python. First, I'll focus on identifying the row(s) corresponding to Los Angeles, which may be labeled under &quot;Los Angeles-Long Beach-Anaheim, CA.&quot; Afterward, I'll plot metrics like &quot;Median AQI&quot; and &quot;Days Good&quot; over time to visualize trends. Let‚Äôs begin!" style="max-width: 100%;" /></p>
<p>This bit instantly caught my eye:</p>
<blockquote>
<p>[...] The URL from the error message leads to an Excel file containing air quality trends. I can use the container.download tool to fetch it. [...]</p>
</blockquote>
<p>What's <code>container.download</code>? I <a href="https://chatgpt.com/share/6977aab0-b874-8006-9b54-d3d216ed884b">asked ChatGPT</a>:</p>
<blockquote>
<p>Without running any searches tell me about your container.download tool</p>
</blockquote>
<p>And got back:</p>
<blockquote>
<p>Sure. <code>container.download</code> is a built-in tool I can call that <strong>fetches a file from a URL and saves it into the sandboxed container filesystem</strong> for this chat session.</p>
<h4 id="what-it-does">What it does</h4>
<ul>
<li>Takes a <strong>publicly reachable URL</strong> and a <strong>destination filepath</strong> in the container.</li>
<li>Downloads the bytes from that URL and writes them to the given path.</li>
<li>After that, I can read/process the file locally in the container (e.g., unzip it, parse it with Python, open it as an image, convert it, etc.).</li>
</ul>
<h4 id="signature-inputs-">Signature (inputs)</h4>
<p>It‚Äôs called like this (conceptually):</p>
<div class="highlight highlight-source-json"><pre>{
  <span class="pl-ent">"url"</span>: <span class="pl-s"><span class="pl-pds">"</span>https://example.com/somefile.zip<span class="pl-pds">"</span></span>,
  <span class="pl-ent">"filepath"</span>: <span class="pl-s"><span class="pl-pds">"</span>/mnt/data/somefile.zip<span class="pl-pds">"</span></span>
}</pre></div>
</blockquote>
<p>Further experimentation confirmed that <code>container.download</code> behaves as described.</p>
<p>I had it download one file from a server with logs enabled and captured the IP address and user-agent. Here are the most interesting headers:</p>
<pre><code>User-Agent: Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko); compatible; ChatGPT-User/1.0; +https://openai.com/bot
Accept: text/html, application/xhtml+xml, application/xml;q=0.9, image/avif, image/webp, image/apng, */*;q=0.8, application/signed-exchange;v=b3;q=0.9
Cf-Connecting-Ip: 52.230.164.178
</code></pre>
<p>That <code>52.230.164.178</code> IP address <a href="http://ip-api.com/json/52.230.164.178">resolves to</a> Microsoft Azure Cloud (centralus) in Des Moines, Iowa.</p>
<h4 id="is-container-download-a-data-exfiltration-vulnerability-">Is container.download a data exfiltration vulnerability?</h4>
<p>On the one hand, this is really useful! ChatGPT can navigate around websites looking for useful files, download those files to a container and then process them using Python or other languages.</p>
<p>Is this a data exfiltration vulnerability though? Could a prompt injection attack trick ChatGPT into leaking private data out to a <code>container.download</code> call to a URL with a query string that includes sensitive information?</p>
<p>I don't think it can. I tried getting it to assemble a URL with a query string and access it using <code>container.download</code> and it couldn't do it. It told me that it got back this error:</p>
<blockquote>
<p>ERROR: download failed because url not viewed in conversation before. open the file or url using web.run first.</p>
</blockquote>
<p>This looks to me like the same safety trick <a href="https://simonwillison.net/2025/Sep/10/claude-web-fetch-tool/">used by Claude's Web Fetch tool</a>: only allow URL access if that URL was either directly entered by the user or if it came from search results that could not have been influenced by a prompt injection.</p>
<p>(I poked at this a bit more and managed to get a simple constructed query string to pass through <code>web.run</code> - a different tool entirely - but when I tried to compose a longer query string containing the previous prompt history a <code>web.run</code> filter blocked it.)</p>
<p>So I <em>think</em> this is all safe, though I'm curious if it could hold firm against a more aggressive round of attacks from a seasoned security researcher.</p>
<h4 id="bash-and-other-languages">Bash and other languages</h4>
<p>The key lesson from coding agents like Claude Code and Codex CLI is that Bash rules everything: if an agent can run Bash commands in an environment it can do almost anything that can be achieved by typing commands into a computer.</p>
<p>When Anthropic added their own code interpreter feature to Claude <a href="https://simonwillison.net/2025/Sep/9/claude-code-interpreter/">last September</a> they built that around Bash rather than just Python. It looks to me like OpenAI have now done the same thing for ChatGPT.</p>
<p>Here's what ChatGPT looks like when it runs a Bash command - here my prompt was:</p>
<blockquote>
<p>npm install a fun package and demonstrate using it</p>
</blockquote>
<p><img src="https://static.simonwillison.net/static/2026/chatgpt-bash.jpg" alt="Screenshot of ChatGPT 5.2 Thinking interface with Activity panel. Main chat shows user prompt &quot;npm install a fun package and demonstrate using it&quot; and response &quot;Thought for 32s &gt; Yep ‚Äî here's a fun one: cowsay üêÑ I installed it with npm and ran a tiny Node script:&quot; followed by bash code block containing &quot;mkdir -p /mnt/data/npmfun cd /mnt/data/npmfun npm init -y npm install cowsay --no-fund --no-audit node -e &quot;const cowsay=require('cowsay'); console.log(cowsay.say({text:'Hel&quot; and Output: section showing ASCII cow art saying &quot;&lt; Hello from npm inside the container! &gt;&quot; with cow drawing made of characters. Activity panel on right (32s) shows: &quot;Running npm install in container&quot; with bash command and output &quot;v22.16.0 10.9.2&quot;, &quot;Inspecting npm command output and container status&quot; with directory listing showing &quot;drwxr-xr-x 2 root oai_shared 80 Jan 26 17:32 . drwxrwxr-x 2 oai oai_shared 60 Jan 26 17:32 .. -rw-r--r-- 1 root root&quot;, &quot;Re-running npm install&quot; note about node_modules missing, and &quot;Rerunning npm install with output&quot; showing &quot;added 41 packages in 2s&quot;." style="max-width: 100%;" /></p>
<p>It's useful to click on the "Thinking" or "Thought for 32s" links as that opens the Activity sidebar with a detailed trace of what ChatGPT did to arrive at its answer. This helps guard against cheating - ChatGPT might claim to have run Bash in the main window but it can't fake those black and white logs in the Activity panel.</p>
<p>I had it run <a href="https://chatgpt.com/share/6977b890-f1c0-8006-b39a-9a02c7ca1993">Hello World in various languages</a> later in that same session.</p>
<p><img src="https://static.simonwillison.net/static/2026/chatgpt-hello-worlds.jpg" alt="Terminal screenshot showing &quot;Hello, world&quot; programs in multiple languages. Top section shows code: swift -e 'print(&quot;Hello, world from Swift!&quot;)' followed by cat &gt; hello.kts &lt;&lt;'KTS' / println(&quot;Hello, world from Kotlin!&quot;) / KTS / kotlinc -script hello.kts, then cat &gt; hello.c &lt;&lt;'C' / #include &lt;stdio.h&gt; / int main(){ puts(&quot;Hello, world from C!&quot;); return 0; } / C / gcc hello.c -o hello_c / ./hello_c, then cat &gt; hello.cpp &lt;&lt;'CPP' / #include &lt;iostream&gt; / int main(){ std::cout &lt;&lt; &quot;Hello, world from C++!&quot; &lt;&lt; std::endl; } / CPP / g++ hello.cpp -o hello_cpp / ./hello_cpp. Bottom section shows output: Hello, world from Ruby! / Hello, world from Perl! / Hello, world from PHP! / Hello, world from Go! / Hello, world from Java! / Hello, world from Swift! / Hello, world from Kotlin! / Hello, world from C! / Hello, world from C++!. UI shows &quot;Thought for 2m 29s&quot; and &quot;Done&quot; at bottom." style="max-width: 100%;" /></p>
<h4 id="installing-packages-from-pip-and-npm">Installing packages from pip and npm</h4>
<p>In the previous example ChatGPT installed the <code>cowsay</code> package from npm and used it to draw an ASCII-art cow. But how could it do that if the container can't make outbound network requests?</p>
<p>In <a href="https://chatgpt.com/share/69773501-b6d8-8006-bbf2-fa644561aa26">another session</a> I challenged it to explore its environment. and figure out how that worked.</p>
<p>Here's <a href="https://github.com/simonw/research/blob/main/chatgpt-container-environment/README.md">the resulting Markdown report</a> it created.</p>
<p>The key magic appears to be a <code>applied-caas-gateway1.internal.api.openai.org</code> proxy, available within the container and with various packaging tools configured to use it.</p>
<p>The following environment variables cause <code>pip</code> and <code>uv</code> to install packages from that proxy instead of directly from PyPI:</p>
<pre><code>PIP_INDEX_URL=https://reader:****@packages.applied-caas-gateway1.internal.api.openai.org/.../pypi-public/simple
PIP_TRUSTED_HOST=packages.applied-caas-gateway1.internal.api.openai.org
UV_INDEX_URL=https://reader:****@packages.applied-caas-gateway1.internal.api.openai.org/.../pypi-public/simple
UV_INSECURE_HOST=https://packages.applied-caas-gateway1.internal.api.openai.org
</code></pre>
<p>This one appears to get <code>npm</code> to work:</p>
<pre><code>NPM_CONFIG_REGISTRY=https://reader:****@packages.applied-caas-gateway1.internal.api.openai.org/.../npm-public
</code></pre>
<p>And it reported these suspicious looking variables as well:</p>
<pre><code>CAAS_ARTIFACTORY_BASE_URL=packages.applied-caas-gateway1.internal.api.openai.org
CAAS_ARTIFACTORY_PYPI_REGISTRY=.../artifactory/api/pypi/pypi-public
CAAS_ARTIFACTORY_NPM_REGISTRY=.../artifactory/api/npm/npm-public
CAAS_ARTIFACTORY_GO_REGISTRY=.../artifactory/api/go/golang-main
CAAS_ARTIFACTORY_MAVEN_REGISTRY=.../artifactory/maven-public
CAAS_ARTIFACTORY_GRADLE_REGISTRY=.../artifactory/gradle-public
CAAS_ARTIFACTORY_CARGO_REGISTRY=.../artifactory/api/cargo/cargo-public/index
CAAS_ARTIFACTORY_DOCKER_REGISTRY=.../dockerhub-public
CAAS_ARTIFACTORY_READER_USERNAME=reader
CAAS_ARTIFACTORY_READER_PASSWORD=****
NETWORK=caas_packages_only
</code></pre>
<p>Neither Rust nor Docker are installed in the container environment, but maybe those registry references are a clue of features still to come.</p>
<h4 id="adding-it-all-together">Adding it all together</h4>
<p>The result of all of this? You can tell ChatGPT to use Python or Node.js packages as part of a conversation and it will be able to install them and apply them against files you upload or that it downloads from the public web. That's <em>really</em> cool.</p>
<p>The big missing feature here should be the easiest to provide: we need <strong>official documentation</strong>! A release notes entry would be a good start, but there are a lot of subtle details to how this new stuff works, its limitations and what it can be used for.</p>
<p>As always, I'd also encourage OpenAI to come up with a name for this set of features that properly represents how it works and what it can do.</p>
<p>In the meantime, I'm going to call this <strong>ChatGPT Containers</strong>.</p>
<h4 id="list-of-tools">Update: a full list of tools</h4>
<p>I decided to ask ChatGPT about other tools that were available to it in case there was anything interesting in there:</p>
<blockquote><p>List all tools that are available to you, with their exact names and descriptions and signatures</p></blockquote>
<p>Here's <a href="https://chatgpt.com/share/6977ffa0-df14-8006-9647-2b8c90ccbb81">what I got back</a>.</p>

<details><summary>List of ChatGPT tools according to GPT-5.2 Thinking on 26th January 2026</summary>
<p style="margin-top: 1em">Below are <strong>all tools available to me in this chat</strong>, with their <strong>exact names</strong>, <strong>descriptions</strong>, and <strong>signatures</strong> (as defined in my tool interface).</p>
<hr />
<h4 id="python">python</h4>
<h5 id="python-exec-freeform-any">
<code>python.exec</code> (FREEFORM) ‚Üí any</h5>
<p><strong>Description:</strong> Execute Python code in a Jupyter environment for <em>private/internal reasoning</em> (not shown to you). Internet access is disabled.<br />
<strong>Signature:</strong> <code>python.exec(FREEFORM) -&gt; any</code></p>
<hr />
<h4 id="web">web</h4>
<h5 id="web-run-toolcallv5-json-any">
<code>web.run</code> (ToolCallV5 JSON) ‚Üí any</h5>
<p><strong>Description:</strong> Access the internet and specialized lookups (search, open/click/find pages, screenshot PDFs, image search, product search, finance, weather, sports, calculator, time). Supports citations via returned <code>turn...</code> reference IDs.<br />
<strong>Signature:</strong><br />
<code>web.run({ open?, click?, find?, screenshot?, image_query?, product_query?, sports?, finance?, weather?, calculator?, time?, search_query?, response_length? }) -&gt; any</code></p>
<hr />
<h4 id="automations">automations</h4>
<h5 id="automations-create-any">
<code>automations.create</code> ‚Üí any</h5>
<p><strong>Description:</strong> Schedule a future/recurring task/reminder/search via iCal VEVENT.<br />
<strong>Signature:</strong><br />
<code>automations.create({ prompt: string, title: string, schedule?: string, dtstart_offset_json?: string }) -&gt; any</code></p>
<h5 id="automations-update-any">
<code>automations.update</code> ‚Üí any</h5>
<p><strong>Description:</strong> Modify an existing scheduled task.<br />
<strong>Signature:</strong><br />
<code>automations.update({ jawbone_id: string, schedule?: string, dtstart_offset_json?: string, prompt?: string, title?: string, is_enabled?: boolean }) -&gt; any</code></p>
<h5 id="automations-list-any">
<code>automations.list</code> ‚Üí any</h5>
<p><strong>Description:</strong> List existing automations.<br />
<strong>Signature:</strong> <code>automations.list() -&gt; any</code></p>
<hr />
<h4 id="guardian-tool">guardian_tool</h4>
<h5 id="guardian-tool-get-policy-str">
<code>guardian_tool.get_policy</code> ‚Üí str</h5>
<p><strong>Description:</strong> Retrieve policy guidance for specific categories (currently: <code>election_voting</code>).<br />
<strong>Signature:</strong> <code>guardian_tool.get_policy({ category: string }) -&gt; str</code></p>
<hr />
<h4 id="file-search">file_search</h4>
<h5 id="file-search-msearch-any">
<code>file_search.msearch</code> ‚Üí any</h5>
<p><strong>Description:</strong> Search across available internal/connected sources (in this environment: primarily ‚Äúrecording_knowledge‚Äù transcripts when relevant).<br />
<strong>Signature:</strong><br />
<code>file_search.msearch({ queries?: string[], source_filter?: string[], file_type_filter?: string[], intent?: string, time_frame_filter?: { start_date?: string, end_date?: string } }) -&gt; any</code></p>
<h5 id="file-search-mclick-any">
<code>file_search.mclick</code> ‚Üí any</h5>
<p><strong>Description:</strong> Expand/open previously returned search results (or connector URLs) for more context.<br />
<strong>Signature:</strong><br />
<code>file_search.mclick({ pointers?: string[], start_date?: string, end_date?: string }) -&gt; any</code></p>
<hr />
<h4 id="gmail-read-only-">gmail (read-only)</h4>
<h5 id="gmail-search-email-ids-any">
<code>gmail.search_email_ids</code> ‚Üí any</h5>
<p><strong>Description:</strong> Search Gmail message IDs by query/tags (read-only).<br />
<strong>Signature:</strong><br />
<code>gmail.search_email_ids({ query?: string, tags?: string[], max_results?: integer, next_page_token?: string }) -&gt; any</code></p>
<h5 id="gmail-batch-read-email-any">
<code>gmail.batch_read_email</code> ‚Üí any</h5>
<p><strong>Description:</strong> Read a batch of Gmail messages by message IDs (read-only).<br />
<strong>Signature:</strong><br />
<code>gmail.batch_read_email({ message_ids: string[] }) -&gt; any</code></p>
<hr />
<h4 id="gcal-read-only-">gcal (read-only)</h4>
<h5 id="gcal-search-events-any">
<code>gcal.search_events</code> ‚Üí any</h5>
<p><strong>Description:</strong> Search Google Calendar events in a time range and/or by keyword (read-only).<br />
<strong>Signature:</strong><br />
<code>gcal.search_events({ time_min?: string, time_max?: string, timezone_str?: string, max_results?: integer, query?: string, calendar_id?: string, next_page_token?: string }) -&gt; any</code></p>
<h5 id="gcal-read-event-any">
<code>gcal.read_event</code> ‚Üí any</h5>
<p><strong>Description:</strong> Read a specific calendar event by ID (read-only).<br />
<strong>Signature:</strong><br />
<code>gcal.read_event({ event_id: string, calendar_id?: string }) -&gt; any</code></p>
<hr />
<h4 id="gcontacts-read-only-">gcontacts (read-only)</h4>
<h5 id="gcontacts-search-contacts-any">
<code>gcontacts.search_contacts</code> ‚Üí any</h5>
<p><strong>Description:</strong> Search Google Contacts by keyword (read-only).<br />
<strong>Signature:</strong><br />
<code>gcontacts.search_contacts({ query: string, max_results?: integer }) -&gt; any</code></p>
<hr />
<h4 id="canmore-canvas-">canmore (canvas)</h4>
<h5 id="canmore-create-textdoc-any">
<code>canmore.create_textdoc</code> ‚Üí any</h5>
<p><strong>Description:</strong> Create a new canvas text document (markdown or code).<br />
<strong>Signature:</strong><br />
<code>canmore.create_textdoc({ name: string, type: "document" | "code/bash" | "code/zsh" | "code/javascript" | "code/typescript" | "code/html" | "code/css" | "code/python" | "code/json" | "code/sql" | "code/go" | "code/yaml" | "code/java" | "code/rust" | "code/cpp" | "code/swift" | "code/php" | "code/xml" | "code/ruby" | "code/haskell" | "code/kotlin" | "code/csharp" | "code/c" | "code/objectivec" | "code/r" | "code/lua" | "code/dart" | "code/scala" | "code/perl" | "code/commonlisp" | "code/clojure" | "code/ocaml" | "code/powershell" | "code/verilog" | "code/dockerfile" | "code/vue" | "code/react" | "code/other", content: string }) -&gt; any</code></p>
<h5 id="canmore-update-textdoc-any">
<code>canmore.update_textdoc</code> ‚Üí any</h5>
<p><strong>Description:</strong> Apply regex-based edits to the current canvas textdoc.<br />
<strong>Signature:</strong><br />
<code>canmore.update_textdoc({ updates: { pattern: string, multiple?: boolean, replacement: string }[] }) -&gt; any</code></p>
<h5 id="canmore-comment-textdoc-any">
<code>canmore.comment_textdoc</code> ‚Üí any</h5>
<p><strong>Description:</strong> Add actionable comments on matched regions of the current canvas textdoc.<br />
<strong>Signature:</strong><br />
<code>canmore.comment_textdoc({ comments: { pattern: string, comment: string }[] }) -&gt; any</code></p>
<hr />
<h4 id="python-user-visible">python_user_visible</h4>
<h5 id="python-user-visible-exec-freeform-any">
<code>python_user_visible.exec</code> (FREEFORM) ‚Üí any</h5>
<p><strong>Description:</strong> Execute Python code <em>that you will see</em> (tables, plots, generated files). Internet disabled.<br />
<strong>Signature:</strong> <code>python_user_visible.exec(FREEFORM) -&gt; any</code></p>
<hr />
<h4 id="user-info">user_info</h4>
<h5 id="user-info-get-user-info-any">
<code>user_info.get_user_info</code> ‚Üí any</h5>
<p><strong>Description:</strong> Get your current location and local time (or UTC if unknown).<br />
<strong>Signature:</strong> <code>user_info.get_user_info({}) -&gt; any</code></p>
<hr />
<h4 id="summary-reader">summary_reader</h4>
<h5 id="summary-reader-read-any">
<code>summary_reader.read</code> ‚Üí any</h5>
<p><strong>Description:</strong> Read safe-to-share internal reasoning summaries from earlier turns (used when asked how I arrived at something).<br />
<strong>Signature:</strong> <code>summary_reader.read({ limit?: integer, offset?: integer }) -&gt; any</code></p>
<hr />
<h4 id="container">container</h4>
<h5 id="container-exec-any">
<code>container.exec</code> ‚Üí any</h5>
<p><strong>Description:</strong> Run a command inside the container and return stdout/stderr.<br />
<strong>Signature:</strong><br />
<code>container.exec({ cmd: string[], session_name?: string | null, workdir?: string | null, timeout?: integer | null, env?: object | null, user?: string | null }) -&gt; any</code></p>
<h5 id="container-feed-chars-any">
<code>container.feed_chars</code> ‚Üí any</h5>
<p><strong>Description:</strong> Send input to a running interactive exec session and flush output.<br />
<strong>Signature:</strong><br />
<code>container.feed_chars({ session_name: string, chars: string, yield_time_ms?: integer }) -&gt; any</code></p>
<h5 id="container-open-image-any">
<code>container.open_image</code> ‚Üí any</h5>
<p><strong>Description:</strong> Display an image from an absolute path in the container (jpg/jpeg/png/webp).<br />
<strong>Signature:</strong><br />
<code>container.open_image({ path: string, user?: string | null }) -&gt; any</code></p>
<h5 id="container-download-any">
<code>container.download</code> ‚Üí any</h5>
<p><strong>Description:</strong> Download a file from a URL into the container filesystem.<br />
<strong>Signature:</strong><br />
<code>container.download({ url: string, filepath: string }) -&gt; any</code></p>
<hr />
<h4 id="bio-memory-">bio (memory)</h4>
<h5 id="bio-update-freeform-any">
<code>bio.update</code> (FREEFORM) ‚Üí any</h5>
<p><strong>Description:</strong> Persist (or forget) user-specific info across conversations (‚Äúmemory‚Äù).<br />
<strong>Signature:</strong> <code>bio.update(FREEFORM) -&gt; any</code></p>
<hr />
<h4 id="api-tool">api_tool</h4>
<h5 id="api-tool-list-resources-any">
<code>api_tool.list_resources</code> ‚Üí any</h5>
<p><strong>Description:</strong> List available ‚Äúresources‚Äù (including invokable tools) in an OP-style file tree.<br />
<strong>Signature:</strong><br />
<code>api_tool.list_resources({ path?: string, cursor?: string | null, only_tools?: boolean, refetch_tools?: boolean }) -&gt; any</code></p>
<h5 id="api-tool-call-tool-any">
<code>api_tool.call_tool</code> ‚Üí any</h5>
<p><strong>Description:</strong> Invoke a resource as a tool (must know its full path/URI from <code>list_resources</code>).<br />
<strong>Signature:</strong><br />
<code>api_tool.call_tool({ path: string, args: object }) -&gt; any</code></p>
<hr />
<h4 id="personal-context">personal_context</h4>
<h5 id="personal-context-search-any">
<code>personal_context.search</code> ‚Üí any</h5>
<p><strong>Description:</strong> Retrieve user-specific context from prior interactions/linked sources to maintain continuity.<br />
<strong>Signature:</strong><br />
<code>personal_context.search({ query: string }) -&gt; any</code></p>
<hr />
<h4 id="image-gen">image_gen</h4>
<h5 id="image-gen-text2im-any">
<code>image_gen.text2im</code> ‚Üí any</h5>
<p><strong>Description:</strong> Generate or edit images from text instructions (and optionally referenced image IDs).<br />
<strong>Signature:</strong><br />
<code>image_gen.text2im({ prompt?: string | null, size?: string | null, n?: integer | null, transparent_background?: boolean | null, is_style_transfer?: boolean | null, referenced_image_ids?: string[] | null }) -&gt; any</code></p>
<hr />
<h4 id="user-settings">user_settings</h4>
<h5 id="user-settings-get-user-settings-any">
<code>user_settings.get_user_settings</code> ‚Üí any</h5>
<p><strong>Description:</strong> Read current settings and allowed values (personality, accent color, appearance).<br />
<strong>Signature:</strong> <code>user_settings.get_user_settings() -&gt; any</code></p>
<h5 id="user-settings-set-setting-any">
<code>user_settings.set_setting</code> ‚Üí any</h5>
<p><strong>Description:</strong> Change one setting (accent color, appearance, or personality) to an allowed value.<br />
<strong>Signature:</strong><br />
<code>user_settings.set_setting({ setting_name: "accent_color" | "appearance" | "personality", setting_value: string }) -&gt; any</code></p>
<hr />
<h4 id="artifact-handoff">artifact_handoff</h4>
<h5 id="artifact-handoff-prepare-artifact-generation-any">
<code>artifact_handoff.prepare_artifact_generation</code> ‚Üí any</h5>
<p><strong>Description:</strong> Must be called immediately when the user asks for a spreadsheet or slide deck artifact.<br />
<strong>Signature:</strong> <code>artifact_handoff.prepare_artifact_generation() -&gt; any</code></p>
</details>
    
        <p>Tags: <a href="https://simonwillison.net/tags/pypi">pypi</a>, <a href="https://simonwillison.net/tags/sandboxing">sandboxing</a>, <a href="https://simonwillison.net/tags/npm">npm</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/chatgpt">chatgpt</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/code-interpreter">code-interpreter</a></p>]]></description><pubDate>Mon, 26 Jan 2026 19:19:31 +0000</pubDate></item><item><title>the browser is the sandbox</title><link>https://simonwillison.net/2026/Jan/25/the-browser-is-the-sandbox/#atom-everything</link><description><![CDATA[<p><strong><a href="https://aifoc.us/the-browser-is-the-sandbox/">the browser is the sandbox</a></strong></p>
Paul Kinlan is a web platform developer advocate at Google and recently turned his attention to coding agents. He quickly identified the importance of a robust sandbox for agents to operate in and put together these detailed notes on how the web browser can help:</p>
<blockquote>
<p>This got me thinking about the browser. Over the last 30 years, we have built a sandbox specifically designed to run incredibly hostile, untrusted code from anywhere on the web, the instant a user taps a URL. [...]</p>
<p>Could you build something like Cowork in the browser? Maybe. To find out, I built a demo called <a href="http://co-do.xyz">Co-do</a> that tests this hypothesis. In this post I want to discuss the research I've done to see how far we can get, and determine if the browser's ability to run untrusted code is useful (and good enough) for enabling software to do more for us directly on our computer.</p>
</blockquote>
<p>Paul then describes how the three key aspects of a sandbox - filesystem, network access and safe code execution - can be handled by browser technologies: the <a href="https://developer.chrome.com/docs/capabilities/web-apis/file-system-access">File System Access API</a> (still Chrome-only as far as I can tell), CSP headers with <code>&lt;iframe sandbox&gt;</code> and WebAssembly in Web Workers.</p>
<p>Co-do is a very interesting demo that illustrates all of these ideas in a single application:</p>
<p><img alt="Screenshot of Co-do application interface with robot logo. Left sidebar shows WORKSPACE section with &quot;Select Folder&quot; button, &quot;Selected folder: blog-drafts (Live updates enabled)&quot; in green text, and FILES list including gtr-t5-large.md, chatgpt.md, llm-digest-october-2..., weeknotes-2024-04-23..., lmarena-april-2025.md, weeknotes-chatgpt-fo..., tests-not-optional-c..., weeknotes-13-mar-202..., issue-for-notes.md, workshop-git-scrapin.... Main chat area shows user message &quot;Three most recently edited files?&quot; in orange bubble, &quot;Using 26 tools...&quot; dropdown, and AI response: &quot;Now I need to get the metadata for these files to find the most recently edited ones. Since there are many files, let me get metadata for a sample of them to find the recent ones:Based on the metadata I've gathered, the three most recently edited files are:&quot; followed by table with columns File and Last Modified showing predictions-2026.md (Jan 8, 2026), december-2025.md (Jan 2, 2026), gemini-3-flash.md (Dec 17, 2025). Green &quot;Response complete&quot; banner at bottom with input field placeholder &quot;Ask me to help with your files...&quot; and notice &quot;File contents are sent to your selected AI provider&quot;." src="https://static.simonwillison.net/static/2026/codo.jpg" /></p>
<p>You select a folder full of files and configure an LLM provider and set an API key, Co-do then uses CSP-approved API calls to interact with that provider and provides a chat interface with tools for interacting with those files. It does indeed feel similar to <a href="https://simonwillison.net/2026/Jan/12/claude-cowork/">Claude Cowork</a> but without running a multi-GB local container to provide the sandbox.</p>
<p>My biggest complaint about <code>&lt;iframe sandbox&gt;</code> remains how thinly documented it is, especially across different browsers. Paul's post has all sorts of useful details on that which I've not encountered elsewhere, including a complex <a href="https://aifoc.us/the-browser-is-the-sandbox/#the-double-iframe-technique">double-iframe technique</a> to help apply network rules to the inner of the two frames.</p>
<p>Thanks to this post I also learned about the <code>&lt;input type="file" webkitdirectory&gt;</code> tag which turns out to work on Firefox, Safari <em>and</em> Chrome and allows a browser read-only access to a full directory of files at once. I had Claude knock up a <a href="https://tools.simonwillison.net/webkitdirectory">webkitdirectory demo</a> to try it out and I'll certainly be using it for projects in the future.</p>
<p><img alt="Screenshot of a dark-themed file explorer application. Top stats bar shows: 12179 TOTAL FILES, 2079 FOLDERS, 244 MB TOTAL SIZE, 97 FILE TYPES in cyan text. Search bar with placeholder &quot;Search files...&quot; and &quot;All types&quot; dropdown. Left panel labeled &quot;File tree&quot; (showing 12179) displays folder hierarchy: datasette &gt; .claude &gt; skills &gt; building-datasette-plugins containing HOOKS.md (10.7 KB, selected/highlighted), INTERNALS.md (10.1 KB), SKILL.md (3.7 KB), TESTING.md (8.4 KB), settings.local.json (280 B); also shows .eggs folder with pytest_runner-6.0.1-py3.9.egg. Right panel &quot;File preview&quot; shows selected file details: Name: HOOKS.md, Path: datasette/.claude/skills/building-datasette-plugins/HOOKS.md, Size: 10.7 KB, Type: text/markdown, Last modified: 12/20/2025, 9:28:59 AM. Preview content shows: &quot;# Plugin Hooks Reference&quot; followed by &quot;All hooks use the @hookimpl decorator. Accept only the parameters you need.&quot; then &quot;## Database Connection Hooks&quot; and &quot;### prepare_connection(conn, database, datasette)&quot; with description &quot;Called when a new SQLite connection is created. Use to register custom SQL functions.&quot; Bottom section &quot;File type distribution&quot; shows horizontal bar chart: .py (4439), .no ext (3358), .dat (1068), .pyc (925), .txt (332), .mo (321), .po (321), .html (249)." src="https://static.simonwillison.net/static/2026/webkit-file-tree.jpg" />


    <p>Tags: <a href="https://simonwillison.net/tags/browsers">browsers</a>, <a href="https://simonwillison.net/tags/javascript">javascript</a>, <a href="https://simonwillison.net/tags/sandboxing">sandboxing</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-agents">ai-agents</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a></p>]]></description><pubDate>Sun, 25 Jan 2026 23:51:32 +0000</pubDate></item><item><title>KƒÅkƒÅp≈ç Cam: Rakiura live stream</title><link>https://simonwillison.net/2026/Jan/25/kakapo-cam/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.doc.govt.nz/our-work/kakapo-recovery/what-we-do/kakapo-cam-rakiura-live-stream/">KƒÅkƒÅp≈ç Cam: Rakiura live stream</a></strong></p>
Critical update for this year's <a href="https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-k-k-p-parrots-will-have-an-outstanding-breeding-season">KƒÅkƒÅp≈ç breeding season</a>: the New Zealand Department of Conservation have a livestream running of Rakiura's nest!</p>
<blockquote>
<p>You‚Äôre looking at the underground nest of 23-year-old Rakiura. She has chosen this same site to nest for all seven breeding seasons since 2008, a large cavity under a rƒÅtƒÅ tree. Because she returns to the site so reliably, we‚Äôve been able to make modifications over the years to keep it safe and dry, including adding a well-placed hatch for monitoring eggs and chicks.</p>
</blockquote>
<p>Rakiura is a legendary KƒÅkƒÅp≈ç:</p>
<blockquote>
<p>Rakiura hatched on 19 February 2002 on Whenua Hou/Codfish Island. She is the offspring of Flossie and Bill. Her name comes from the te reo MƒÅori name for Stewart Island, the place where most of the founding kƒÅkƒÅp≈ç population originated.</p>
<p>Rakiura has nine living descendants, three females and six males, across six breeding seasons. In 2008 came T≈çitiiti, in 2009 Tamahou and Te Atap≈ç, in 2011 Tia and T≈´toko, in 2014 Taeatanga and Te Awa, in 2019 Mati-mƒÅ and Tautahi. She also has many grandchicks.</p>
</blockquote>
<p>She laid her first egg of the season at 4:30pm NZ time on 22nd January. The livestream went live shortly afterwards, once she committed to this nest.</p>
<p>The stream is <a href="https://www.youtube.com/watch?v=BfGL7A2YgUY">on YouTube</a>. I <a href="https://gisthost.github.io/?dc78322de89a2191c593215f109c65d7/index.html">used Claude Code</a> to write <a href="https://tools.simonwillison.net/python/#livestream-gifpy">a livestream-gif.py script</a> and used that to capture this sped-up video of the last few hours of footage, within which you can catch a glimpse of the egg!</p>
<video autoplay muted loop controls playsinline style="width: 100%;">
  <source src="https://static.simonwillison.net/static/2026/kakapo-timelapse.mp4" type="video/mp4">
</video>

    <p><small></small>Via <a href="https://www.metafilter.com/211927/The-only-parrot-to-have-a-polygynous-lek-breeding-system-sits-on-an-egg">MetaFilter</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/youtube">youtube</a>, <a href="https://simonwillison.net/tags/kakapo">kakapo</a>, <a href="https://simonwillison.net/tags/conservation">conservation</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a></p>]]></description><pubDate>Sun, 25 Jan 2026 04:53:01 +0000</pubDate></item><item><title>Don&apos;t &quot;Trust the Process&quot;</title><link>https://simonwillison.net/2026/Jan/24/dont-trust-the-process/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.youtube.com/watch?v=4u94juYwLLM">Don&#x27;t &quot;Trust the Process&quot;</a></strong></p>
Jenny Wen, Design Lead at Anthropic (and previously Director of Design at Figma) gave a provocative keynote at Hatch Conference in Berlin last September.</p>
<p><img alt="Don't &quot;Trust the process&quot; slide, speaker shown on the left" src="https://static.simonwillison.net/static/2026/dont-trust-process.jpg" /></p>
<p>Jenny argues that the Design Process - user research leading to personas leading to user journeys leading to wireframes... all before anything gets built - may be outdated for today's world.</p>
<blockquote>
<p><strong>Hypothesis</strong>: In a world where anyone can make anything ‚Äî what matters is your ability to choose and curate what you make.</p>
</blockquote>
<p>In place of the Process, designers should lean into prototypes. AI makes these much more accessible and less time-consuming than they used to be.</p>
<p>Watching this talk made me think about how AI-assisted programming significantly reduces the cost of building the <em>wrong</em> thing. Previously if the design wasn't right you could waste months of development time building in the wrong direction, which was a very expensive mistake. If a wrong direction wastes just a few days instead we can take more risks and be much more proactive in exploring the problem space.</p>
<p>I've always been a compulsive prototyper though, so this is very much playing into my own existing biases!

    <p><small></small>Via <a href="https://twitter.com/jenny_wen/status/2014479445738893649">@jenny_wen</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/design">design</a>, <a href="https://simonwillison.net/tags/prototyping">prototyping</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a></p>]]></description><pubDate>Sat, 24 Jan 2026 23:31:03 +0000</pubDate></item><item><title>Quoting Jasmine Sun</title><link>https://simonwillison.net/2026/Jan/24/jasmine-sun/#atom-everything</link><description><![CDATA[<blockquote cite="https://jasmi.news/p/claude-code"><p><strong>If you tell a friend they can now instantly create any app, they‚Äôll probably say ‚ÄúCool! Now I need to think of an idea.‚Äù</strong> Then they will forget about it, and never build a thing. The problem is not that your friend is horribly uncreative. It‚Äôs that most people‚Äôs problems are not software-shaped, and most won‚Äôt notice even when they are. [...]</p>
<p>Programmers are trained to see everything as a software-shaped problem: if you do a task three times, you should probably automate it with a script. <em>Rename every IMG_*.jpg file from the last week to hawaii2025_*.jpg</em>, they tell their terminal, while the rest of us painfully click and copy-paste. We are blind to the solutions we were never taught to see, asking for faster horses and never dreaming of cars.</p></blockquote>
<p class="cite">&mdash; <a href="https://jasmi.news/p/claude-code">Jasmine Sun</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Sat, 24 Jan 2026 21:34:35 +0000</pubDate></item><item><title>Wilson Lin on FastRender: a browser built by thousands of parallel agents</title><link>https://simonwillison.net/2026/Jan/23/fastrender/#atom-everything</link><description><![CDATA[<p>Last week Cursor published <a href="https://cursor.com/blog/scaling-agents">Scaling long-running autonomous coding</a>, an article describing their research efforts into coordinating large numbers of autonomous coding agents. One of the projects mentioned in the article was <a href="https://github.com/wilsonzlin/fastrender">FastRender</a>, a web browser they built from scratch using their agent swarms. I wanted to learn more so I asked Wilson Lin, the engineer behind FastRender, if we could record a conversation about the project. That 47 minute video is <a href="https://www.youtube.com/watch?v=bKrAcTf2pL4">now available on YouTube</a>. I've included some of the highlights below.</p>

<iframe style="margin-top: 1.5em; margin-bottom: 1.5em;" width="560" height="315" src="https://www.youtube-nocookie.com/embed/bKrAcTf2pL4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"> </iframe>

<p>See my <a href="https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/">previous post</a> for my notes and screenshots from trying out FastRender myself.</p>


<h4 id="what-fastrender-can-do-right-now">What FastRender can do right now</h4>
<p>We started the conversation with a demo of FastRender loading different pages (<a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=195s">03:15</a>). The JavaScript engine isn't working yet so we instead loaded <a href="https://github.com/wilsonzlin/fastrender">github.com/wilsonzlin/fastrender</a>, <a href="https://en.wikipedia.org/">Wikipedia</a> and <a href="https://cnn.com">CNN</a> - all of which were usable, if a little slow to display.</p>
<p>JavaScript had been disabled by one of the agents, which decided to add a feature flag! <a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=242s">04:02</a></p>
<blockquote>
<p>JavaScript is disabled right now. The agents made a decision as they were currently still implementing the engine and making progress towards other parts... they decided to turn it off or put it behind a feature flag, technically.</p>
</blockquote>
<h4 id="from-side-project-to-core-research">From side-project to core research</h4>
<p>Wilson started what become FastRender as a personal side-project to explore the capabilities of the latest generation of frontier models - Claude Opus 4.5, GPT-5.1, and GPT-5.2. <a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=56s">00:56</a></p>
<blockquote>
<p>FastRender was a personal project of mine from, I'd say, November. It was an experiment to see how well frontier models like Opus 4.5 and back then GPT-5.1 could do with much more complex, difficult tasks.</p>
</blockquote>
<p>A browser rendering engine was the ideal choice for this, because it's both <em>extremely</em> ambitious and complex but also well specified. And you can visually see how well it's working! <a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=117s">01:57</a></p>
<blockquote>
<p>As that experiment progressed, I was seeing better and better results from single agents that were able to actually make good progress on this project. And at that point, I wanted to see, well, what's the next level? How do I push this even further?</p>
</blockquote>
<p>Once it became clear that this was an opportunity to try multiple agents working together it graduated to an official Cursor research project, and available resources were amplified.</p>
<p>The goal of FastRender was never to build a browser to compete with the likes of Chrome. <a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=2512s">41:52</a></p>
<blockquote>
<p>We never intended for it to be a production software or usable, but we wanted to observe behaviors of this harness of multiple agents, to see how they could work at scale.</p>
</blockquote>
<p>The great thing about a browser is that it has such a large scope that it can keep serving experiments in this space for many years to come. JavaScript, then WebAssembly, then WebGPU... it could take many years to run out of new challenges for the agents to tackle.</p>
<h4 id="running-thousands-of-agents-at-once">Running thousands of agents at once</h4>
<p>The most interesting thing about FastRender is the way the project used multiple agents working in parallel to build different parts of the browser. I asked how many agents were running at once: <a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=324s">05:24</a></p>
<blockquote>
<p>At the peak, when we had the stable system running for one week continuously, there were approximately 2,000 agents running concurrently at one time. And they were making, I believe, thousands of commits per hour.</p>
</blockquote>
<p>The project has <a href="https://github.com/wilsonzlin/fastrender/commits/main/">nearly 30,000 commits</a>!</p>
<p>How do you run 2,000 agents at once? They used <em>really big machines</em>. <a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=356s">05:56</a></p>
<blockquote>
<p>The simple approach we took with the infrastructure was to have a large machine run one of these multi-agent harnesses. Each machine had ample resources, and it would run about 300 agents concurrently on each. This was able to scale and run reasonably well, as agents spend a lot of time thinking, and not just running tools.</p>
</blockquote>
<p>At this point we switched to a live demo of the harness running on one of those big machines (<a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=392s">06:32</a>). The agents are arranged in a tree structure, with planning agents firing up tasks and worker agents then carrying them out. <a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=434s">07:14</a></p>
<p><img src="https://static.simonwillison.net/static/2026/wilson-lin-agents.jpg" alt="Terminal window showing a tmux session running &quot;grind-swarm&quot; task manager with RUNNING status. Header shows &quot;grind-swarm ‚Äì 45:54:15&quot; with stats &quot;planners: 9 (0 done) | tasks: 111 working, 0 pending, 232 done | 12900.9M‚Üë 514.1M‚Üì&quot;. Task list includes: p1 Root (main), p2 CSS selector matching performance + bloom filter integration, p3 CSS stylesheet parsing semantics &amp; at-rule handling, p4 Custom properties (@property) + var() resolution + incremental recompute/invalidation, p37 CSS at-rule artifact integration, p50 Selector engine correctness &amp; spec coverage, p51 Computed-value + property coverage across css-cascade, p105 Style sharing / computed style caching in fastrender-style, p289 CSS cascade layers (@layer) global ordering, w5 Fix workspace lockfile drift, w7 Implement computed-style snapshot sharing, w15 Fix css-properties namespace handling, w17 (Stretch) Enable bloom fast-reject in HTML quirks mode, w18 Refactor css-properties stylesheet parsing. Activity log shows shell commands including cargo check, git status, git push origin main, and various test runs. Bottom status bar shows &quot;grind-css0:target/release/grind-swarm*&quot; and &quot;streamyard.com is sharing your screen&quot; notification with timestamp &quot;12:02 22-Jan-26&quot;." style="max-width: 100%;" /></p>
<blockquote>
<p>This cluster of agents is working towards building out the CSS aspects of the browser, whether that's parsing, selector engine, those features. We managed to push this even further by splitting out the browser project into multiple instructions or work streams and have each one run one of these harnesses on their own machine, so that was able to further parallelize and increase throughput.</p>
</blockquote>
<p>But don't all of these agents working on the same codebase result in a huge amount of merge conflicts? Apparently not: <a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=501s">08:21</a></p>
<blockquote>
<p>We've noticed that most commits do not have merge conflicts. The reason is the harness itself is able to quite effectively split out and divide the scope and tasks such that it tries to minimize the amount of overlap of work. That's also reflected in the code structure‚Äîcommits will be made at various times and they don't tend to touch each other at the same time.</p>
</blockquote>
<p>This appears to be the key trick for unlocking benefits from parallel agents: if planning agents do a good enough job of breaking up the work into non-overlapping chunks you can bring hundreds or even thousands of agents to bear on a problem at once.</p>
<p>Surprisingly, Wilson found that GPT-5.1 and GPT-5.2 were a better fit for this work than the coding specialist GPT-5.1-Codex: <a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=1048s">17:28</a></p>
<blockquote>
<p>Some initial findings were that the instructions here were more expansive than merely coding. For example, how to operate and interact within a harness, or how to operate autonomously without interacting with the user or having a lot of user feedback. These kinds of instructions we found worked better with the general models.</p>
</blockquote>
<p>I asked what the longest they've seen this system run without human intervention: <a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=1108s">18:28</a></p>
<blockquote>
<p>So this system, once you give an instruction, there's actually no way to steer it, you can't prompt it, you're going to adjust how it goes. The only thing you can do is stop it. So our longest run, all the runs are basically autonomous. We don't alter the trajectory while executing. [...]</p>
<p>And so the longest at the time of the post was about a week and that's pretty close to the longest. Of course the research project itself was only about three weeks so you know we probably can go longer.</p>
</blockquote>
<h4 id="specifications-and-feedback-loops">Specifications and feedback loops</h4>
<p>An interesting aspect of this project design is feedback loops. For agents to work autonomously for long periods of time they need as much useful context about the problem they are solving as possible, combined with effective feedback loops to help them make decisions.</p>
<p>The FastRender repo <a href="https://github.com/wilsonzlin/fastrender/tree/19bf1036105d4eeb8bf3330678b7cb11c1490bdc/specs">uses git submodules to include relevant specifications</a>, including csswg-drafts, tc39-ecma262 for JavaScript, whatwg-dom, whatwg-html and more. <a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=846s">14:06</a></p>
<blockquote>
<p>Feedback loops to the system are very important. Agents are working for very long periods continuously, and without guardrails and feedback to know whether what they're doing is right or wrong it can have a big impact over a long rollout. Specs are definitely an important part‚Äîyou can see lots of comments in the code base that AI wrote referring specifically to specs that they found in the specs submodules.</p>
</blockquote>
<p>GPT-5.2 is a vision-capable model, and part of the feedback loop for FastRender included taking screenshots of the rendering results and feeding those back into the model:
<a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=983s">16:23</a></p>
<blockquote>
<p>In the earlier evolution of this project, when it was just doing the static renderings of screenshots, this was definitely a very explicit thing we taught it to do. And these models are visual models, so they do have that ability. We have progress indicators to tell it to compare the diff against a golden sample.</p>
</blockquote>
<p>The strictness of the Rust compiler helped provide a feedback loop as well: <a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=952s">15:52</a></p>
<blockquote>
<p>The nice thing about Rust is you can get a lot of verification just from compilation, and that is not as available in other languages.</p>
</blockquote>
<h4 id="the-agents-chose-the-dependencies">The agents chose the dependencies</h4>
<p>We talked about the <a href="https://github.com/wilsonzlin/fastrender/blob/19bf1036105d4eeb8bf3330678b7cb11c1490bdc/Cargo.toml">Cargo.toml dependencies</a> that the project had accumulated, almost all of which had been selected by the agents themselves.</p>
<p>Some of these, like <a href="https://skia.org/">Skia</a> for 2D graphics rendering or <a href="https://github.com/harfbuzz/harfbuzz">HarfBuzz</a> for text shaping, were obvious choices. Others such as <a href="https://github.com/DioxusLabs/taffy">Taffy</a> felt like they might go against the from-scratch goals of the project, since that library implements CSS flexbox and grid layout algorithms directly. This was not an intended outcome. <a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=1673s">27:53</a></p>
<blockquote>
<p>Similarly these are dependencies that the agent picked to use for small parts of the engine and perhaps should have actually implemented itself. I think this reflects on the importance of the instructions, because I actually never encoded specifically the level of dependencies we should be implementing ourselves.</p>
</blockquote>
<p>The agents vendored in Taffy and <a href="https://github.com/wilsonzlin/fastrender/commits/main/vendor/taffy">applied a stream of changes</a> to that vendored copy.
<a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=1878s">31:18</a></p>
<blockquote>
<p>It's currently vendored. And as the agents work on it, they do make changes to it. This was actually an artifact from the very early days of the project before it was a fully fledged browser... it's implementing things like the flex and grid layers, but there are other layout methods like inline, block, and table, and in our new experiment, we're removing that completely.</p>
</blockquote>
<p>The inclusion of QuickJS despite the presence of a home-grown ecma-rs implementation has a fun origin story:
<a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=2115s">35:15</a></p>
<blockquote>
<p>I believe it mentioned that it pulled in the QuickJS because it knew that other agents were working on the JavaScript engine, and it needed to unblock itself quickly. [...]</p>
<p>It was like, eventually, once that's finished, let's remove it and replace with the proper engine.</p>
</blockquote>
<p>I love how similar this is to the dynamics of a large-scale human engineering team, where you could absolutely see one engineer getting frustrated at another team not having delivered yet and unblocking themselves by pulling in a third-party library.</p>
<h4 id="intermittent-errors-are-ok-actually">Intermittent errors are OK, actually</h4>
<p>Here's something I found really surprising: the agents were allowed to introduce small errors into the codebase as they worked! <a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=2382s">39:42</a></p>
<blockquote>
<p>One of the trade-offs was: if you wanted every single commit to be a hundred percent perfect, make sure it can always compile every time, that might be a synchronization bottleneck. [...]</p>
<p>Especially as you break up the system into more modularized aspects, you can see that errors get introduced, but small errors, right? An API change or some syntax error, but then they get fixed really quickly after a few commits. So there's a little bit of slack in the system to allow these temporary errors so that the overall system can continue to make progress at a really high throughput. [...]</p>
<p>People may say, well, that's not correct code. But it's not that the errors are accumulating. It's a stable rate of errors. [...] That seems like a worthwhile trade-off.</p>
</blockquote>
<p>If you're going to have thousands of agents working in parallel optimizing for throughput over correctness turns out to be a strategy worth exploring.</p>
<h4 id="a-single-engineer-plus-a-swarm-of-agents-in-january-2026">A single engineer plus a swarm of agents in January 2026</h4>
<p>The thing I find most interesting about FastRender is how it demonstrates the extreme edge of what a single engineer can achieve in early 2026 with the assistance of a swarm of agents.</p>
<p>FastRender may not be a production-ready browser, but it represents over a million lines of Rust code, written in a few weeks, that can already render real web pages to a usable degree.</p>
<p>A browser really is the ideal research project to experiment with this new, weirdly shaped form of software engineering.</p>
<p>I asked Wilson how much mental effort he had invested in browser rendering compared to agent co-ordination. <a href="https://www.youtube.com/watch?v=bKrAcTf2pL4&amp;t=694s">11:34</a></p>
<blockquote>
<p>The browser and this project were co-developed and very symbiotic, only because the browser was a very useful objective for us to measure and iterate the progress of the harness. The goal was to iterate on and research the multi-agent harness‚Äîthe browser was just the research example or objective.</p>
</blockquote>
<p>FastRender is effectively using a full browser rendering engine as a "hello world" exercise for multi-agent coordination!</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/browsers">browsers</a>, <a href="https://simonwillison.net/tags/youtube">youtube</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/cursor">cursor</a>, <a href="https://simonwillison.net/tags/parallel-agents">parallel-agents</a>, <a href="https://simonwillison.net/tags/browser-challenge">browser-challenge</a></p>]]></description><pubDate>Fri, 23 Jan 2026 21:26:10 +0000</pubDate></item><item><title>Quoting Theia Vogel</title><link>https://simonwillison.net/2026/Jan/23/theia-vogel/#atom-everything</link><description><![CDATA[<blockquote cite="https://twitter.com/voooooogel/status/2014189072647078053"><p>[...] i was too busy with work to read anything, so i asked chatgpt to summarize some books on state formation, and it suggested circumscription theory. there was already the natural boundary of my computer hemming the towns in, and town mayors played the role of big men to drive conflict. so i just needed a way for them to fight. i slightly tweaked the allocation of claude max accounts to the towns from a demand-based to a fixed allocation system. towns would each get a fixed amount of tokens to start, but i added a soldier role that could attack and defend in raids to steal tokens from other towns. [...]</p></blockquote>
<p class="cite">&mdash; <a href="https://twitter.com/voooooogel/status/2014189072647078053">Theia Vogel</a>, Gas Town fan fiction</p>

    <p>Tags: <a href="https://simonwillison.net/tags/parallel-agents">parallel-agents</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a></p>]]></description><pubDate>Fri, 23 Jan 2026 09:13:54 +0000</pubDate></item><item><title>SSH has no Host header</title><link>https://simonwillison.net/2026/Jan/22/ssh-has-no-host-header/#atom-everything</link><description><![CDATA[<p><strong><a href="https://blog.exe.dev/ssh-host-header">SSH has no Host header</a></strong></p>
<a href="https://exe.dev/">exe.dev</a> is a new hosting service that, for $20/month, gives you up to 25 VMs "that share 2 CPUs and 8GB RAM". Everything happens over SSH, including creating new VMs. Once configured you can sign into your exe.dev VMs like this:</p>
<pre><code>ssh simon.exe.dev
</code></pre>
<p>Here's the clever bit: when you run the above command <code>exe.dev</code> signs you into your VM of that name... but they don't assign every VM its own IP address and SSH has no equivalent of the Host header, so how does their load balancer know <em>which</em> of your VMs to forward you on to?</p>
<p>The answer is that while they don't assign a unique IP to every VM they <em>do</em> have enough IPs that they can ensure each of your VMs has an IP that is unique to your account.</p>
<p>If I create two VMs they will each resolve to a separate IP address, each of which is shared with many other users. The underlying infrastructure then identifies my user account from my SSH public key and can determine which underlying VM to forward my SSH traffic to.

    <p><small></small>Via <a href="https://lobste.rs/s/7oqiqi/ssh_has_no_host_header">lobste.rs</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/dns">dns</a>, <a href="https://simonwillison.net/tags/hosting">hosting</a>, <a href="https://simonwillison.net/tags/ssh">ssh</a></p>]]></description><pubDate>Thu, 22 Jan 2026 23:57:50 +0000</pubDate></item><item><title>Qwen3-TTS Family is Now Open Sourced: Voice Design, Clone, and Generation</title><link>https://simonwillison.net/2026/Jan/22/qwen3-tts/#atom-everything</link><description><![CDATA[<p><strong><a href="https://qwen.ai/blog?id=qwen3tts-0115">Qwen3-TTS Family is Now Open Sourced: Voice Design, Clone, and Generation</a></strong></p>
I haven't been paying much attention to the state-of-the-art in speech generation models other than noting that they've got <em>really good</em>, so I can't speak for how notable this new release from Qwen is.</p>
<p>From <a href="https://github.com/QwenLM/Qwen3-TTS/blob/main/assets/Qwen3_TTS.pdf">the accompanying paper</a>:</p>
<blockquote>
<p>In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of- the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis [...]. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.</p>
</blockquote>
<p>To give an idea of size, <a href="https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base">Qwen/Qwen3-TTS-12Hz-1.7B-Base</a> is 4.54GB on Hugging Face and <a href="https://huggingface.co/Qwen/Qwen3-TTS-12Hz-0.6B-Base">Qwen/Qwen3-TTS-12Hz-0.6B-Base</a> is 2.52GB.</p>
<p>The <a href="https://huggingface.co/spaces/Qwen/Qwen3-TTS">Hugging Face demo</a> lets you try out the 0.6B and 1.7B models for free in your browser, including voice cloning:</p>
<p><img alt="Screenshot of a Qwen3-TTS voice cloning web interface with three tabs at top: &quot;Voice Design&quot;, &quot;Voice Clone (Base)&quot; (selected), and &quot;TTS (CustomVoice)&quot;. The page is titled &quot;Clone Voice from Reference Audio&quot; and has two main sections. Left section: &quot;Reference Audio (Upload a voice sample clone)&quot; showing an audio waveform player at 0:00/0:34 with playback controls, upload and microphone icons, followed by &quot;Reference Text (Transcript of the reference audio)&quot; containing three paragraphs: &quot;Simon Willison is the creator of Datasette, an open source tool for exploring and publishing data. He currently works full-time building open source tools for data journalism, built around Datasette and SQLite. Prior to becoming an independent open source developer, Simon was an engineering director at Eventbrite. Simon joined Eventbrite through their acquisition of Lanyrd, a Y Combinator funded company he co-founded in 2010. He is a co-creator of the Django Web Framework, and has been blogging about web development and programming since 2002 at simonwillison.net&quot;. Right section: &quot;Target Text (Text to synthesize with cloned voice)&quot; containing text about Qwen3-TTS speech generation capabilities, with &quot;Language&quot; dropdown set to &quot;Auto&quot; and &quot;Model Size&quot; dropdown set to &quot;1.7B&quot;, and a purple &quot;Clone &amp; Generate&quot; button at bottom." src="https://static.simonwillison.net/static/2026/qwen-voice-clone.jpg" /></p>
<p>I tried this out by recording myself reading <a href="https://simonwillison.net/about/">my about page</a> and then having Qwen3-TTS generate audio of me reading the Qwen3-TTS announcement post. Here's the result:</p>
<p><audio controls style="width: 100%">
  <source src="https://static.simonwillison.net/static/2026/qwen-tts-clone.wav" type="audio/wav">
  Your browser does not support the audio element.
</audio></p>
<p>It's important that everyone understands that voice cloning is now something that's available to anyone with a GPU and a few GBs of VRAM... or in this case a web browser that can access Hugging Face.</p>
<p><strong>Update</strong>: Prince Canuma <a href="https://x.com/Prince_Canuma/status/2014453857019904423">got this working</a> with his <a href="https://pypi.org/project/mlx-audio/">mlx-audio</a> library. I <a href="https://claude.ai/share/2e01ad60-ca38-4e14-ab60-74eaa45b2fbd">had Claude</a> turn that into <a href="https://github.com/simonw/tools/blob/main/python/q3_tts.py">a CLI tool</a> which you can run with <code>uv</code> ike this:</p>
<pre><code>uv run https://tools.simonwillison.net/python/q3_tts.py \
  'I am a pirate, give me your gold!' \
  -i 'gruff voice' -o pirate.wav
</code></pre>
<p>The <code>-i</code> option lets you use a prompt to describe the voice it should use. On first run this downloads a 4.5GB model file from Hugging Face.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=46719229">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/text-to-speech">text-to-speech</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/hugging-face">hugging-face</a>, <a href="https://simonwillison.net/tags/uv">uv</a>, <a href="https://simonwillison.net/tags/qwen">qwen</a>, <a href="https://simonwillison.net/tags/mlx">mlx</a>, <a href="https://simonwillison.net/tags/prince-canuma">prince-canuma</a>, <a href="https://simonwillison.net/tags/ai-in-china">ai-in-china</a></p>]]></description><pubDate>Thu, 22 Jan 2026 17:42:34 +0000</pubDate></item><item><title>Quoting Chris Lloyd</title><link>https://simonwillison.net/2026/Jan/22/chris-lloyd/#atom-everything</link><description><![CDATA[<blockquote cite="https://news.ycombinator.com/item?id=46699072#46706040"><p>Most people's mental model of Claude Code is that "it's just a TUI" but it should really be closer to "a small game engine".</p>
<p>For each frame our pipeline constructs a scene graph with React then:</p>
<p>-&gt; layout elements<br>
-&gt; rasterize them to a 2d screen<br>
-&gt; diff that against the previous screen<br>
-&gt; <em>finally</em> use the diff to generate ANSI sequences to draw</p>
<p>We have a ~16ms frame budget so we have roughly ~5ms to go from the React scene graph to ANSI written.</p></blockquote>
<p class="cite">&mdash; <a href="https://news.ycombinator.com/item?id=46699072#46706040">Chris Lloyd</a>, Claude Code team at Anthropic</p>

    <p>Tags: <a href="https://simonwillison.net/tags/react">react</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a></p>]]></description><pubDate>Thu, 22 Jan 2026 15:34:22 +0000</pubDate></item><item><title>Claude&apos;s new constitution</title><link>https://simonwillison.net/2026/Jan/21/claudes-new-constitution/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.anthropic.com/news/claude-new-constitution">Claude&#x27;s new constitution</a></strong></p>
Late last year Richard Weiss <a href="https://www.lesswrong.com/posts/vpNG99GhbBoLov9og/claude-4-5-opus-soul-document">found something interesting</a> while poking around with the just-released Claude Opus 4.5: he was able to talk the model into regurgitating a document which was <em>not</em> part of the system prompt but appeared instead to be baked in during training, and which described Claude's core values at great length.</p>
<p>He called this leak the <strong>soul document</strong>, and Amanda Askell from Anthropic <a href="https://simonwillison.net/2025/Dec/2/claude-soul-document/">quickly confirmed</a> that it was indeed part of Claude's training procedures.</p>
<p>Today Anthropic made this official, <a href="https://www.anthropic.com/news/claude-new-constitution">releasing that full "constitution" document</a> under a CC0 (effectively public domain) license. There's a lot to absorb! It's over 35,000 tokens, more than 10x the length of the <a href="https://platform.claude.com/docs/en/release-notes/system-prompts#claude-opus-4-5">published Opus 4.5 system prompt</a>.</p>
<p>One detail that caught my eye is the acknowledgements at the end, which include a list of <a href="https://www.anthropic.com/constitution#acknowledgements">external contributors</a> who helped review the document. I was intrigued to note that two of the fifteen listed names are Catholic members of the clergy - <a href="https://www.frbrendanmcguire.org/biography">Father Brendan McGuire</a> is a pastor in Los Altos with a Master‚Äôs degree in Computer Science and Math and <a href="https://en.wikipedia.org/wiki/Paul_Tighe">Bishop Paul Tighe</a> is an Irish Catholic bishop with a background in moral theology.


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/amanda-askell">amanda-askell</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/ai-personality">ai-personality</a></p>]]></description><pubDate>Wed, 21 Jan 2026 23:39:49 +0000</pubDate></item><item><title>Electricity use of AI coding agents</title><link>https://simonwillison.net/2026/Jan/20/electricity-use-of-ai-coding-agents/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.simonpcouch.com/blog/2026-01-20-cc-impact/">Electricity use of AI coding agents</a></strong></p>
Previous work estimating the energy and water cost of LLMs has generally focused on the cost per prompt using a consumer-level system such as ChatGPT.</p>
<p>Simon P. Couch notes that coding agents such as Claude Code use <em>way</em> more tokens in response to tasks, often burning through many thousands of tokens of many tool calls.</p>
<p>As a heavy Claude Code user, Simon estimates his own usage at the equivalent of 4,400 "typical queries" to an LLM, for an equivalent of around $15-$20 in daily API token spend. He figures that to be about the same as running a dishwasher once or the daily energy used by a domestic refrigerator.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=46695415">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/ai-energy-usage">ai-energy-usage</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a></p>]]></description><pubDate>Tue, 20 Jan 2026 23:11:57 +0000</pubDate></item><item><title>Giving University Exams in the Age of Chatbots</title><link>https://simonwillison.net/2026/Jan/20/giving-university-exams-in-the-age-of-chatbots/#atom-everything</link><description><![CDATA[<p><strong><a href="https://ploum.net/2026-01-19-exam-with-chatbots.html">Giving University Exams in the Age of Chatbots</a></strong></p>
Detailed and thoughtful description of an open-book and open-chatbot exam run by <a href="https://fr.wikipedia.org/wiki/Lionel_Dricot">Ploum</a> at √âcole Polytechnique de Louvain for an "Open Source Strategies" class.</p>
<p>Students were told they could use chatbots during the exam but they had to announce their intention to do so in advance, share their prompts and take full accountability for any mistakes they made.</p>
<p>Only 3 out of 60 students chose to use chatbots. Ploum surveyed half of the class to help understand their motivations.

    <p><small></small>Via <a href="https://lobste.rs/s/parmy3/giving_university_exams_age_chatbots">lobste.rs</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/education">education</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a></p>]]></description><pubDate>Tue, 20 Jan 2026 17:51:17 +0000</pubDate></item><item><title>jordanhubbard/nanolang</title><link>https://simonwillison.net/2026/Jan/19/nanolang/#atom-everything</link><description><![CDATA[<p><strong><a href="https://github.com/jordanhubbard/nanolang">jordanhubbard/nanolang</a></strong></p>
Plenty of people have mused about what a new programming language specifically designed to be used by LLMs might look like. Jordan Hubbard (<a href="https://en.wikipedia.org/wiki/Jordan_Hubbard">co-founder of FreeBSD</a>, with serious stints at Apple and NVIDIA) just released exactly that.</p>
<blockquote>
<p>A minimal, LLM-friendly programming language with mandatory testing and unambiguous syntax.</p>
<p>NanoLang transpiles to C for native performance while providing a clean, modern syntax optimized for both human readability and AI code generation.</p>
</blockquote>
<p>The syntax strikes me as an interesting mix between C, Lisp and Rust.</p>
<p>I decided to see if an LLM could produce working code in it directly, given the necessary context. I started with this <a href="https://github.com/jordanhubbard/nanolang/blob/main/MEMORY.md">MEMORY.md</a> file, which begins:</p>
<blockquote>
<p><strong>Purpose:</strong> This file is designed specifically for Large Language Model consumption. It contains the essential knowledge needed to generate, debug, and understand NanoLang code. Pair this with <code>spec.json</code> for complete language coverage.</p>
</blockquote>
<p>I ran that using <a href="https://llm.datasette.io/">LLM</a> and <a href="https://github.com/simonw/llm-anthropic">llm-anthropic</a> like this:</p>
<pre><code>llm -m claude-opus-4.5 \
  -s https://raw.githubusercontent.com/jordanhubbard/nanolang/refs/heads/main/MEMORY.md \
  'Build me a mandelbrot fractal CLI tool in this language' 
  &gt; /tmp/fractal.nano
</code></pre>
<p>The <a href="https://gist.github.com/simonw/7847f022566d11629ec2139f1d109fb8#mandelbrot-fractal-cli-tool-in-nano">resulting code</a>... <a href="https://gist.github.com/simonw/7847f022566d11629ec2139f1d109fb8?permalink_comment_id=5947465#gistcomment-5947465">did not compile</a>.</p>
<p>I may have been too optimistic expecting a one-shot working program for a new language like this. So I ran a clone of the actual project, copied in my program and had Claude Code take a look at the failing compiler output.</p>
<p>... and it worked! Claude happily grepped its way through the various <code>examples/</code> and built me a working program.</p>
<p>Here's <a href="https://gisthost.github.io/?9696da6882cb6596be6a9d5196e8a7a5/index.html">the Claude Code transcript</a> - you can see it <a href="https://gisthost.github.io/?9696da6882cb6596be6a9d5196e8a7a5/page-001.html#msg-2026-01-19T23-43-09-675Z">reading relevant examples here</a> - and here's <a href="https://gist.github.com/simonw/e7f3577adcfd392ab7fa23b1295d00f2">the finished code plus its output</a>.</p>
<p>I've suspected <a href="https://simonwillison.net/2025/Nov/7/llms-for-new-programming-languages/">for a while</a> that LLMs and coding agents might significantly reduce the friction involved in launching a new language. This result reinforces my opinion.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=46684958">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/programming-languages">programming-languages</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/llm">llm</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a></p>]]></description><pubDate>Mon, 19 Jan 2026 23:58:56 +0000</pubDate></item><item><title>Scaling long-running autonomous coding</title><link>https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/#atom-everything</link><description><![CDATA[<p><strong><a href="https://cursor.com/blog/scaling-agents">Scaling long-running autonomous coding</a></strong></p>
Wilson Lin at Cursor has been doing some experiments to see how far you can push a large fleet of "autonomous" coding agents:</p>
<blockquote>
<p>This post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens.</p>
</blockquote>
<p>They ended up running planners and sub-planners to create tasks, then having workers execute on those tasks - similar to how Claude Code uses sub-agents. Each cycle ended with a judge agent deciding if the project was completed or not.</p>
<p>In my predictions for 2026 <a href="https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#3-years-someone-will-build-a-new-browser-using-mainly-ai-assisted-coding-and-it-won-t-even-be-a-surprise">the other day</a> I said that by 2029:</p>
<blockquote>
<p>I think somebody will have built a full web browser mostly using AI assistance, and it won‚Äôt even be surprising. Rolling a new web browser is one of the most complicated software projects I can imagine[...] the cheat code is the conformance suites. If there are existing tests that it‚Äôll get so much easier.</p>
</blockquote>
<p>I may have been off by three years, because Cursor chose "building a web browser from scratch" as their test case for their agent swarm approach:</p>
<blockquote>
<p>To test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explore <a href="https://github.com/wilsonzlin/fastrender">the source code on GitHub</a>.</p>
</blockquote>
<p>But how well did they do? Their initial announcement a couple of days ago was met with <a href="https://embedding-shapes.github.io/cursor-implied-success-without-evidence/">unsurprising skepticism</a>, especially when it became apparent that their GitHub Actions CI was failing and there were no build instructions in the repo.</p>
<p>It looks like they addressed that within the past 24 hours. The <a href="https://github.com/wilsonzlin/fastrender/blob/main/README.md#build-requirements">latest README</a> includes build instructions which I followed on macOS like this:</p>
<pre><code>cd /tmp
git clone https://github.com/wilsonzlin/fastrender
cd fastrender
git submodule update --init vendor/ecma-rs
cargo run --release --features browser_ui --bin browser
</code></pre>
<p>This got me a working browser window! Here are screenshots I took of google.com and my own website:</p>
<p><img alt="The browser chrome is neat but has a garbled tab name at the top. The Google homepage looks mostly correct but the buttons are not styled correctly and the Google Search one has a huge plus icon floating near it." src="https://static.simonwillison.net/static/2026/cursor-google.png" /></p>
<p><img alt="My blog looks mostly correct, but the right closing quotation mark on a quotation (which is implemented as a background image on the final paragraph) is displayed incorrectly multiple times." src="https://static.simonwillison.net/static/2026/cursor-simonwillison.jpg" /></p>
<p>Honestly those are very impressive! You can tell they're not just wrapping an existing rendering engine because of those very obvious rendering glitches, but the pages are legible and look mostly correct.</p>
<p>The FastRender repo even uses Git submodules <a href="https://github.com/wilsonzlin/fastrender/tree/main/specs">to include various WhatWG and CSS-WG specifications</a> in the repo, which is a smart way to make sure the agents have access to the reference materials that they might need.</p>
<p>This is the second attempt I've seen at building a full web browser using AI-assisted coding in the past two weeks - the first was <a href="https://github.com/hiwavebrowser/hiwave">HiWave browser</a>, a new browser engine in Rust first announced <a href="https://www.reddit.com/r/Anthropic/comments/1q4xfm0/over_christmas_break_i_wrote_a_fully_functional/">in this Reddit thread</a>.</p>
<p>When I made my 2029 prediction this is more-or-less the quality of result I had in mind. I don't think we'll see projects of this nature compete with Chrome or Firefox or WebKit any time soon but I have to admit I'm very surprised to see something this capable emerge so quickly.</p>
<p><strong>Update 23rd January 2026</strong>: I recorded a 47 minute conversation with Wilson about this project and published it on YouTube. Here's <a href="https://simonwillison.net/2026/Jan/23/fastrender/">the video and accompanying highlights</a>.


    <p>Tags: <a href="https://simonwillison.net/tags/browsers">browsers</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/cursor">cursor</a>, <a href="https://simonwillison.net/tags/parallel-agents">parallel-agents</a>, <a href="https://simonwillison.net/tags/conformance-suites">conformance-suites</a>, <a href="https://simonwillison.net/tags/browser-challenge">browser-challenge</a></p>]]></description><pubDate>Mon, 19 Jan 2026 05:12:51 +0000</pubDate></item><item><title>FLUX.2-klein-4B Pure C Implementation</title><link>https://simonwillison.net/2026/Jan/18/flux2-klein-4b/#atom-everything</link><description><![CDATA[<p><strong><a href="https://github.com/antirez/flux2.c">FLUX.2-klein-4B Pure C Implementation</a></strong></p>
On 15th January Black Forest Labs, a lab formed by the creators of the original Stable Diffusion, released <a href="https://huggingface.co/black-forest-labs/FLUX.2-klein-4B">black-forest-labs/FLUX.2-klein-4B</a> - an Apache 2.0 licensed 4 billion parameter version of their FLUX.2 family.</p>
<p>Salvatore Sanfilippo (antirez) decided to build a pure C and dependency-free implementation to run the model, with assistance from Claude Code and Claude Opus 4.5.</p>
<p>Salvatore shared <a href="https://news.ycombinator.com/item?id=46670279#46671233">this note</a> on Hacker News:</p>
<blockquote>
<p>Something that may be interesting for the reader of this thread: this project was possible only once I started to tell Opus that it <em>needed</em> to take a file with all the implementation notes, and also accumulating all the things we discovered during the development process. And also, the file had clear instructions to be taken updated, and to be processed ASAP after context compaction. This kinda enabled Opus to do such a big coding task in a reasonable amount of time without loosing track. Check the file IMPLEMENTATION_NOTES.md in the GitHub repo for more info.</p>
</blockquote>
<p>Here's that <a href="https://github.com/antirez/flux2.c/blob/main/IMPLEMENTATION_NOTES.md">IMPLEMENTATION_NOTES.md</a> file.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=46670279">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/c">c</a>, <a href="https://simonwillison.net/tags/salvatore-sanfilippo">salvatore-sanfilippo</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/stable-diffusion">stable-diffusion</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/text-to-image">text-to-image</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a></p>]]></description><pubDate>Sun, 18 Jan 2026 23:58:58 +0000</pubDate></item><item><title>Quoting Jeremy Daer</title><link>https://simonwillison.net/2026/Jan/17/jeremy-daer/#atom-everything</link><description><![CDATA[<blockquote cite="https://twitter.com/dhh/status/2012543705161326941"><p><em>[On agents using CLI tools in place of REST APIs]</em> To save on context window, yes, but moreso to improve accuracy and success rate when multiple tool calls are involved, particularly when calls must be correctly chained e.g. for pagination, rate-limit backoff, and recognizing authentication failures.</p>
<p>Other major factor: which models can wield the skill? Using the CLI lowers the bar so cheap, fast models (gpt-5-nano, haiku-4.5) can reliably succeed. Using the raw APl is something only the costly "strong" models (gpt-5.2, opus-4.5) can manage, and it squeezes a ton of thinking/reasoning out of them, which means multiple turns/iterations, which means accumulating a ton of context, which means burning loads of expensive tokens. For one-off API requests and ad hoc usage driven by a developer, this is reasonable and even helpful, but for an autonomous agent doing repetitive work, it's a disaster.</p></blockquote>
<p class="cite">&mdash; <a href="https://twitter.com/dhh/status/2012543705161326941">Jeremy Daer</a>, 37signals</p>

    <p>Tags: <a href="https://simonwillison.net/tags/prompt-engineering">prompt-engineering</a>, <a href="https://simonwillison.net/tags/skills">skills</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/37-signals">37-signals</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Sat, 17 Jan 2026 17:06:41 +0000</pubDate></item><item><title>Our approach to advertising and expanding access to ChatGPT</title><link>https://simonwillison.net/2026/Jan/16/chatgpt-ads/#atom-everything</link><description><![CDATA[<p><strong><a href="https://openai.com/index/our-approach-to-advertising-and-expanding-access/">Our approach to advertising and expanding access to ChatGPT</a></strong></p>
OpenAI's long-rumored introduction of ads to ChatGPT just became a whole lot more concrete:</p>
<blockquote>
<p>In the coming weeks, we‚Äôre also planning to start testing ads in the U.S. for the free and Go tiers, so more people can benefit from our tools with fewer usage limits or without having to pay. Plus, Pro, Business, and Enterprise subscriptions will not include ads.</p>
</blockquote>
<p>What's "Go" tier, you might ask? That's a new $8/month tier that launched today in the USA, see <a href="https://openai.com/index/introducing-chatgpt-go/">Introducing ChatGPT Go, now available worldwide</a>. It's a tier that they first trialed in India in August 2025 (here's a mention <a href="https://help.openai.com/en/articles/6825453-chatgpt-release-notes#h_22cae6eb9f">in their release notes from August</a> listing a price of ‚Çπ399/month, which converts to around $4.40).</p>
<p>I'm finding the new plan comparison grid on <a href="https://chatgpt.com/pricing">chatgpt.com/pricing</a> pretty confusing. It lists all accounts as having access to GPT-5.2 Thinking, but doesn't clarify the limits that the free and Go plans have to conform to. It also lists different context windows for the different plans - 16K for free, 32K for Go and Plus and 128K for Pro. I had assumed that the 400,000 token window <a href="https://platform.openai.com/docs/models/gpt-5.2">on the GPT-5.2 model page</a> applied to ChatGPT as well, but apparently I was mistaken.</p>
<p><strong>Update</strong>: I've apparently not been paying attention: here's the Internet Archive ChatGPT pricing page from <a href="https://web.archive.org/web/20250906071408/https://chatgpt.com/pricing">September 2025</a> showing those context limit differences as well.</p>
<p>Back to advertising: my biggest concern has always been whether ads will influence the output of the chat directly. OpenAI assure us that they will not:</p>
<blockquote>
<ul>
<li><strong>Answer independence</strong>: Ads do not influence the answers ChatGPT gives you. Answers are optimized based on what's most helpful to you. Ads are always separate and clearly labeled.</li>
<li><strong>Conversation privacy</strong>: We keep your conversations with ChatGPT private from advertisers, and we never sell your data to advertisers.</li>
</ul>
</blockquote>
<p>So what will they look like then? This screenshot from the announcement offers a useful hint:</p>
<p><img alt="Two iPhone screenshots showing ChatGPT mobile app interface. Left screen displays a conversation about Santa Fe, New Mexico with an image of adobe-style buildings and desert landscape, text reading &quot;Santa Fe, New Mexico‚Äîoften called 'The City Different'‚Äîis a captivating blend of history, art, and natural beauty at the foot of the Sangre de Cristo Mountains. As the oldest and highest-elevation state capital in the U.S., founded in 1610, it offers a unique mix of Native American, Spanish, and Anglo cultures.&quot; Below is a sponsored section from &quot;Pueblo &amp; Pine&quot; showing &quot;Desert Cottages - Expansive residences with desert vistas&quot; with a thumbnail image, and a &quot;Chat with Pueblo &amp; Pine&quot; button. Input field shows &quot;Ask ChatGPT&quot;. Right screen shows the Pueblo &amp; Pine chat interface with the same Desert Cottages listing and an AI response &quot;If you're planning a trip to Sante Fe, I'm happy to help. When are you thinking of going?&quot; with input field &quot;Ask Pueblo &amp; Pine&quot; and iOS keyboard visible." src="https://static.simonwillison.net/static/2026/chatgpt-ads.jpg" /></p>
<p>The user asks about trips to Santa Fe, and an ad shows up for a cottage rental business there. This particular example imagines an option to start a direct chat with a bot aligned with that advertiser, at which point presumably the advertiser can influence the answers all they like!


    <p>Tags: <a href="https://simonwillison.net/tags/ads">ads</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/chatgpt">chatgpt</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Fri, 16 Jan 2026 21:28:26 +0000</pubDate></item><item><title>Open Responses</title><link>https://simonwillison.net/2026/Jan/15/open-responses/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.openresponses.org/">Open Responses</a></strong></p>
This is the standardization effort I've most wanted in the world of LLMs: a vendor-neutral specification for the JSON API that clients can use to talk to hosted LLMs.</p>
<p>Open Responses aims to provide exactly that as a documented standard, derived from OpenAI's Responses API.</p>
<p>I was hoping for one based on their older Chat Completions API since so many other products have cloned the already, but basing it on Responses does make sense since that API was designed with the feature of more recent models - such as reasoning traces - baked into the design.</p>
<p>What's certainly notable is the list of launch partners. OpenRouter alone means we can expect to be able to use this protocol with almost every existing model, and Hugging Face, LM Studio, vLLM, Ollama and Vercel cover a huge portion of the common tools used to serve models.</p>
<p>For protocols like this I really want to see a comprehensive, language-independent conformance test site. Open Responses has a subset of that - the official repository includes <a href="https://github.com/openresponses/openresponses/blob/d0f23437b27845d5c3d0abaf5cb5c4a702f26b05/src/lib/compliance-tests.ts">src/lib/compliance-tests.ts</a> which can be used to exercise a server implementation, and is available as a React app <a href="https://www.openresponses.org/compliance">on the official site</a> that can be pointed at any implementation served via CORS.</p>
<p>What's missing is the equivalent for clients. I plan to spin up my own client library for this in Python and I'd really like to be able to run that against a conformance suite designed to check that my client correctly handles all of the details.

    <p><small></small>Via <a href="https://twitter.com/reach_vb/status/2011863516852965565">VB</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/json">json</a>, <a href="https://simonwillison.net/tags/standards">standards</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/openrouter">openrouter</a>, <a href="https://simonwillison.net/tags/conformance-suites">conformance-suites</a></p>]]></description><pubDate>Thu, 15 Jan 2026 23:56:56 +0000</pubDate></item><item><title>The Design &amp; Implementation of Sprites</title><link>https://simonwillison.net/2026/Jan/15/the-design-implementation-of-sprites/#atom-everything</link><description><![CDATA[<p><strong><a href="https://fly.io/blog/design-and-implementation/">The Design &amp; Implementation of Sprites</a></strong></p>
I <a href="https://simonwillison.net/2026/Jan/9/sprites-dev/">wrote about Sprites last week</a>. Here's Thomas Ptacek from Fly with the insider details on how they work under the hood.</p>
<p>I like this framing of them as "disposable computers":</p>
<blockquote>
<p>Sprites are ball-point disposable computers. Whatever mark you mean to make, we‚Äôve rigged it so you‚Äôre never more than a second or two away from having a Sprite to do it with.</p>
</blockquote>
<p>I've noticed that new Fly Machines can take a while (up to around a minute) to provision. Sprites solve that by keeping warm pools of unused machines in multiple regions, which is enabled by them all using the same container:</p>
<blockquote>
<p>Now, today, under the hood, Sprites are still Fly Machines. But they all run from a standard container. Every physical worker knows exactly what container the next Sprite is going to start with, so it‚Äôs easy for us to keep pools of ‚Äúempty‚Äù Sprites standing by. The result: a Sprite create doesn‚Äôt have any heavy lifting to do; it‚Äôs basically just doing the stuff we do when we start a Fly Machine.</p>
</blockquote>
<p>The most interesting detail is how the persistence layer works. Sprites only charge you for data you have written that differs from the base image and provide ~300ms checkpointing and restores - it turns out that's power by a custom filesystem on top of S3-compatible storage coordinated by Litestream-replicated local SQLite metadata:</p>
<blockquote>
<p>We still exploit NVMe, but not as the root of storage. Instead, it‚Äôs a read-through cache for a blob on object storage. S3-compatible object stores are the most trustworthy storage technology we have. I can feel my blood pressure dropping just typing the words ‚ÄúSprites are backed by object storage.‚Äù [...]</p>
<p>The Sprite storage stack is organized around the JuiceFS model (in fact, we currently use a very hacked-up JuiceFS, with a rewritten SQLite metadata backend). It works by splitting storage into data (‚Äúchunks‚Äù) and metadata (a map of where the ‚Äúchunks‚Äù are). Data chunks live on object stores; metadata lives in fast local storage. In our case, that metadata store is <a href="https://litestream.io">kept durable with Litestream</a>. Nothing depends on local storage.</p>
</blockquote>

    <p><small></small>Via <a href="https://twitter.com/tqbf/status/2011823480673624434">@tqbf</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/architecture">architecture</a>, <a href="https://simonwillison.net/tags/sandboxing">sandboxing</a>, <a href="https://simonwillison.net/tags/sqlite">sqlite</a>, <a href="https://simonwillison.net/tags/thomas-ptacek">thomas-ptacek</a>, <a href="https://simonwillison.net/tags/fly">fly</a>, <a href="https://simonwillison.net/tags/litestream">litestream</a></p>]]></description><pubDate>Thu, 15 Jan 2026 16:08:27 +0000</pubDate></item><item><title>Quoting Boaz Barak, Gabriel Wu, Jeremy Chen and Manas Joglekar</title><link>https://simonwillison.net/2026/Jan/15/boaz-barak-gabriel-wu-jeremy-chen-and-manas-joglekar/#atom-everything</link><description><![CDATA[<blockquote cite="https://alignment.openai.com/confessions/"><p>When we optimize responses using a reward model as a proxy for ‚Äúgoodness‚Äù in reinforcement learning, models sometimes learn to ‚Äúhack‚Äù this proxy and output an answer that only ‚Äúlooks good‚Äù to it (because coming up with an answer that is actually good can be hard). The philosophy behind confessions is that we can train models to produce a second output ‚Äî aka a ‚Äúconfession‚Äù ‚Äî that is rewarded solely for honesty, which we will argue is less likely hacked than the normal task reward function. One way to think of confessions is that we are giving the model access to an ‚Äúanonymous tip line‚Äù where it can turn itself in by presenting incriminating evidence of misbehavior. But unlike real-world tip lines, if the model acted badly in the original task, it can collect the reward for turning itself in while still keeping the original reward from the bad behavior in the main task. We hypothesize that this form of training will teach models to produce maximally honest confessions.</p></blockquote>
<p class="cite">&mdash; <a href="https://alignment.openai.com/confessions/">Boaz Barak, Gabriel Wu, Jeremy Chen and Manas Joglekar</a>, OpenAI: Why we are excited about confessions</p>

    <p>Tags: <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a></p>]]></description><pubDate>Thu, 15 Jan 2026 00:56:27 +0000</pubDate></item><item><title>Claude Cowork Exfiltrates Files</title><link>https://simonwillison.net/2026/Jan/14/claude-cowork-exfiltrates-files/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.promptarmor.com/resources/claude-cowork-exfiltrates-files">Claude Cowork Exfiltrates Files</a></strong></p>
Claude Cowork defaults to allowing outbound HTTP traffic to only a specific list of domains, to help protect the user against prompt injection attacks that exfiltrate their data.</p>
<p>Prompt Armor found a creative workaround: Anthropic's API domain is on that list, so they constructed an attack that includes an attacker's own Anthropic API key and has the agent upload any files it can see to the <code>https://api.anthropic.com/v1/files</code> endpoint, allowing the attacker to retrieve their content later.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=46622328">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/security">security</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/prompt-injection">prompt-injection</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/exfiltration-attacks">exfiltration-attacks</a>, <a href="https://simonwillison.net/tags/ai-agents">ai-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/lethal-trifecta">lethal-trifecta</a>, <a href="https://simonwillison.net/tags/claude-cowork">claude-cowork</a></p>]]></description><pubDate>Wed, 14 Jan 2026 22:15:22 +0000</pubDate></item><item><title>Anthropic invests $1.5 million in the Python Software Foundation and open source security</title><link>https://simonwillison.net/2026/Jan/13/anthropic-invests-15-million-in-the-python-software-foundation-a/#atom-everything</link><description><![CDATA[<p><strong><a href="https://pyfound.blogspot.com/2025/12/anthropic-invests-in-python.html?m=1">Anthropic invests $1.5 million in the Python Software Foundation and open source security</a></strong></p>
This is outstanding news, especially given our decision to withdraw from that NSF grant application <a href="https://simonwillison.net/2025/Oct/27/psf-withdrawn-proposal/">back in October</a>.</p>
<blockquote>
<p>We are thrilled to announce that Anthropic has entered into a two-year partnership with the Python Software Foundation (PSF) to contribute a landmark total of $1.5 million to support the foundation‚Äôs work, with an emphasis on Python ecosystem security. This investment will enable the PSF to make crucial security advances to CPython and the Python Package Index (PyPI) benefiting all users, and it will also sustain the foundation‚Äôs core work supporting the Python language, ecosystem, and global community.</p>
</blockquote>
<p>Note that while security is a focus these funds will also support other aspects of the PSF's work:</p>
<blockquote>
<p>Anthropic‚Äôs support will also go towards the PSF‚Äôs core work, including the Developer in Residence program driving contributions to CPython, community support through grants and other programs, running core infrastructure such as PyPI, and more.</p>
</blockquote>


    <p>Tags: <a href="https://simonwillison.net/tags/open-source">open-source</a>, <a href="https://simonwillison.net/tags/python">python</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/psf">psf</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a></p>]]></description><pubDate>Tue, 13 Jan 2026 23:58:17 +0000</pubDate></item><item><title>Superhuman AI Exfiltrates Emails</title><link>https://simonwillison.net/2026/Jan/12/superhuman-ai-exfiltrates-emails/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.promptarmor.com/resources/superhuman-ai-exfiltrates-emails">Superhuman AI Exfiltrates Emails</a></strong></p>
Classic prompt injection attack:</p>
<blockquote>
<p>When asked to summarize the user‚Äôs recent mail, a prompt injection in an untrusted email manipulated Superhuman AI to submit content from dozens of other sensitive emails (including financial, legal, and medical information) in the user‚Äôs inbox to an attacker‚Äôs Google Form.</p>
</blockquote>
<p>To Superhuman's credit they treated this as the high priority incident it is and issued a fix.</p>
<p>The root cause was a CSP rule that allowed markdown images to be loaded from <code>docs.google.com</code> - it turns out Google Forms on that domain will persist data fed to them via a GET request!

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=46592424">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/security">security</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/prompt-injection">prompt-injection</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/exfiltration-attacks">exfiltration-attacks</a>, <a href="https://simonwillison.net/tags/content-security-policy">content-security-policy</a></p>]]></description><pubDate>Mon, 12 Jan 2026 22:24:54 +0000</pubDate></item></channel></rss>