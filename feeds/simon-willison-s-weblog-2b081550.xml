<?xml version="1.0" encoding="utf-8"?><rss version="2.0"><channel><title>Simon Willison&apos;s Weblog</title><link>https://raw.githubusercontent.com/xavwe/rss-aggregator/refs/heads/main/feeds/simon-willison-s-weblog-2b081550.xml</link><description>Archived feed from https://simonwillison.net/atom/everything</description><item><title>How I think about Codex</title><link>https://simonwillison.net/2026/Feb/22/how-i-think-about-codex/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.linkedin.com/pulse/how-i-think-codex-gabriel-chua-ukhic">How I think about Codex</a></strong></p>
Gabriel Chua (Developer Experience Engineer for APAC at OpenAI) provides his take on the confusing terminology behind the term "Codex", which can refer to a bunch of of different things within the OpenAI ecosystem:</p>
<blockquote>
<p>In plain terms, Codex is OpenAI‚Äôs software engineering agent, available through multiple interfaces, and an agent is a model plus instructions and tools, wrapped in a runtime that can execute tasks on your behalf. [...]</p>
<p>At a high level, I see Codex as three parts working together:</p>
<p><em>Codex = Model + Harness + Surfaces</em> [...]</p>
<ul>
<li>Model + Harness = the Agent</li>
<li>Surfaces = how you interact with the Agent</li>
</ul>
</blockquote>
<p>He defines the harness as "the collection of instructions and tools", which is notably open source and lives in the <a href="https://github.com/openai/codex">openai/codex</a> repository.</p>
<p>Gabriel also provides the first acknowledgment I've seen from an OpenAI insider that the Codex model family are directly trained for the Codex harness:</p>
<blockquote>
<p>Codex models are trained in the presence of the harness. Tool use, execution loops, compaction, and iterative verification aren‚Äôt bolted on behaviors ‚Äî they‚Äôre part of how the model learns to operate. The harness, in turn, is shaped around how the model plans, invokes tools, and recovers from failure.</p>
</blockquote>


    <p>Tags: <a href="https://simonwillison.net/tags/definitions">definitions</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/codex-cli">codex-cli</a></p>]]></description><pubDate>Sun, 22 Feb 2026 15:53:43 +0000</pubDate></item><item><title>Quoting Thibault Sottiaux</title><link>https://simonwillison.net/2026/Feb/21/thibault-sottiaux/#atom-everything</link><description><![CDATA[<blockquote cite="https://twitter.com/thsottiaux/status/2024947946849186064"><p>We‚Äôve made GPT-5.3-Codex-Spark about 30% faster. It is now serving at over 1200 tokens per second.</p></blockquote>
<p class="cite">&mdash; <a href="https://twitter.com/thsottiaux/status/2024947946849186064">Thibault Sottiaux</a>, OpenAI</p>

    <p>Tags: <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llm-performance">llm-performance</a></p>]]></description><pubDate>Sat, 21 Feb 2026 01:30:21 +0000</pubDate></item><item><title>Andrej Karpathy talks about &quot;Claws&quot;</title><link>https://simonwillison.net/2026/Feb/21/claws/#atom-everything</link><description><![CDATA[<p><strong><a href="https://twitter.com/karpathy/status/2024987174077432126">Andrej Karpathy talks about &quot;Claws&quot;</a></strong></p>
Andrej Karpathy tweeted a mini-essay about buying a Mac Mini ("The apple store person told me they are selling like hotcakes and everyone is confused") to tinker with Claws:</p>
<blockquote>
<p>I'm definitely a bit sus'd to run OpenClaw specifically [...] But I do love the concept and I think that just like LLM agents were a new layer on top of LLMs, Claws are now a new layer on top of LLM agents, taking the orchestration, scheduling, context, tool calls and a kind of persistence to a next level.</p>
<p>Looking around, and given that the high level idea is clear, there are a lot of smaller Claws starting to pop out. For example, on a quick skim NanoClaw looks really interesting in that the core engine is ~4000 lines of code (fits into both my head and that of AI agents, so it feels manageable, auditable, flexible, etc.) and runs everything in containers by default. [...]</p>
<p>Anyway there are many others - e.g. nanobot, zeroclaw, ironclaw, picoclaw (lol @ prefixes). [...]</p>
<p>Not 100% sure what my setup ends up looking like just yet but Claws are an awesome, exciting new layer of the AI stack.</p>
</blockquote>
<p>Andrej has an ear for fresh terminology (see <a href="https://simonwillison.net/2025/Mar/19/vibe-coding/">vibe coding</a>, <a href="https://simonwillison.net/2026/Feb/11/glm-5/">agentic engineering</a>) and I think he's right about this one, too: "<strong>Claw</strong>" is becoming a term of art for the entire category of OpenClaw-like agent systems - AI agents that generally run on personal hardware, communicate via messaging protocols and can both act on direct instructions and schedule tasks.</p>
<p>It even comes with an established emoji ü¶û


    <p>Tags: <a href="https://simonwillison.net/tags/definitions">definitions</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/andrej-karpathy">andrej-karpathy</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-agents">ai-agents</a>, <a href="https://simonwillison.net/tags/openclaw">openclaw</a></p>]]></description><pubDate>Sat, 21 Feb 2026 00:37:45 +0000</pubDate></item><item><title>Adding TILs, releases, museums, tools and research to my blog</title><link>https://simonwillison.net/2026/Feb/20/beats/#atom-everything</link><description><![CDATA[<p>I've been wanting to add indications of my various other online activities to my blog for a while now. I just turned on a new feature I'm calling "beats" (after story beats, naming this was hard!) which adds five new types of content to my site, all corresponding to activity elsewhere.</p>
<p>Here's what beats look like:</p>
<p><img src="https://static.simonwillison.net/static/2026/three-beats.jpg" alt="Screenshot of a fragment of a page showing three entries from 30th Dec 2025. First: [RELEASE] &quot;datasette-turnstile 0.1a0 ‚Äî Configurable CAPTCHAs for Datasette paths usin‚Ä¶&quot; at 7:23 pm. Second: [TOOL] &quot;Software Heritage Repository Retriever ‚Äî Download archived Git repositories f‚Ä¶&quot; at 11:41 pm. Third: [TIL] &quot;Downloading archived Git repositories from archive.softwareheritage.org ‚Äî ‚Ä¶&quot; at 11:43 pm." style="max-width: 100%;" /></p>
<p>Those three are from <a href="https://simonwillison.net/2025/Dec/30/">the 30th December 2025</a> archive page.</p>
<p>Beats are little inline links with badges that fit into different content timeline views around my site, including the homepage, search and archive pages.</p>
<p>There are currently five types of beats:</p>
<ul>
<li>
<a href="https://simonwillison.net/elsewhere/release/">Releases</a> are GitHub releases of my many different open source projects, imported from <a href="https://github.com/simonw/simonw/blob/main/releases_cache.json">this JSON file</a> that was constructed <a href="https://simonwillison.net/2020/Jul/10/self-updating-profile-readme/">by GitHub Actions</a>.</li>
<li>
<a href="https://simonwillison.net/elsewhere/til/">TILs</a> are the posts from my <a href="https://til.simonwillison.net/">TIL blog</a>, imported using <a href="https://github.com/simonw/simonwillisonblog/blob/f883b92be23892d082de39dbada571e406f5cfbf/blog/views.py#L1169">a SQL query over JSON and HTTP</a> against the Datasette instance powering that site.</li>
<li>
<a href="https://simonwillison.net/elsewhere/museum/">Museums</a> are new posts on my <a href="https://www.niche-museums.com/">niche-museums.com</a> blog, imported from <a href="https://github.com/simonw/museums/blob/909bef71cc8d336bf4ac1f13574db67a6e1b3166/plugins/export.py">this custom JSON feed</a>.</li>
<li>
<a href="https://simonwillison.net/elsewhere/tool/">Tools</a> are HTML and JavaScript tools I've vibe-coded on my <a href="https://tools.simonwillison.net/">tools.simonwillison.net</a> site, as described in <a href="https://simonwillison.net/2025/Dec/10/html-tools/">Useful patterns for building HTML tools</a>.</li>
<li>
<a href="https://simonwillison.net/elsewhere/research/">Research</a> is for AI-generated research projects, hosted in my <a href="https://github.com/simonw/research">simonw/research repo</a> and described in <a href="https://simonwillison.net/2025/Nov/6/async-code-research/">Code research projects with async coding agents like Claude Code and Codex</a>.</li>
</ul>
<p>That's five different custom integrations to pull in all of that data. The good news is that this kind of integration project is the kind of thing that coding agents <em>really</em> excel at. I knocked most of the feature out in a single morning while working in parallel on various other things.</p>
<p>I didn't have a useful structured feed of my Research projects, and it didn't matter because I gave Claude Code a link to <a href="https://raw.githubusercontent.com/simonw/research/refs/heads/main/README.md">the raw Markdown README</a> that lists them all and it <a href="https://github.com/simonw/simonwillisonblog/blob/f883b92be23892d082de39dbada571e406f5cfbf/blog/importers.py#L77-L80">spun up a parser regex</a>. Since I'm responsible for both the source and the destination I'm fine with a brittle solution that would be too risky against a source that I don't control myself.</p>
<p>Claude also handled all of the potentially tedious UI integration work with my site, making sure the new content worked on all of my different page types and was handled correctly by my <a href="https://simonwillison.net/2017/Oct/5/django-postgresql-faceted-search/">faceted search engine</a>.</p>
<h4 id="prototyping-with-claude-artifacts">Prototyping with Claude Artifacts</h4>
<p>I actually prototyped the initial concept for beats in regular Claude - not Claude Code - taking advantage of the fact that it can clone public repos from GitHub these days. I started with:</p>
<blockquote>
<p><code>Clone simonw/simonwillisonblog and tell me about the models and views</code></p>
</blockquote>
<p>And then later in the brainstorming session said:</p>
<blockquote>
<p><code>use the templates and CSS in this repo to create a new artifact with all HTML and CSS inline that shows me my homepage with some of those inline content types mixed in</code></p>
</blockquote>
<p>After some iteration we got to <a href="https://gisthost.github.io/?c3f443cc4451cf8ce03a2715a43581a4/preview.html">this artifact mockup</a>, which was enough to convince me that the concept had legs and was worth handing over to full <a href="https://code.claude.com/docs/en/claude-code-on-the-web">Claude Code for web</a> to implement.</p>
<p>If you want to see how the rest of the build played out the most interesting PRs are <a href="https://github.com/simonw/simonwillisonblog/pull/592">Beats #592</a> which implemented the core feature and <a href="https://github.com/simonw/simonwillisonblog/pull/595/changes">Add Museums Beat importer #595</a> which added the Museums content type.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/blogging">blogging</a>, <a href="https://simonwillison.net/tags/museums">museums</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/til">til</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/claude-artifacts">claude-artifacts</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a></p>]]></description><pubDate>Fri, 20 Feb 2026 23:47:10 +0000</pubDate></item><item><title>Taalas serves Llama 3.1 8B at 17,000 tokens/second</title><link>https://simonwillison.net/2026/Feb/20/taalas/#atom-everything</link><description><![CDATA[<p><strong><a href="https://taalas.com/the-path-to-ubiquitous-ai/">Taalas serves Llama 3.1 8B at 17,000 tokens/second</a></strong></p>
This new Canadian hardware startup just announced their first product - a custom hardware implementation of the Llama 3.1 8B model (from <a href="https://simonwillison.net/2024/Jul/23/introducing-llama-31/">July 2024</a>) that can run at a staggering 17,000 tokens/second.</p>
<p>I was going to include a video of their demo but it's so fast it would look more like a screenshot. You can try it out at <a href="https://chatjimmy.ai">chatjimmy.ai</a>.</p>
<p>They describe their Silicon Llama as ‚Äúaggressively quantized, combining 3-bit and 6-bit parameters.‚Äù Their next generation will use 4-bit - presumably they have quite a long lead time for baking out new models!

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=47086181">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llama">llama</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/llm-performance">llm-performance</a></p>]]></description><pubDate>Fri, 20 Feb 2026 22:10:04 +0000</pubDate></item><item><title>ggml.ai joins Hugging Face to ensure the long-term progress of Local AI</title><link>https://simonwillison.net/2026/Feb/20/ggmlai-joins-hugging-face/#atom-everything</link><description><![CDATA[<p><strong><a href="https://github.com/ggml-org/llama.cpp/discussions/19759">ggml.ai joins Hugging Face to ensure the long-term progress of Local AI</a></strong></p>
I don't normally cover acquisition news like this, but I have some thoughts.</p>
<p>It's hard to overstate the impact Georgi Gerganov has had on the local model space. Back in March 2023 his release of <a href="https://github.com/ggml-org/llama.cpp">llama.cpp</a> made it possible to run a local LLM on consumer hardware. The <a href="https://github.com/ggml-org/llama.cpp/blob/775328064e69db1ebd7e19ccb59d2a7fa6142470/README.md?plain=1#L7">original README</a> said:</p>
<blockquote>
<p>The main goal is to run the model using 4-bit quantization on a MacBook. [...] This was hacked in an evening - I have no idea if it works correctly.</p>
</blockquote>
<p>I wrote about trying llama.cpp out at the time in <a href="https://simonwillison.net/2023/Mar/11/llama/#llama-cpp">Large language models are having their Stable Diffusion moment</a>:</p>
<blockquote>
<p>I used it to run the 7B LLaMA model on my laptop last night, and then this morning upgraded to the 13B model‚Äîthe one that Facebook claim is competitive with GPT-3.</p>
</blockquote>
<p>Meta's <a href="https://github.com/meta-llama/llama/tree/llama_v1">original LLaMA release</a> depended on PyTorch and their <a href="https://github.com/facebookresearch/fairscale">FairScale</a> PyTorch extension for running on multiple GPUs, and required CUDA and NVIDIA hardware. Georgi's work opened that up to a much wider range of hardware and kicked off the local model movement that has continued to grow since then.</p>
<p>Hugging Face are already responsible for the incredibly influential <a href="https://github.com/huggingface/transformers">Transformers</a> library used by the majority of LLM releases today. They've proven themselves a good steward for that open source project, which makes me optimistic for the future of llama.cpp and related projects.</p>
<p>This section from the announcement looks particularly promising:</p>
<blockquote>
<p>Going forward, our joint efforts will be geared towards the following objectives:</p>
<ul>
<li>Towards seamless "single-click" integration with the <a href="https://github.com/huggingface/transformers">transformers</a> library. The <code>transformers</code> framework has established itself as the 'source of truth' for AI model definitions. Improving the compatibility between the transformers and the ggml ecosystems is essential for wider model support and quality control.</li>
<li>Better packaging and user experience of ggml-based software. As we enter the phase in which local inference becomes a meaningful and competitive alternative to cloud inference, it is crucial to improve and simplify the way in which casual users deploy and access local models. We will work towards making llama.cpp ubiquitous and readily available everywhere, and continue partnering with great downstream projects.</li>
</ul>
</blockquote>
<p>Given the influence of Transformers, this closer integration could lead to model releases that are compatible with the GGML ecosystem out of the box. That would be a big win for the local model ecosystem.</p>
<p>I'm also excited to see investment in "packaging and user experience of ggml-based software". This has mostly been left to tools like <a href="https://ollama.com">Ollama</a> and <a href="https://lmstudio.ai">LM Studio</a>. ggml-org released <a href="https://github.com/ggml-org/LlamaBarn">LlamaBarn</a> last year - "a macOS menu bar app for running local LLMs" - and I'm hopeful that further investment in this area will result in more high quality open source tools for running local models from the team best placed to deliver them.

    <p><small></small>Via <a href="https://twitter.com/ggerganov/status/2024839991482777976">@ggerganov</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/open-source">open-source</a>, <a href="https://simonwillison.net/tags/transformers">transformers</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llama">llama</a>, <a href="https://simonwillison.net/tags/local-llms">local-llms</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/hugging-face">hugging-face</a>, <a href="https://simonwillison.net/tags/llama-cpp">llama-cpp</a></p>]]></description><pubDate>Fri, 20 Feb 2026 17:12:55 +0000</pubDate></item><item><title>Quoting Thariq Shihipar</title><link>https://simonwillison.net/2026/Feb/20/thariq-shihipar/#atom-everything</link><description><![CDATA[<blockquote cite="https://twitter.com/trq212/status/2024574133011673516"><p>Long running agentic products like Claude Code are made feasible by prompt caching which allows us to reuse computation from previous roundtrips and significantly decrease latency and cost. [...]</p>
<p>At Claude Code, we build our entire harness around prompt caching. A high prompt cache hit rate decreases costs and helps us create more generous rate limits for our subscription plans, so we run alerts on our prompt cache hit rate and declare SEVs if they're too low.</p></blockquote>
<p class="cite">&mdash; <a href="https://twitter.com/trq212/status/2024574133011673516">Thariq Shihipar</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/prompt-engineering">prompt-engineering</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/ai-agents">ai-agents</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Fri, 20 Feb 2026 07:13:19 +0000</pubDate></item><item><title>Recovering lost code</title><link>https://simonwillison.net/2026/Feb/19/recovering-lost-code/#atom-everything</link><description><![CDATA[<p>Reached the stage of parallel agent psychosis where I've lost a whole feature - I know I had it yesterday, but I can't seem to find the branch or worktree or cloud instance or checkout with it in.</p>
<p>... found it! Turns out I'd been hacking on a random prototype in <code>/tmp</code> and then my computer crashed and rebooted and I lost the code... but it's all still there in <code>~/.claude/projects/</code> session logs and Claude Code can extract it out and spin up the missing feature again.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/parallel-agents">parallel-agents</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Thu, 19 Feb 2026 23:48:35 +0000</pubDate></item><item><title>Gemini 3.1 Pro</title><link>https://simonwillison.net/2026/Feb/19/gemini-31-pro/#atom-everything</link><description><![CDATA[<p><strong><a href="https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-1-pro/">Gemini 3.1 Pro</a></strong></p>
The first in the Gemini 3.1 series, priced the same as Gemini 3 Pro ($2/million input, $12/million output under 200,000 tokens, $4/$18 for 200,000 to 1,000,000). That's less than half the price of Claude Opus 4.6 with very similar benchmark scores to that model.</p>
<p>They boast about its improved SVG animation performance compared to Gemini 3 Pro in the announcement!</p>
<p>I tried "Generate an SVG of a pelican riding a bicycle" <a href="https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221ugF9fBfLGxnNoe8_rLlluzo9NSPJDWuF%22%5D,%22action%22:%22open%22,%22userId%22:%22106366615678321494423%22,%22resourceKeys%22:%7B%7D%7D&amp;usp=sharing">in Google AI Studio</a> and it thought for 323.9 seconds (<a href="https://gist.github.com/simonw/03a755865021739a3659943a22c125ba#thinking-trace">thinking trace here</a>) before producing this one:</p>
<p><img alt="Whimsical flat-style illustration of a pelican wearing a blue and white baseball cap, riding a red bicycle with yellow-rimmed wheels along a road. The pelican has a large orange bill and a green scarf. A small fish peeks out of a brown basket on the handlebars. The background features a light blue sky with a yellow sun, white clouds, and green hills." src="https://static.simonwillison.net/static/2026/gemini-3.1-pro-pelican.png" /></p>
<p>It's good to see the legs clearly depicted on both sides of the frame (should <a href="https://twitter.com/elonmusk/status/2023833496804839808">satisfy Elon</a>), the fish in the basket is a nice touch and I appreciated this comment in <a href="https://gist.github.com/simonw/03a755865021739a3659943a22c125ba#response">the SVG code</a>:</p>
<pre><code>&lt;!-- Black Flight Feathers on Wing Tip --&gt;
&lt;path d="M 420 175 C 440 182, 460 187, 470 190 C 450 210, 430 208, 410 198 Z" fill="#374151" /&gt;
</code></pre>
<p>I've <a href="https://github.com/simonw/llm-gemini/issues/121">added</a> the two new model IDs <code>gemini-3.1-pro-preview</code> and <code>gemini-3.1-pro-preview-customtools</code> to my <a href="https://github.com/simonw/llm-gemini">llm-gemini plugin</a> for <a href="https://llm.datasette.io/">LLM</a>. That "custom tools" one is <a href="https://ai.google.dev/gemini-api/docs/models/gemini-3.1-pro-preview#gemini-31-pro-preview-customtools">described here</a> - apparently it may provide better tool performance than the default model in some situations.</p>
<p>The model appears to be <em>incredibly</em> slow right now - it took 104s to respond to a simple "hi" and a few of my other tests met "Error: This model is currently experiencing high demand. Spikes in demand are usually temporary. Please try again later." or "Error: Deadline expired before operation could complete" errors. I'm assuming that's just teething problems on launch day.</p>
<p>It sounds like last week's <a href="https://simonwillison.net/2026/Feb/12/gemini-3-deep-think/">Deep Think release</a> was our first exposure to the 3.1 family:</p>
<blockquote>
<p>Last week, we released a major update to Gemini 3 Deep Think to solve modern challenges across science, research and engineering. Today, we‚Äôre releasing the upgraded core intelligence that makes those breakthroughs possible: Gemini 3.1 Pro.</p>
</blockquote>
<p><strong>Update</strong>: In <a href="https://simonwillison.net/2025/nov/13/training-for-pelicans-riding-bicycles/">What happens if AI labs train for pelicans riding bicycles?</a> last November I said:</p>
<blockquote>
<p>If a model finally comes out that produces an excellent SVG of a pelican riding a bicycle you can bet I‚Äôm going to test it on all manner of creatures riding all sorts of transportation devices.</p>
</blockquote>
<p>Google's Gemini Lead Jeff Dean <a href="https://x.com/JeffDean/status/2024525132266688757">tweeted this video</a> featuring an animated pelican riding a bicycle, plus a frog on a penny-farthing and a giraffe driving a tiny car and an ostrich on roller skates and a turtle kickflipping a skateboard and a dachshund driving a stretch limousine.</p>
<video style="margin-bottom: 1em" poster="https://static.simonwillison.net/static/2026/gemini-animated-pelicans.jpg" muted controls preload="none" style="max-width: 100%">
  <source src="https://static.simonwillison.net/static/2026/gemini-animated-pelicans.mp4" type="video/mp4">
</video>

<p>I've been saying for a while that I wish AI labs would highlight things that their new models can do that their older models could not, so top marks to the Gemini team for this video.</p>
<p><strong>Update 2</strong>: I used <code>llm-gemini</code> to run my <a href="https://simonwillison.net/2025/Nov/18/gemini-3/#and-a-new-pelican-benchmark">more detailed Pelican prompt</a>, with <a href="https://gist.github.com/simonw/a3bdd4ec9476ba9e9ba7aa61b46d8296">this result</a>:</p>
<p><img alt="Flat-style illustration of a brown pelican riding a teal bicycle with dark blue-rimmed wheels against a plain white background. Unlike the previous image's white cartoon pelican, this pelican has realistic brown plumage with detailed feather patterns, a dark maroon head, yellow eye, and a large pink-tinged pouch bill. The bicycle is a simpler design without a basket, and the scene lacks the colorful background elements like the sun, clouds, road, hills, cap, and scarf from the first illustration, giving it a more minimalist feel." src="https://static.simonwillison.net/static/2026/gemini-3.1-pro-pelican-2.png" /></p>
<p>From the SVG comments:</p>
<pre><code>&lt;!-- Pouch Gradient (Breeding Plumage: Red to Olive/Green) --&gt;
...
&lt;!-- Neck Gradient (Breeding Plumage: Chestnut Nape, White/Yellow Front) --&gt;
</code></pre>


    <p>Tags: <a href="https://simonwillison.net/tags/google">google</a>, <a href="https://simonwillison.net/tags/svg">svg</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/llm">llm</a>, <a href="https://simonwillison.net/tags/gemini">gemini</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a></p>]]></description><pubDate>Thu, 19 Feb 2026 17:58:37 +0000</pubDate></item><item><title>Experimenting with sponsorship for my blog and newsletter</title><link>https://simonwillison.net/2026/Feb/19/sponsorship/#atom-everything</link><description><![CDATA[<p>I've long been resistant to the idea of accepting sponsorship for my blog. I value my credibility as an independent voice, and I don't want to risk compromising that reputation.</p>
<p>Then I learned about Troy Hunt's <a href="https://www.troyhunt.com/sponsorship/">approach to sponsorship</a>, which he first wrote about <a href="https://www.troyhunt.com/im-now-offering-sponsorship-of-this-blog/">in 2016</a>. Troy runs with a simple text row in the page banner - no JavaScript, no cookies, unobtrusive while providing value to the sponsor. I can live with that!</p>
<p>Accepting sponsorship in this way helps me maintain my independence while offsetting the opportunity cost of not taking a full-time job.</p>
<p>To start with I'm selling sponsorship by the week. Sponsors get that unobtrusive banner across my blog and also their sponsored message at the top of <a href="https://simonw.substack.com/">my newsletter</a>.</p>
<p><img alt="Screenshot of my blog's homepage. Below the Simon Willison's Weblog heading and list of tags is a new blue page-wide banner reading &quot;Sponsored by: Teleport - Secure, Govern, and Operate Al at Engineering Scale. Learn more&quot;." src="https://static.simonwillison.net/static/2026/sponsor-banner.jpg" /></p>
<p>I <strong>will not write content in exchange for sponsorship</strong>. I hope the sponsors I work with understand that my credibility as an independent voice is a key reason I have an audience, and compromising that trust would be bad for everyone.</p>
<p><a href="https://www.freemanandforrest.com/">Freeman &amp; Forrest</a> helped me set up and sell my first slots. Thanks also to <a href="https://t3.gg/">Theo Browne</a> for helping me think through my approach.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/newsletter">newsletter</a>, <a href="https://simonwillison.net/tags/blogging">blogging</a>, <a href="https://simonwillison.net/tags/troy-hunt">troy-hunt</a></p>]]></description><pubDate>Thu, 19 Feb 2026 05:44:29 +0000</pubDate></item><item><title>SWE-bench February 2026 leaderboard update</title><link>https://simonwillison.net/2026/Feb/19/swe-bench/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.swebench.com/">SWE-bench February 2026 leaderboard update</a></strong></p>
SWE-bench is one of the benchmarks that the labs love to list in their model releases. The official leaderboard is infrequently updated but they just did a full run of it against the current generation of models, which is notable because it's always good to see benchmark results like this that <em>weren't</em> self-reported by the labs.</p>
<p>The fresh results are for their "Bash Only" benchmark, which runs their <a href="https://github.com/SWE-agent/mini-swe-agent">mini-swe-bench</a> agent (~9,000 lines of Python, <a href="https://github.com/SWE-agent/mini-swe-agent/blob/v2.2.1/src/minisweagent/config/benchmarks/swebench.yaml">here are the prompts</a> they use) against the <a href="https://huggingface.co/datasets/princeton-nlp/SWE-bench">SWE-bench</a> dataset of coding problems - 2,294 real-world examples pulled from 12 open source repos: <a href="https://github.com/django/django">django/django</a> (850), <a href="https://github.com/sympy/sympy">sympy/sympy</a> (386), <a href="https://github.com/scikit-learn/scikit-learn">scikit-learn/scikit-learn</a> (229), <a href="https://github.com/sphinx-doc/sphinx">sphinx-doc/sphinx</a> (187), <a href="https://github.com/matplotlib/matplotlib">matplotlib/matplotlib</a> (184), <a href="https://github.com/pytest-dev/pytest">pytest-dev/pytest</a> (119), <a href="https://github.com/pydata/xarray">pydata/xarray</a> (110), <a href="https://github.com/astropy/astropy">astropy/astropy</a> (95), <a href="https://github.com/pylint-dev/pylint">pylint-dev/pylint</a> (57), <a href="https://github.com/psf/requests">psf/requests</a> (44), <a href="https://github.com/mwaskom/seaborn">mwaskom/seaborn</a> (22), <a href="https://github.com/pallets/flask">pallets/flask</a> (11).</p>
<p><strong>Correction</strong>: <em>The Bash only benchmark runs against SWE-bench Verified, not original SWE-bench. Verified is a manually curated subset of 500 samples <a href="https://openai.com/index/introducing-swe-bench-verified/">described here</a>, funded by OpenAI. Here's <a href="https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified">SWE-bench Verified</a> on Hugging Face - since it's just 2.1MB of Parquet it's easy to browse <a href="https://lite.datasette.io/?parquet=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fprinceton-nlp%2FSWE-bench_Verified%2Fresolve%2Fmain%2Fdata%2Ftest-00000-of-00001.parquet#/data/test-00000-of-00001?_facet=repo">using Datasette Lite</a>, which cuts those numbers down to django/django (231), sympy/sympy (75), sphinx-doc/sphinx (44), matplotlib/matplotlib (34), scikit-learn/scikit-learn (32), astropy/astropy (22), pydata/xarray (22), pytest-dev/pytest (19), pylint-dev/pylint (10), psf/requests (8), mwaskom/seaborn (2), pallets/flask (1)</em>.</p>
<p>Here's how the top ten models performed:</p>
<p><img alt="Bar chart showing &quot;% Resolved&quot; by &quot;Model&quot;. Bars in descending order: Claude 4.5 Opus (high reasoning) 76.8%, Gemini 3 Flash (high reasoning) 75.8%, MiniMax M2.5 (high reasoning) 75.8%, Claude Opus 4.6 75.6%, GLM-5 (high reasoning) 72.8%, GPT-5.2 (high reasoning) 72.8%, Claude 4.5 Sonnet (high reasoning) 72.8%, Kimi K2.5 (high reasoning) 71.4%, DeepSeek V3.2 (high reasoning) 70.8%, Claude 4.5 Haiku (high reasoning) 70.0%, and a partially visible final bar at 66.6%." src="https://static.simonwillison.net/static/2026/swbench-feb-2026.jpg" /></p>
<p>It's interesting to see Claude Opus 4.5 beat Opus 4.6, though only by about a percentage point. 4.5 Opus is top, then Gemini 3 Flash, then MiniMax M2.5 - a 229B model released <a href="https://www.minimax.io/news/minimax-m25">last week</a> by Chinese lab MiniMax. GLM-5, Kimi K2.5 and DeepSeek V3.2 are three more Chinese models that make the top ten as well.</p>
<p>OpenAI's GPT-5.2 is their highest performing model at position 6, but it's worth noting that their best coding model, GPT-5.3-Codex, is not represented - maybe because it's not yet available in the OpenAI API.</p>
<p>This benchmark uses the same system prompt for every model, which is important for a fair comparison but does mean that the quality of the different harnesses or optimized prompts is not being measured here.</p>
<p>The chart above is a screenshot from the SWE-bench website, but their charts don't include the actual percentage values visible on the bars. I successfully used Claude for Chrome to add these - <a href="https://claude.ai/share/81a0c519-c727-4caa-b0d4-0d866375d0da">transcript here</a>. My prompt sequence included:</p>
<blockquote>
<p>Use claude in chrome to open https://www.swebench.com/</p>
<p>Click on "Compare results" and then select "Select top 10"</p>
<p>See those bar charts? I want them to display the percentage on each bar so I can take a better screenshot, modify the page like that</p>
</blockquote>
<p>I'm impressed at how well this worked - Claude injected custom JavaScript into the page to draw additional labels on top of the existing chart.</p>
<p><img alt="Screenshot of a Claude AI conversation showing browser automation. A thinking step reads &quot;Pivoted strategy to avoid recursion issues with chart labeling &gt;&quot; followed by the message &quot;Good, the chart is back. Now let me carefully add the labels using an inline plugin on the chart instance to avoid the recursion issue.&quot; A collapsed &quot;Browser_evaluate&quot; section shows a browser_evaluate tool call with JavaScript code using Chart.js canvas context to draw percentage labels on bars: meta.data.forEach((bar, index) =&gt; { const value = dataset.data[index]; if (value !== undefined &amp;&amp; value !== null) { ctx.save(); ctx.textAlign = 'center'; ctx.textBaseline = 'bottom'; ctx.fillStyle = '#333'; ctx.font = 'bold 12px sans-serif'; ctx.fillText(value.toFixed(1) + '%', bar.x, bar.y - 5); A pending step reads &quot;Let me take a screenshot to see if it worked.&quot; followed by a completed &quot;Done&quot; step, and the message &quot;Let me take a screenshot to check the result.&quot;" src="https://static.simonwillison.net/static/2026/claude-chrome-draw-on-chart.jpg" /></p>
<p><strong>Update</strong>: If you look at the transcript Claude claims to have switched to Playwright, which is confusing because I didn't think I had that configured.

    <p><small></small>Via <a href="https://twitter.com/KLieret/status/2024176335782826336">@KLieret</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/benchmarks">benchmarks</a>, <a href="https://simonwillison.net/tags/django">django</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/ai-in-china">ai-in-china</a>, <a href="https://simonwillison.net/tags/minimax">minimax</a></p>]]></description><pubDate>Thu, 19 Feb 2026 04:48:47 +0000</pubDate></item><item><title>LadybirdBrowser/ladybird: Abandon Swift adoption</title><link>https://simonwillison.net/2026/Feb/19/ladybird/#atom-everything</link><description><![CDATA[<p><strong><a href="https://github.com/LadybirdBrowser/ladybird/commit/e87f889e31afbb5fa32c910603c7f5e781c97afd">LadybirdBrowser/ladybird: Abandon Swift adoption</a></strong></p>
Back <a href="https://simonwillison.net/2024/Aug/11/ladybird-set-to-adopt-swift/">in August 2024</a> the Ladybird browser project announced an intention to adopt Swift as their memory-safe language of choice.</p>
<p>As of <a href="https://github.com/LadybirdBrowser/ladybird/commit/e87f889e31afbb5fa32c910603c7f5e781c97afd">this commit</a> it looks like they've changed their mind:</p>
<blockquote>
<p><strong>Everywhere: Abandon Swift adoption</strong></p>
<p>After making no progress on this for a very long time, let's acknowledge it's not going anywhere and remove it from the codebase.</p>
</blockquote>

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=47067678">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ladybird">ladybird</a>, <a href="https://simonwillison.net/tags/swift">swift</a></p>]]></description><pubDate>Thu, 19 Feb 2026 01:25:33 +0000</pubDate></item><item><title>Typing without having to type</title><link>https://simonwillison.net/2026/Feb/18/typing/#atom-everything</link><description><![CDATA[<p>25+ years into my career as a programmer I think I may <em>finally</em> be coming around to preferring type hints or even strong typing. I resisted those in the past because they slowed down the rate at which I could iterate on code, especially in the REPL environments that were key to my productivity. But if a coding agent is doing all that <em>typing</em> for me, the benefits of explicitly defining all of those types are suddenly much more attractive.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/programming">programming</a>, <a href="https://simonwillison.net/tags/programming-languages">programming-languages</a>, <a href="https://simonwillison.net/tags/static-typing">static-typing</a></p>]]></description><pubDate>Wed, 18 Feb 2026 18:56:56 +0000</pubDate></item><item><title>The A.I. Disruption We‚Äôve Been Waiting for Has Arrived</title><link>https://simonwillison.net/2026/Feb/18/the-ai-disruption/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.nytimes.com/2026/02/18/opinion/ai-software.html?unlocked_article_code=1.NFA.UkLv.r-XczfzYRdXJ&amp;smid=url-share">The A.I. Disruption We‚Äôve Been Waiting for Has Arrived</a></strong></p>
New opinion piece from Paul Ford in the New York Times. Unsurprisingly for a piece by Paul it's packed with quoteworthy snippets, but a few stood out for me in particular.</p>
<p>Paul describes the <a href="https://simonwillison.net/2026/Jan/4/inflection/">November moment</a> that so many other programmers have observed, and highlights Claude Code's ability to revive old side projects:</p>
<blockquote>
<p>[Claude Code] was always a helpful coding assistant, but in November it suddenly got much better, and ever since I‚Äôve been knocking off side projects that had sat in folders for a decade or longer. It‚Äôs fun to see old ideas come to life, so I keep a steady flow. Maybe it adds up to a half-hour a day of my time, and an hour of Claude‚Äôs.</p>
<p>November was, for me and many others in tech, a great surprise. Before, A.I. coding tools were often useful, but halting and clumsy. Now, the bot can run for a full hour and make whole, designed websites and apps that may be flawed, but credible. I spent an entire session of therapy talking about it.</p>
</blockquote>
<p>And as the former CEO of a respected consultancy firm (Postlight) he's well positioned to evaluate the potential impact:</p>
<blockquote>
<p>When you watch a large language model slice through some horrible, expensive problem ‚Äî like migrating data from an old platform to a modern one ‚Äî you feel the earth shifting. I was the chief executive of a software services firm, which made me a professional software cost estimator. When I rebooted my messy personal website a few weeks ago, I realized: I would have paid $25,000 for someone else to do this. When a friend asked me to convert a large, thorny data set, I downloaded it, cleaned it up and made it pretty and easy to explore. In the past I would have charged $350,000.</p>
<p>That last price is full 2021 retail ‚Äî it implies a product manager, a designer, two engineers (one senior) and four to six months of design, coding and testing. Plus maintenance. Bespoke software is joltingly expensive. Today, though, when the stars align and my prompts work out, I can do hundreds of thousands of dollars worth of work for fun (fun for me) over weekends and evenings, for the price of the Claude $200-a-month plan.</p>
</blockquote>
<p>He also neatly captures the inherent community tension involved in exploring this technology:</p>
<blockquote>
<p>All of the people I love hate this stuff, and all the people I hate love it. And yet, likely because of the same personality flaws that drew me to technology in the first place, I am annoyingly excited.</p>
</blockquote>


    <p>Tags: <a href="https://simonwillison.net/tags/new-york-times">new-york-times</a>, <a href="https://simonwillison.net/tags/paul-ford">paul-ford</a>, <a href="https://simonwillison.net/tags/careers">careers</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a></p>]]></description><pubDate>Wed, 18 Feb 2026 17:07:31 +0000</pubDate></item><item><title>Quoting Martin Fowler</title><link>https://simonwillison.net/2026/Feb/18/martin-fowler/#atom-everything</link><description><![CDATA[<blockquote cite="https://martinfowler.com/fragments/2026-02-18.html"><p>LLMs are eating specialty skills. There will be less use of specialist front-end and back-end developers as the LLM-driving skills become more important than the details of platform usage. Will this lead to a greater recognition of the role of <a href="https://martinfowler.com/articles/expert-generalist.html">Expert Generalists</a>? Or will the ability of LLMs to write lots of code mean they code around the silos rather than eliminating them?</p></blockquote>
<p class="cite">&mdash; <a href="https://martinfowler.com/fragments/2026-02-18.html">Martin Fowler</a>, tidbits from the Thoughtworks Future of Software Development Retreat, <a href="https://news.ycombinator.com/item?id=47062534">via HN</a>)</p>

    <p>Tags: <a href="https://simonwillison.net/tags/martin-fowler">martin-fowler</a>, <a href="https://simonwillison.net/tags/careers">careers</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a></p>]]></description><pubDate>Wed, 18 Feb 2026 16:50:07 +0000</pubDate></item><item><title>Introducing Claude Sonnet 4.6</title><link>https://simonwillison.net/2026/Feb/17/claude-sonnet-46/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.anthropic.com/news/claude-sonnet-4-6">Introducing Claude Sonnet 4.6</a></strong></p>
Sonnet 4.6 is out today, and Anthropic claim it offers similar performance to <a href="https://simonwillison.net/2025/Nov/24/claude-opus/">November's Opus 4.5</a> while maintaining the Sonnet pricing of $3/million input and $15/million output tokens (the Opus models are $5/$25). Here's <a href="https://www-cdn.anthropic.com/78073f739564e986ff3e28522761a7a0b4484f84.pdf">the system card PDF</a>.</p>
<p>Sonnet 4.6 has a "reliable knowledge cutoff" of August 2025, compared to Opus 4.6's May 2025 and Haiku 4.5's February 2025. Both Opus and Sonnet default to 200,000 max input tokens but can stretch to 1 million in beta and at a higher cost.</p>
<p>I just released <a href="https://github.com/simonw/llm-anthropic/releases/tag/0.24">llm-anthropic 0.24</a> with support for both Sonnet 4.6 and Opus 4.6. Claude Code <a href="https://github.com/simonw/llm-anthropic/pull/65">did most of the work</a> - the new models had a fiddly amount of extra details around adaptive thinking and no longer supporting prefixes, as described <a href="https://platform.claude.com/docs/en/about-claude/models/migration-guide">in Anthropic's migration guide</a>.</p>
<p>Here's <a href="https://gist.github.com/simonw/b185576a95e9321b441f0a4dfc0e297c">what I got</a> from:</p>
<pre><code>uvx --with llm-anthropic llm 'Generate an SVG of a pelican riding a bicycle' -m claude-sonnet-4.6
</code></pre>
<p><img alt="The pelican has a jaunty top hat with a red band. There is a string between the upper and lower beaks for some reason. The bicycle frame is warped in the wrong way." src="https://static.simonwillison.net/static/2026/pelican-sonnet-4.6.png" /></p>
<p>The SVG comments include:</p>
<pre><code>&lt;!-- Hat (fun accessory) --&gt;
</code></pre>
<p>I tried a second time and also got a top hat. Sonnet 4.6 apparently loves top hats!</p>
<p>For comparison, here's the pelican Opus 4.5 drew me <a href="(https://simonwillison.net/2025/Nov/24/claude-opus/)">in November</a>:</p>
<p><img alt="The pelican is cute and looks pretty good. The bicycle is not great - the frame is wrong and the pelican is facing backwards when the handlebars appear to be forwards.There is also something that looks a bit like an egg on the handlebars." src="https://static.simonwillison.net/static/2025/claude-opus-4.5-pelican.jpg" /></p>
<p>And here's Anthropic's current best pelican, drawn by Opus 4.6 <a href="https://simonwillison.net/2026/Feb/5/two-new-models/">on February 5th</a>:</p>
<p><img alt="Slightly wonky bicycle frame but an excellent pelican, very clear beak and pouch, nice feathers." src="https://static.simonwillison.net/static/2026/opus-4.6-pelican.png" /></p>
<p>Opus 4.6 produces the best pelican beak/pouch. I do think the top hat from Sonnet 4.6 is a nice touch though.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=47050488">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/llm">llm</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/llm-pricing">llm-pricing</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a></p>]]></description><pubDate>Tue, 17 Feb 2026 23:58:58 +0000</pubDate></item><item><title>Rodney v0.4.0</title><link>https://simonwillison.net/2026/Feb/17/rodney/#atom-everything</link><description><![CDATA[<p><strong><a href="https://github.com/simonw/rodney/releases/tag/v0.4.0">Rodney v0.4.0</a></strong></p>
My <a href="https://github.com/simonw/rodney">Rodney</a> CLI tool for browser automation attracted quite the flurry of PRs since I announced it <a href="https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#rodney-cli-browser-automation-designed-to-work-with-showboat">last week</a>. Here are the release notes for the just-released v0.4.0:</p>
<blockquote>
<ul>
<li>Errors now use exit code 2, which means exit code 1 is just for for check failures. <a href="https://github.com/simonw/rodney/pull/15">#15</a></li>
<li>New <code>rodney assert</code> command for running JavaScript tests, exit code 1 if they fail. <a href="https://github.com/simonw/rodney/issues/19">#19</a></li>
<li>New directory-scoped sessions with <code>--local</code>/<code>--global</code> flags. <a href="https://github.com/simonw/rodney/pull/14">#14</a></li>
<li>New <code>reload --hard</code> and <code>clear-cache</code> commands. <a href="https://github.com/simonw/rodney/pull/17">#17</a></li>
<li>New <code>rodney start --show</code> option to make the browser window visible. Thanks, <a href="https://github.com/antocuni">Antonio Cuni</a>. <a href="https://github.com/simonw/rodney/paull/13">#13</a></li>
<li>New <code>rodney connect PORT</code> command to debug an already-running Chrome instance. Thanks, <a href="https://github.com/pnf">Peter Fraenkel</a>. <a href="https://github.com/simonw/rodney/pull/12">#12</a></li>
<li>New <code>RODNEY_HOME</code> environment variable to support custom state directories. Thanks, <a href="https://github.com/senko">Senko Ra≈°iƒá</a>. <a href="https://github.com/simonw/rodney/pull/11">#11</a></li>
<li>New <code>--insecure</code> flag to ignore certificate errors. Thanks, <a href="https://github.com/zgolus">Jakub Zgoli≈Ñski</a>. <a href="https://github.com/simonw/rodney/pull/10">#10</a></li>
<li>Windows support: avoid <code>Setsid</code> on Windows via build-tag helpers. Thanks, <a href="https://github.com/adm1neca">adm1neca</a>. <a href="https://github.com/simonw/rodney/pull/18">#18</a></li>
<li>Tests now run on <code>windows-latest</code> and <code>macos-latest</code> in addition to Linux.</li>
</ul>
</blockquote>
<p>I've been using <a href="https://github.com/simonw/showboat">Showboat</a> to create demos of new features - here those are for <a href="https://github.com/simonw/rodney/tree/v0.4.0/notes/assert-command-demo">rodney assert</a>, <a href="https://github.com/simonw/rodney/tree/v0.4.0/notes/clear-cache-demo">rodney reload --hard</a>, <a href="https://github.com/simonw/rodney/tree/v0.4.0/notes/error-codes-demo">rodney exit codes</a>, and <a href="https://github.com/simonw/rodney/tree/v0.4.0/notes/local-sessions-demo">rodney start --local</a>.</p>
<p>The <code>rodney assert</code> command is pretty neat: you can now Rodney to test a web app through multiple steps in a shell script that looks something like this (adapted from <a href="https://github.com/simonw/rodney/blob/v0.4.0/README.md#combining-checks-in-a-shell-script">the README</a>):</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#!</span>/bin/bash</span>
<span class="pl-c1">set</span> -euo pipefail

FAIL=0

<span class="pl-en">check</span>() {
    <span class="pl-k">if</span> <span class="pl-k">!</span> <span class="pl-s"><span class="pl-pds">"</span><span class="pl-smi">$@</span><span class="pl-pds">"</span></span><span class="pl-k">;</span> <span class="pl-k">then</span>
        <span class="pl-c1">echo</span> <span class="pl-s"><span class="pl-pds">"</span>FAIL: <span class="pl-smi">$*</span><span class="pl-pds">"</span></span>
        FAIL=1
    <span class="pl-k">fi</span>
}

rodney start
rodney open <span class="pl-s"><span class="pl-pds">"</span>https://example.com<span class="pl-pds">"</span></span>
rodney waitstable

<span class="pl-c"><span class="pl-c">#</span> Assert elements exist</span>
check rodney exists <span class="pl-s"><span class="pl-pds">"</span>h1<span class="pl-pds">"</span></span>

<span class="pl-c"><span class="pl-c">#</span> Assert key elements are visible</span>
check rodney visible <span class="pl-s"><span class="pl-pds">"</span>h1<span class="pl-pds">"</span></span>
check rodney visible <span class="pl-s"><span class="pl-pds">"</span>#main-content<span class="pl-pds">"</span></span>

<span class="pl-c"><span class="pl-c">#</span> Assert JS expressions</span>
check rodney assert <span class="pl-s"><span class="pl-pds">'</span>document.title<span class="pl-pds">'</span></span> <span class="pl-s"><span class="pl-pds">'</span>Example Domain<span class="pl-pds">'</span></span>
check rodney assert <span class="pl-s"><span class="pl-pds">'</span>document.querySelectorAll("p").length<span class="pl-pds">'</span></span> <span class="pl-s"><span class="pl-pds">'</span>2<span class="pl-pds">'</span></span>

<span class="pl-c"><span class="pl-c">#</span> Assert accessibility requirements</span>
check rodney ax-find --role navigation

rodney stop

<span class="pl-k">if</span> [ <span class="pl-s"><span class="pl-pds">"</span><span class="pl-smi">$FAIL</span><span class="pl-pds">"</span></span> <span class="pl-k">-ne</span> 0 ]<span class="pl-k">;</span> <span class="pl-k">then</span>
    <span class="pl-c1">echo</span> <span class="pl-s"><span class="pl-pds">"</span>Some checks failed<span class="pl-pds">"</span></span>
    <span class="pl-c1">exit</span> 1
<span class="pl-k">fi</span>
<span class="pl-c1">echo</span> <span class="pl-s"><span class="pl-pds">"</span>All checks passed<span class="pl-pds">"</span></span></pre></div>


    <p>Tags: <a href="https://simonwillison.net/tags/browsers">browsers</a>, <a href="https://simonwillison.net/tags/projects">projects</a>, <a href="https://simonwillison.net/tags/testing">testing</a>, <a href="https://simonwillison.net/tags/annotated-release-notes">annotated-release-notes</a>, <a href="https://simonwillison.net/tags/rodney">rodney</a></p>]]></description><pubDate>Tue, 17 Feb 2026 23:02:33 +0000</pubDate></item><item><title>Quoting ROUGH DRAFT 8/2/66</title><link>https://simonwillison.net/2026/Feb/17/rough-draft-8266/#atom-everything</link><description><![CDATA[<blockquote cite="https://www.neatorama.com/2026/02/11/The-Original-Drafts-for-Star-Treks-Opening-Narration/"><p>This is the story of the United Space Ship Enterprise. Assigned a five year patrol of our galaxy, the giant starship visits Earth colonies, regulates commerce, and explores strange new worlds and civilizations. These are its voyages... and its adventures.</p></blockquote>
<p class="cite">&mdash; <a href="https://www.neatorama.com/2026/02/11/The-Original-Drafts-for-Star-Treks-Opening-Narration/">ROUGH DRAFT 8/2/66</a>, before the Star Trek opening narration reached its final form</p>

    <p>Tags: <a href="https://simonwillison.net/tags/screen-writing">screen-writing</a>, <a href="https://simonwillison.net/tags/science-fiction">science-fiction</a></p>]]></description><pubDate>Tue, 17 Feb 2026 14:49:04 +0000</pubDate></item><item><title>First kƒÅkƒÅp≈ç chick in four years hatches on Valentine&apos;s Day</title><link>https://simonwillison.net/2026/Feb/17/first-kakapo-chick-in-four-years/#atom-everything</link><description><![CDATA[<p><strong><a href="https://www.doc.govt.nz/news/media-releases/2026-media-releases/first-kakapo-chick-in-four-years-hatches-on-valentines-day/">First kƒÅkƒÅp≈ç chick in four years hatches on Valentine&#x27;s Day</a></strong></p>
First chick of <a href="https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/#1-year-k-k-p-parrots-will-have-an-outstanding-breeding-season">the 2026 breeding season</a>!</p>
<blockquote>
<p>KƒÅkƒÅp≈ç Yasmine hatched an egg fostered from kƒÅkƒÅp≈ç Tƒ´whiri on Valentine's Day, bringing the total number of kƒÅkƒÅp≈ç to 237 ‚Äì though it won‚Äôt be officially added to the population until it fledges.</p>
</blockquote>
<p>Here's why the egg was fostered:</p>
<blockquote>
<p>"KƒÅkƒÅp≈ç mums typically have the best outcomes when raising a maximum of two chicks. Biological mum Tƒ´whiri has four fertile eggs this season already, while Yasmine, an experienced foster mum, had no fertile eggs."</p>
</blockquote>
<p>And an <a href="https://bsky.app/profile/digs.bsky.social/post/3mf25glzt2c2b">update from conservation biologist Andrew Digby</a> - a second chick hatched this morning!</p>
<blockquote>
<p>The second #kakapo chick of the #kakapo2026 breeding season hatched this morning: Hine Taumai-A1-2026 on Ako's nest on Te KƒÅkahu. We transferred the egg from Anchor two nights ago. This is Ako's first-ever chick, which is just a few hours old in this video.</p>
</blockquote>
<p>That post <a href="https://bsky.app/profile/digs.bsky.social/post/3mf25glzt2c2b">has a video</a> of mother and chick.</p>
<p><img alt="A beautiful charismatic green KƒÅkƒÅp feeding a little grey chick" src="https://static.simonwillison.net/static/2026/kakapo-plus-chick.jpg" />

    <p><small></small>Via <a href="https://www.metafilter.com/212231/Happy-Valen-Kkp-Tines">MetaFilter</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/kakapo">kakapo</a></p>]]></description><pubDate>Tue, 17 Feb 2026 14:09:43 +0000</pubDate></item><item><title>Quoting Dimitris Papailiopoulos</title><link>https://simonwillison.net/2026/Feb/17/dimitris-papailiopoulos/#atom-everything</link><description><![CDATA[<blockquote cite="https://twitter.com/dimitrispapail/status/2023080289828831349"><p>But the intellectually interesting part for me is something else. <strong>I now have something close to a magic box where I throw in a question and a first answer comes back basically for free, in terms of human effort</strong>. Before this, the way I'd explore a new idea is to either clumsily put something together myself or ask a student to run something short for signal, and if it's there, we‚Äôd go deeper. That quick signal step, i.e., finding out if a question has any meat to it, is what I can now do without taking up anyone else's time. It‚Äôs now between just me, Claude Code, and a few days of GPU time.</p>
<p>I don‚Äôt know what this means for how we do research long term. I don‚Äôt think anyone does yet. But <strong>the distance between a question and a first answer just got very small</strong>.</p></blockquote>
<p class="cite">&mdash; <a href="https://twitter.com/dimitrispapail/status/2023080289828831349">Dimitris Papailiopoulos</a>, on running research questions though Claude Code</p>

    <p>Tags: <a href="https://simonwillison.net/tags/research">research</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></description><pubDate>Tue, 17 Feb 2026 14:04:44 +0000</pubDate></item><item><title>Nano Banana Pro diff to webcomic</title><link>https://simonwillison.net/2026/Feb/17/release-notes-webcomic/#atom-everything</link><description><![CDATA[<p>Given the threat of <a href="https://simonwillison.net/tags/cognitive-debt/">cognitive debt</a> brought on by AI-accelerated software development leading to more projects and less deep understanding of how they work and what they actually do, it's interesting to consider artifacts that might be able to help.</p>
<p>Nathan Baschez <a href="https://twitter.com/nbaschez/status/2023501535343509871">on Twitter</a>:</p>
<blockquote>
<p>my current favorite trick for reducing "cognitive debt" (h/t @simonw
) is to ask the LLM to write two versions of the plan:</p>
<ol>
<li>The version for it (highly technical and detailed)</li>
<li>The version for me (an entertaining essay designed to build my intuition)</li>
</ol>
<p>Works great</p>
</blockquote>
<p>This inspired me to try something new. I generated <a href="https://github.com/simonw/showboat/compare/v0.5.0...v0.6.0.diff">the diff</a> between v0.5.0 and v0.6.0 of my Showboat project - which introduced <a href="https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#showboat-remote-publishing">the remote publishing feature</a> - and dumped that into Nano Banana Pro with the prompt:</p>
<blockquote>
<p>Create a webcomic that explains the new feature as clearly and entertainingly as possible</p>
</blockquote>
<p>Here's <a href="https://gemini.google.com/share/cce6da8e5083">what it produced</a>:</p>
<p><img alt="A six-panel comic strip illustrating a tool called &quot;Showboat&quot; for live-streaming document building. Panel 1, titled &quot;THE OLD WAY: Building docs was a lonely voyage. You finished it all before anyone saw it.&quot;, shows a sad bearded man on a wooden boat labeled &quot;THE LOCALHOST&quot; holding papers and saying &quot;Almost done... then I have to export and email the HTML...&quot;. Panel 2, titled &quot;THE UPGRADE: Just set the environment variable!&quot;, shows the same man excitedly plugging in a device with a speech bubble reading &quot;ENV VAR: SHOWBOAT_REMOTE_URL&quot; and the sound effect &quot;*KA-CHUNK!*&quot;. Panel 3, titled &quot;init establishes the uplink and generates a unique UUID beacon.&quot;, shows the man typing at a keyboard with a terminal reading &quot;$ showboat init 'Live Demo'&quot;, a satellite dish transmitting to a floating label &quot;UUID: 550e84...&quot;, and a monitor reading &quot;WAITING FOR STREAM...&quot;. Panel 4, titled &quot;Every note and exec is instantly beamed to the remote viewer!&quot;, shows the man coding with sound effects &quot;*HAMMER!*&quot;, &quot;ZAP!&quot;, &quot;ZAP!&quot;, &quot;BANG!&quot; as red laser beams shoot from a satellite dish to a remote screen displaying &quot;NOTE: Step 1...&quot; and &quot;SUCCESS&quot;. Panel 5, titled &quot;Even image files are teleported in real-time!&quot;, shows a satellite dish firing a cyan beam with the sound effect &quot;*FOOMP!*&quot; toward a monitor displaying a bar chart. Panel 6, titled &quot;You just build. The audience gets the show live.&quot;, shows the man happily working at his boat while a crowd of cheering people watches a projected screen reading &quot;SHOWBOAT LIVE STREAM: Live Demo&quot;, with a label &quot;UUID: 550e84...&quot; and one person in the foreground eating popcorn." src="https://static.simonwillison.net/static/2026/nano-banana-diff.jpg" /></p>
<p>Good enough to publish with the release notes? I don't think so. I'm sharing it here purely to demonstrate the idea. Creating assets like this as a personal tool for thinking about novel ways to explain a feature feels worth exploring further.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/nano-banana">nano-banana</a>, <a href="https://simonwillison.net/tags/gemini">gemini</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/cognitive-debt">cognitive-debt</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/text-to-image">text-to-image</a>, <a href="https://simonwillison.net/tags/showboat">showboat</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a></p>]]></description><pubDate>Tue, 17 Feb 2026 04:51:58 +0000</pubDate></item><item><title>Qwen3.5: Towards Native Multimodal Agents</title><link>https://simonwillison.net/2026/Feb/17/qwen35/#atom-everything</link><description><![CDATA[<p><strong><a href="https://qwen.ai/blog?id=qwen3.5">Qwen3.5: Towards Native Multimodal Agents</a></strong></p>
Alibaba's Qwen just released the first two models in the Qwen 3.5 series - one open weights, one proprietary. Both are multi-modal for vision input.</p>
<p>The open weight one is a Mixture of Experts model called Qwen3.5-397B-A17B. Interesting to see Qwen call out serving efficiency as a benefit of that architecture:</p>
<blockquote>
<p>Built on an innovative hybrid architecture that fuses linear attention (via Gated Delta Networks) with a sparse mixture-of-experts, the model attains remarkable inference efficiency: although it comprises 397 billion total parameters, just 17 billion are activated per forward pass, optimizing both speed and cost without sacrificing capability.</p>
</blockquote>
<p>It's <a href="https://huggingface.co/Qwen/Qwen3.5-397B-A17B">807GB on Hugging Face</a>, and Unsloth have a <a href="https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF">collection of smaller GGUFs</a> ranging in size from 94.2GB 1-bit to 462GB Q8_K_XL.</p>
<p>I got this <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle/">pelican</a> from the <a href="https://openrouter.ai/qwen/qwen3.5-397b-a17b">OpenRouter hosted model</a> (<a href="https://gist.github.com/simonw/625546cf6b371f9c0040e64492943b82">transcript</a>):</p>
<p><img alt="Pelican is quite good although the neck lacks an outline for some reason. Bicycle is very basic with an incomplete frame" src="https://static.simonwillison.net/static/2026/qwen3.5-397b-a17b.png" /></p>
<p>The proprietary hosted model is called Qwen3.5 Plus 2026-02-15, and is a little confusing. Qwen researcher <a href="https://twitter.com/JustinLin610/status/2023340126479569140">Junyang Lin  says</a>:</p>
<blockquote>
<p>Qwen3-Plus is a hosted API version of 397B. As the model natively supports 256K tokens, Qwen3.5-Plus supports 1M token context length. Additionally it supports search and code interpreter, which you can use on Qwen Chat with Auto mode.</p>
</blockquote>
<p>Here's <a href="https://gist.github.com/simonw/9507dd47483f78dc1195117735273e20">its pelican</a>, which is similar in quality to the open weights model:</p>
<p><img alt="Similar quality pelican. The bicycle is taller and has a better frame shape. They are visually quite similar." src="https://static.simonwillison.net/static/2026/qwen3.5-plus-02-15.png" />


    <p>Tags: <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/vision-llms">vision-llms</a>, <a href="https://simonwillison.net/tags/qwen">qwen</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a>, <a href="https://simonwillison.net/tags/openrouter">openrouter</a>, <a href="https://simonwillison.net/tags/ai-in-china">ai-in-china</a></p>]]></description><pubDate>Tue, 17 Feb 2026 04:30:57 +0000</pubDate></item><item><title>Two new Showboat tools: Chartroom and datasette-showboat</title><link>https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#atom-everything</link><description><![CDATA[<p>I <a href="https://simonwillison.net/2026/Feb/10/showboat-and-rodney/">introduced Showboat</a> a week ago - my CLI tool that helps coding agents create Markdown documents that demonstrate the code that they have created. I've been finding new ways to use it on a daily basis, and I've just released two new tools to help get the best out of the Showboat pattern. <a href="https://github.com/simonw/chartroom">Chartroom</a> is a CLI charting tool that works well with Showboat, and <a href="https://github.com/simonw/datasette-showboat">datasette-showboat</a> lets Showboat's new remote publishing feature incrementally push documents to a Datasette instance.</p>

<ul>
  <li><a href="https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#showboat-remote-publishing">Showboat remote publishing</a></li>
  <li><a href="https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#datasette-showboat">datasette-showboat</a></li>
  <li><a href="https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#chartroom">Chartroom</a></li>
  <li><a href="https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#how-i-built-chartroom">How I built Chartroom</a></li>
  <li><a href="https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/#the-burgeoning-showboat-ecosystem">The burgeoning Showboat ecosystem</a></li>
</ul>

<h4 id="showboat-remote-publishing">Showboat remote publishing</h4>
<p>I normally use Showboat in Claude Code for web (see <a href="https://simonwillison.net/2026/Feb/16/rodney-claude-code/">note from this morning</a>). I've used it in several different projects in the past few days, each of them with a prompt that looks something like this:</p>
<blockquote>
<p><code>Use "uvx showboat --help" to perform a very thorough investigation of what happens if you use the Python sqlite-chronicle and sqlite-history-json libraries against the same SQLite database table</code></p>
</blockquote>
<p>Here's <a href="https://github.com/simonw/research/blob/main/sqlite-chronicle-vs-history-json/demo.md">the resulting document</a>.</p>
<p>Just telling Claude Code to run <code>uvx showboat --help</code> is enough for it to learn how to use the tool - the <a href="https://github.com/simonw/showboat/blob/main/help.txt">help text</a> is designed to work as a sort of ad-hoc Skill document.</p>
<p>The one catch with this approach is that I can't <em>see</em> the new Showboat document until it's finished. I have to wait for Claude to commit the document plus embedded screenshots and push that to a branch in my GitHub repo - then I can view it through the GitHub interface.</p>
<p>For a while I've been thinking it would be neat to have a remote web server of my own which Claude instances can submit updates to while they are working. Then this morning I realized Showboat might be the ideal mechanism to set that up...</p>
<p>Showboat <a href="https://github.com/simonw/showboat/releases/tag/v0.6.0">v0.6.0</a> adds a new "remote" feature. It's almost invisible to users of the tool itself, instead being configured by an environment variable.</p>
<p>Set a variable like this:</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-k">export</span> SHOWBOAT_REMOTE_URL=https://www.example.com/submit<span class="pl-k">?</span>token=xyz</pre></div>
<p>And every time you run a <code>showboat init</code> or <code>showboat note</code> or <code>showboat exec</code> or <code>showboat image</code> command the resulting document fragments will be POSTed to that API endpoint, in addition to the Showboat Markdown file itself being updated.</p>
<p>There are <a href="https://github.com/simonw/showboat/blob/v0.6.0/README.md#remote-document-streaming">full details in the Showboat README</a> - it's a very simple API format, using regular POST form variables or a multipart form upload for the image attached to <code>showboat image</code>.</p>
<h4 id="datasette-showboat">datasette-showboat</h4>
<p>It's simple enough to build a webapp to receive these updates from Showboat, but I needed one that I could easily deploy and would work well with the rest of my personal ecosystem.</p>
<p>So I had Claude Code write me a Datasette plugin that could act as a Showboat remote endpoint. I actually had this building at the same time as the Showboat remote feature, a neat example of running <a href="https://simonwillison.net/2025/Oct/5/parallel-coding-agents/">parallel agents</a>.</p>
<p><strong><a href="https://github.com/simonw/datasette-showboat">datasette-showboat</a></strong> is a Datasette plugin that adds a <code>/-/showboat</code> endpoint to Datasette for viewing documents and a <code>/-/showboat/receive</code> endpoint for receiving updates from Showboat.</p>
<p>Here's a very quick way to try it out:</p>
<div class="highlight highlight-source-shell"><pre>uvx --with datasette-showboat --prerelease=allow \
  datasette showboat.db --create \
  -s plugins.datasette-showboat.database showboat \
  -s plugins.datasette-showboat.token secret123 \
  --root --secret cookie-secret-123</pre></div>
<p>Click on the sign in as root link that shows up in the console, then navigate to <a href="http://127.0.0.1:8001/-/showboat">http://127.0.0.1:8001/-/showboat</a> to see the interface.</p>
<p>Now set your environment variable to point to this instance:</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-k">export</span> SHOWBOAT_REMOTE_URL=<span class="pl-s"><span class="pl-pds">"</span>http://127.0.0.1:8001/-/showboat/receive?token=secret123<span class="pl-pds">"</span></span></pre></div>
<p>And run Showboat like this:</p>
<div class="highlight highlight-source-shell"><pre>uvx showboat init demo.md <span class="pl-s"><span class="pl-pds">"</span>Showboat Feature Demo<span class="pl-pds">"</span></span></pre></div>
<p>Refresh that page and you should see this:</p>
<p><img src="https://static.simonwillison.net/static/2026/datasette-showboat-documents.jpg" alt="Title: Showboat. Remote viewer for Showboat documents. Showboat Feature Demo 2026-02-17 00:06 ¬∑ 6 chunks, UUID. To send showboat output to this server, set the SHOWBOAT_REMOTE_URL environment variable: export SHOWBOAT_REMOTE_URL=&quot;http://127.0.0.1:8001/-/showboat/receive?token=your-token&quot;" style="max-width: 100%;" /></p>
<p>Click through to the document, then start Claude Code or Codex or your agent of choice and prompt:</p>
<blockquote>
<p><code>Run 'uvx showboat --help' and then use showboat to add to the existing demo.md document with notes and exec and image to demonstrate the tool - fetch a placekitten for the image demo.</code></p>
</blockquote>
<p>The <code>init</code> command assigns a UUID and title and sends those up to Datasette.</p>
<p><img src="https://static.simonwillison.net/static/2026/datasette-showboat.gif" alt="Animated demo - in the foreground a terminal window runs Claude Code, which executes various Showboat commands. In the background a Firefox window where the Showboat Feature Demo adds notes then some bash commands, then a placekitten image." style="max-width: 100%;" /></p>
<p>The best part of this is that it works in Claude Code for web. Run the plugin on a server somewhere (an exercise left up to the reader - I use <a href="https://fly.io/">Fly.io</a> to host mine) and set that <code>SHOWBOAT_REMOTE_URL</code> environment variable in your Claude environment, then any time you tell it to use Showboat the document it creates will be transmitted to your server and viewable in real time.</p>
<p>I built <a href="https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#rodney-cli-browser-automation-designed-to-work-with-showboat">Rodney</a>, a CLI browser automation tool, specifically to work with Showboat. It makes it easy to have a Showboat document load up web pages, interact with them via clicks or injected JavaScript and captures screenshots to embed in the Showboat document and show the effects.</p>
<p>This is wildly useful for hacking on web interfaces using Claude Code for web, especially when coupled with the new remote publishing feature. I only got this stuff working this morning and I've already had several sessions where Claude Code has published screenshots of its work in progress, which I've then been able to provide feedback on directly in the Claude session while it's still working.</p>
<h3 id="chartroom">Chartroom</h3>
<p>A few days ago I had another idea for a way to extend the Showboat ecosystem: what if Showboat documents could easily include charts?</p>
<p>I sometimes fire up Claude Code for data analysis tasks, often telling it to download a SQLite database and then run queries against it to figure out interesting things from the data.</p>
<p>With a simple CLI tool that produced PNG images I could have Claude use Showboat to build a document with embedded charts to help illustrate its findings.</p>
<p><strong><a href="https://github.com/simonw/chartroom">Chartroom</a></strong> is exactly that. It's effectively a thin wrapper around the excellent <a href="https://matplotlib.org/">matplotlib</a> Python library, designed to be used by coding agents to create charts that can be embedded in Showboat documents.</p>
<p>Here's how to render a simple bar chart:</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-c1">echo</span> <span class="pl-s"><span class="pl-pds">'</span>name,value</span>
<span class="pl-s">Alice,42</span>
<span class="pl-s">Bob,28</span>
<span class="pl-s">Charlie,35</span>
<span class="pl-s">Diana,51</span>
<span class="pl-s">Eve,19<span class="pl-pds">'</span></span> <span class="pl-k">|</span> uvx chartroom bar --csv \
  --title <span class="pl-s"><span class="pl-pds">'</span>Sales by Person<span class="pl-pds">'</span></span> --ylabel <span class="pl-s"><span class="pl-pds">'</span>Sales<span class="pl-pds">'</span></span></pre></div>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/simonw/chartroom/8812afc02e1310e9eddbb56508b06005ff2c0ed5/demo/1f6851ec-2026-02-14.png"><img src="https://raw.githubusercontent.com/simonw/chartroom/8812afc02e1310e9eddbb56508b06005ff2c0ed5/demo/1f6851ec-2026-02-14.png" alt="A chart of those numbers, with a title and y-axis label" style="max-width: 100%;" /></a></p>
<p>It can also do line charts, bar charts, scatter charts, and histograms - as seen in <a href="https://github.com/simonw/chartroom/blob/0.2.1/demo/README.md">this demo document</a> that was built using Showboat.</p>
<p>Chartroom can also generate alt text. If you add <code>-f alt</code> to the above it will output the alt text for the chart instead of the image:</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-c1">echo</span> <span class="pl-s"><span class="pl-pds">'</span>name,value</span>
<span class="pl-s">Alice,42</span>
<span class="pl-s">Bob,28</span>
<span class="pl-s">Charlie,35</span>
<span class="pl-s">Diana,51</span>
<span class="pl-s">Eve,19<span class="pl-pds">'</span></span> <span class="pl-k">|</span> uvx chartroom bar --csv \
  --title <span class="pl-s"><span class="pl-pds">'</span>Sales by Person<span class="pl-pds">'</span></span> --ylabel <span class="pl-s"><span class="pl-pds">'</span>Sales<span class="pl-pds">'</span></span> -f alt</pre></div>
<p>Outputs:</p>
<pre><code>Sales by Person. Bar chart of value by name ‚Äî Alice: 42, Bob: 28, Charlie: 35, Diana: 51, Eve: 19
</code></pre>
<p>Or you can use <code>-f html</code> or <code>-f markdown</code> to get the image tag with alt text directly:</p>
<div class="highlight highlight-text-md"><pre><span class="pl-s">![</span>Sales by Person. Bar chart of value by name ‚Äî Alice: 42, Bob: 28, Charlie: 35, Diana: 51, Eve: 19<span class="pl-s">]</span><span class="pl-s">(</span><span class="pl-corl">/Users/simon/chart-7.png</span><span class="pl-s">)</span></pre></div>
<p>I added support for Markdown images with alt text to Showboat in <a href="https://github.com/simonw/showboat/releases/tag/v0.5.0">v0.5.0</a>, to complement this feature of Chartroom.</p>
<p>Finally, Chartroom has support for different <a href="https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html">matplotlib styles</a>. I had Claude build a Showboat document to demonstrate these all in one place - you can see that at <a href="https://github.com/simonw/chartroom/blob/main/demo/styles.md">demo/styles.md</a>.</p>
<h4 id="how-i-built-chartroom">How I built Chartroom</h4>
<p>I started the Chartroom repository with my <a href="https://github.com/simonw/click-app">click-app</a> cookiecutter template, then told a fresh Claude Code for web session:</p>
<blockquote>
<p>We are building a Python CLI tool which uses matplotlib to generate a PNG image containing a chart. It will have multiple sub commands for different chart types, controlled by command line options. Everything you need to know to use it will be available in the single "chartroom --help" output.</p>
<p>It will accept data from files or standard input as CSV or TSV or JSON, similar to how sqlite-utils accepts data - clone simonw/sqlite-utils to /tmp for reference there. Clone matplotlib/matplotlib for reference as well</p>
<p>It will also accept data from --sql path/to/sqlite.db "select ..." which runs in read-only mode</p>
<p>Start by asking clarifying questions - do not use the ask user tool though it is broken - and generate a spec for me to approve</p>
<p>Once approved proceed using red/green TDD running tests with "uv run pytest"</p>
<p>Also while building maintain a demo/README.md document using the "uvx showboat --help" tool - each time you get a new chart type working commit the tests, implementation, root level
README update and a new version of that demo/README.md document with an inline image demo of the new chart type (which should be a UUID image filename managed by the showboat image command and should be stored in the demo/ folder</p>
<p>Make sure "uv build" runs cleanly without complaining about extra directories but also ensure dist/ and uv.lock are in gitignore</p>
</blockquote>
<p>This got most of the work done. You can see the rest <a href="https://github.com/simonw/chartroom/pulls?q=is%3Apr+is%3Aclosed">in the PRs</a> that followed.</p>
<h4 id="the-burgeoning-showboat-ecosystem">The burgeoning Showboat ecosystem</h4>
<p>The Showboat family of tools now consists of <a href="https://github.com/simonw/showboat">Showboat</a> itself, <a href="https://github.com/simonw/rodney">Rodney</a> for browser automation, <a href="https://github.com/simonw/chartroom">Chartroom</a> for charting and <a href="https://github.com/simonw/datasette-showboat">datasette-showboat</a> for streaming remote Showboat documents to Datasette.</p>
<p>I'm enjoying how these tools can operate together based on a very loose set of conventions. If a tool can output a path to an image Showboat can include that image in a document. Any tool that can output text can be used with Showboat.</p>
<p>I'll almost certainly be building more tools that fit this pattern. They're very quick to knock out!</p>
<p>The environment variable mechanism for Showboat's remote streaming is a fun hack too - so far I'm just using it to stream documents somewhere else, but it's effectively a webhook extension mechanism that could likely be used for all sorts of things I haven't thought of yet.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/charting">charting</a>, <a href="https://simonwillison.net/tags/projects">projects</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/datasette">datasette</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/showboat">showboat</a></p>]]></description><pubDate>Tue, 17 Feb 2026 00:43:45 +0000</pubDate></item><item><title>Rodney and Claude Code for Desktop</title><link>https://simonwillison.net/2026/Feb/16/rodney-claude-code/#atom-everything</link><description><![CDATA[<p>I'm a very heavy user of <a href="https://code.claude.com/docs/en/claude-code-on-the-web">Claude Code on the web</a>, Anthropic's excellent but poorly named cloud version of Claude Code where everything runs in a container environment managed by them, greatly reducing the risk of anything bad happening to a computer I care about.</p>
<p>I don't use the web interface at all (hence my dislike of the name) - I access it exclusively through their native iPhone and Mac desktop apps.</p>
<p>Something I particularly appreciate about the desktop app is that it lets you see images that Claude is "viewing" via its <code>Read /path/to/image</code> tool. Here's what that looks like:</p>
<p><img alt="Screenshot of a Claude Code session in Claude Desktop. Claude says: The debug page looks good - all items listed with titles and descriptions. Now let me check the nav
menu -  Analyzed menu image file - Bash uvx rodney open &quot;http://localhost:8765/&quot; 2&gt;&amp;1 &amp;&amp; uvx rodney click &quot;details.nav-menu summary&quot; 2&gt;&amp;1 &amp;% sleep 0.5 &amp;&amp; uvx rodney screenshot /tmp/menu.png 2&gt;&amp;1 Output reads: Datasette: test, Clicked, /tmp/menu.png - then it says Read /tmp/menu.png and reveals a screenshot of the Datasette interface with the nav menu open, showing only &quot;Debug&quot; and &quot;Log out&quot; options. Claude continues: The menu now has just &quot;Debug&quot; and ‚ÄúLog out&quot; ‚Äî much cleaner. Both pages look good. Let me clean up the server and run the remaining tests." src="https://static.simonwillison.net/static/2026/rodney-claude-desktop.jpg" /></p>
<p>This means you can get a visual preview of what it's working on while it's working, without waiting for it to push code to GitHub for you to try out yourself later on.</p>
<p>The prompt I used to trigger the above screenshot was:</p>
<blockquote>
<p><code>Run "uvx rodney --help" and then use Rodney to manually test the new pages and menu - look at screenshots from it and check you think they look OK</code></p>
</blockquote>
<p>I designed <a href="https://simonwillison.net/2026/Feb/10/showboat-and-rodney/#rodney-cli-browser-automation-designed-to-work-with-showboat">Rodney</a> to have <a href="https://github.com/simonw/rodney/blob/main/help.txt">--help output</a> that provides everything a coding agent needs to know in order to use the tool.</p>
<p>The Claude iPhone app doesn't display opened images yet, so I <a href="https://twitter.com/simonw/status/2023432616066879606">requested it as a feature</a> just now in a thread on Twitter.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/claude-code">claude-code</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/async-coding-agents">async-coding-agents</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/projects">projects</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/rodney">rodney</a></p>]]></description><pubDate>Mon, 16 Feb 2026 16:38:57 +0000</pubDate></item><item><title>The AI Vampire</title><link>https://simonwillison.net/2026/Feb/15/the-ai-vampire/#atom-everything</link><description><![CDATA[<p><strong><a href="https://steve-yegge.medium.com/the-ai-vampire-eda6e4f07163">The AI Vampire</a></strong></p>
Steve Yegge's take on agent fatigue, and its relationship to burnout.</p>
<blockquote>
<p>Let's pretend you're the only person at your company using AI.</p>
<p>In Scenario A, you decide you're going to impress your employer, and work for 8 hours a day at 10x productivity. You knock it out of the park and make everyone else look terrible by comparison.</p>
<p>In that scenario, your employer captures 100% of the value from <em>you</em> adopting AI. You get nothing, or at any rate, it ain't gonna be 9x your salary. And everyone hates you now.</p>
<p>And you're <em>exhausted.</em> You're tired, Boss. You got nothing for it.</p>
<p>Congrats, you were just drained by a company. I've been drained to the point of burnout several times in my career, even at Google once or twice. But now with AI, it's oh, so much easier.</p>
</blockquote>
<p>Steve reports needing more sleep due to the cognitive burden involved in agentic engineering, and notes that four hours of agent work a day is a more realistic pace:</p>
<blockquote>
<p>I‚Äôve argued that AI has turned us all into Jeff Bezos, by automating the easy work, and leaving us with all the difficult decisions, summaries, and problem-solving. I find that I am only really comfortable working at that pace for short bursts of a few hours once or occasionally twice a day, even with lots of practice.</p>
</blockquote>

    <p><small></small>Via <a href="https://cosocial.ca/@timbray/116076167774984883">Tim Bray</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/steve-yegge">steve-yegge</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a>, <a href="https://simonwillison.net/tags/cognitive-debt">cognitive-debt</a></p>]]></description><pubDate>Sun, 15 Feb 2026 23:59:36 +0000</pubDate></item><item><title>Em dash</title><link>https://simonwillison.net/2026/Feb/15/em-dashes/#atom-everything</link><description><![CDATA[<p>I'm occasionally accused of using LLMs to write the content on my blog. I don't do that, and I don't think my writing has much of an LLM smell to it... with one notable exception:</p>
<pre>    <span class="pl-c"># Finally, do em dashes</span>
    <span class="pl-s1">s</span> <span class="pl-c1">=</span> <span class="pl-s1">s</span>.<span class="pl-c1">replace</span>(<span class="pl-s">' - '</span>, <span class="pl-s">u'<span class="pl-cce">\u2014</span>'</span>)</pre>

<p>That code to add em dashes to my posts dates back to <a href="https://github.com/simonw/simonwillisonblog/blob/e6d0327b37debdf820b5cfef4fb7d09a9624cea9/blog/templatetags/entry_tags.py#L145-L146">at least 2015</a> when I ported my blog from an older version of Django (in a long-lost Mercurial repository) and started afresh on GitHub.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/typography">typography</a>, <a href="https://simonwillison.net/tags/blogging">blogging</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/python">python</a></p>]]></description><pubDate>Sun, 15 Feb 2026 21:40:46 +0000</pubDate></item><item><title>Deep Blue</title><link>https://simonwillison.net/2026/Feb/15/deep-blue/#atom-everything</link><description><![CDATA[<p>We coined a new term on the <a href="https://simonwillison.net/2026/Jan/8/llm-predictions-for-2026/">Oxide and Friends podcast</a> last month (primary credit to Adam Leventhal) covering the sense of psychological ennui leading into existential dread that many software developers are feeling thanks to the encroachment of generative AI into their field of work.</p>
<p>We're calling it <strong>Deep Blue</strong>.</p>
<p>You can listen to it being coined in real time <a href="https://www.youtube.com/watch?v=lVDhQMiAbR8&amp;t=2835s">from 47:15 in the episode</a>. I've included <a href="https://simonwillison.net/2026/Feb/15/deep-blue/#transcript">a transcript below</a>.</p>
<p>Deep Blue is a very real issue.</p>
<p>Becoming a professional software engineer is <em>hard</em>. Getting good enough for people to pay you money to write software takes years of dedicated work. The rewards are significant: this is a well compensated career which opens up a lot of great opportunities.</p>
<p>It's also a career that's mostly free from gatekeepers and expensive prerequisites. You don't need an expensive degree or accreditation. A laptop, an internet connection and a lot of time and curiosity is enough to get you started.</p>
<p>And it rewards the nerds! Spending your teenage years tinkering with computers turned out to be a very smart investment in your future.</p>
<p>The idea that this could all be stripped away by a chatbot is <em>deeply</em> upsetting.</p>
<p>I've seen signs of Deep Blue in most of the online communities I spend time in. I've even faced accusations from my peers that I am actively harming their future careers through my work helping people understand how well AI-assisted programming can work.</p>
<p>I think this is an issue which is causing genuine mental anguish for a lot of people in our community. Giving it a name makes it easier for us to have conversations about it.</p>
<h4 id="my-experiences-of-deep-blue">My experiences of Deep Blue</h4>
<p>I distinctly remember my first experience of Deep Blue. For me it was triggered by ChatGPT Code Interpreter back in early 2023.</p>
<p>My primary project is <a href="https://datasette.io/">Datasette</a>, an ecosystem of open source tools for telling stories with data. I had dedicated myself to the challenge of helping people (initially focusing on journalists) clean up, analyze and find meaning in data, in all sorts of shapes and sizes.</p>
<p>I expected I would need to build a lot of software for this! It felt like a challenge that could keep me happily engaged for many years to come.</p>
<p>Then I tried uploading a CSV file of <a href="https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-2018-to-Present/wg3w-h783/about_data">San Francisco Police Department Incident Reports</a> - hundreds of thousands of rows - to ChatGPT Code Interpreter and... it did every piece of data cleanup and analysis I had on my napkin roadmap for the next few years with a couple of prompts.</p>
<p>It even converted the data into a neatly normalized SQLite database and let me download the result!</p>
<p>I remember having two competing thoughts in parallel.</p>
<p>On the one hand, as somebody who wants journalists to be able to do more with data, this felt like a <em>huge</em> breakthrough. Imagine giving every journalist in the world an on-demand analyst who could help them tackle any data question they could think of!</p>
<p>But on the other hand... <em>what was I even for</em>? My confidence in the value of my own projects took a painful hit. Was the path I'd chosen for myself suddenly a dead end?</p>
<p>I've had some further pangs of Deep Blue just in the past few weeks, thanks to the Claude Opus 4.5/4.6 and GPT-5.2/5.3 coding agent effect. As many other people are also observing, the latest generation of coding agents, given the right prompts, really can churn away for a few minutes to several hours and produce working, documented and fully tested software that exactly matches the criteria they were given.</p>
<p>"The code they write isn't any good" doesn't really cut it any more.</p>
<h4 id="transcript">A lightly edited transcript</h4>
<blockquote>
<p><strong>Bryan</strong>: I think that we're going to see a real problem with AI induced ennui where software engineers in particular get listless because the AI can do anything. Simon, what do you think about that?</p>
<p><strong>Simon</strong>: Definitely. Anyone who's paying close attention to coding agents is feeling some of that already. There's an extent where you sort of get over it when you realize that you're still useful, even though your ability to memorize the syntax of program languages is completely irrelevant now.</p>
<p>Something I see a lot of is people out there who are having existential crises and are very, very unhappy because they're like, "I dedicated my career to learning this thing and now it just does it. What am I even for?". I will very happily try and convince those people that they are for a whole bunch of things and that none of that experience they've accumulated has gone to waste, but psychologically it's a difficult time for software engineers.</p>
<p>[...]</p>
<p><strong>Bryan</strong>: Okay, so I'm going to predict that we name that. Whatever that is, we have a name for that kind of feeling and that kind of, whether you want to call it a blueness or a loss of purpose, and that we're kind of trying to address it collectively in a directed way.</p>
<p><strong>Adam</strong>: Okay, this is your big moment. Pick the name. If you call your shot from here, this is you pointing to the stands. You know, I ‚Äì Like deep blue, you know.</p>
<p><strong>Bryan</strong>: Yeah, deep blue. I like that. I like deep blue. Deep blue. Oh, did you walk me into that, you bastard? You just blew out the candles on my birthday cake.</p>
<p>It wasn't my big moment at all. That was your big moment. No, that is, Adam, that is very good. That is deep blue.</p>
<p><strong>Simon</strong>: All of the chess players and the Go players went through this a decade ago and they have come out stronger.</p>
</blockquote>
<p>Turns out it was more than a decade ago: <a href="https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov">Deep Blue defeated Garry Kasparov in 1997</a>.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/definitions">definitions</a>, <a href="https://simonwillison.net/tags/careers">careers</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/oxide">oxide</a>, <a href="https://simonwillison.net/tags/bryan-cantrill">bryan-cantrill</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/coding-agents">coding-agents</a></p>]]></description><pubDate>Sun, 15 Feb 2026 21:06:44 +0000</pubDate></item><item><title>Gwtar: a static efficient single-file HTML format</title><link>https://simonwillison.net/2026/Feb/15/gwtar/#atom-everything</link><description><![CDATA[<p><strong><a href="https://gwern.net/gwtar">Gwtar: a static efficient single-file HTML format</a></strong></p>
Fascinating new project from Gwern Branwen and Said Achmiz that targets the challenge of combining large numbers of assets into a single archived HTML file without that file being inconvenient to view in a browser.</p>
<p>The key trick it uses is to fire <a href="https://developer.mozilla.org/en-US/docs/Web/API/Window/stop">window.stop()</a> early in the page to prevent the browser from downloading the whole thing, then following that call with inline tar uncompressed content.</p>
<p>It can then make HTTP range requests to fetch content from that tar data on-demand when it is needed by the page.</p>
<p>The JavaScript that has already loaded rewrites asset URLs to point to <code>https://localhost/</code> purely so that they will fail to load. Then it uses a <a href="https://developer.mozilla.org/en-US/docs/Web/API/PerformanceObserver">PerformanceObserver</a> to catch those attempted loads:</p>
<pre><code>let perfObserver = new PerformanceObserver((entryList, observer) =&gt; {
    resourceURLStringsHandler(entryList.getEntries().map(entry =&gt; entry.name));
});
perfObserver.observe({ entryTypes: [ "resource" ] });
</code></pre>
<p>That <code>resourceURLStringsHandler</code> callback finds the resource if it is already loaded or fetches it with an HTTP range request otherwise and then inserts the resource in the right place using a <code>blob:</code> URL.</p>
<p>Here's what the <code>window.stop()</code> portion of the document looks like if you view the source:</p>
<p><img alt="Screenshot of a macOS terminal window titled &quot;gw ‚Äî more big.html ‚Äî 123√ó46&quot; showing the source code of a gwtar (self-extracting HTML archive) file. The visible code includes JavaScript with requestIdleCallback(getMainPageHTML);, a noscript block with warnings: a &quot;js-disabled-warning&quot; stating &quot;This HTML page requires JavaScript to be enabled to render, as it is a self-extracting gwtar HTML file,&quot; a description of gwtar as &quot;a portable self-contained standalone HTML file which is designed to nevertheless support efficient lazy loading of all assets such as large media files,&quot; with a link to https://gwern.net/gwtar, a &quot;local-file-warning&quot; with a shell command perl -ne'print $_ if $x; $x=1 if /&lt;!-- GWTAR END/' &amp;lt; foo.gwtar.html | tar --extract, and a &quot;server-fail-warning&quot; about misconfigured servers. Below the HTML closing tags and &lt;!-- GWTAR END comment is binary tar archive data with the filename 2010-02-brianmoriarty-thesecretofpsalm46.html, showing null-padded tar header fields including ustar^@00root and octal size/permission values. At the bottom, a SingleFile metadata comment shows url: https://web.archive.org/web/20230512001411/http://ludix.com/moriarty/psalm46.html and saved date: Sat Jan 17 2026 19:26:49 GMT-0800 (Pacific Standard Time)." src="https://static.simonwillison.net/static/2026/gwtar.jpg" /></p>
<p>Amusingly for an archive format it doesn't actually work if you open the file directly on your own computer. Here's what you see if you try to do that:</p>
<blockquote>
<p>You are seeing this message, instead of the page you should be seeing, because <code>gwtar</code> files <strong>cannot be opened locally</strong> (due to web browser security restrictions).</p>
<p>To open this page on your computer, use the following shell command:</p>
<p><code>perl -ne'print $_ if $x; $x=1 if /&lt;!-- GWTAR END/' &lt; foo.gwtar.html | tar --extract</code></p>
<p>Then open the file <code>foo.html</code> in any web browser.</p>
</blockquote>

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=47024506">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/archiving">archiving</a>, <a href="https://simonwillison.net/tags/html">html</a>, <a href="https://simonwillison.net/tags/javascript">javascript</a></p>]]></description><pubDate>Sun, 15 Feb 2026 18:26:08 +0000</pubDate></item><item><title>Three months of OpenClaw</title><link>https://simonwillison.net/2026/Feb/15/openclaw/#atom-everything</link><description><![CDATA[<p>It's wild that the first commit to OpenClaw was <a href="https://github.com/openclaw/openclaw/commit/f6dd362d39b8e30bd79ef7560aab9575712ccc11">on November 25th 2025</a>, and less than three months later it's hit 10,000 commits from 600 contributors, attracted 196,000 GitHub stars and sort-of been featured in an extremely vague <a href="https://www.youtube.com/watch?v=n7I-D4YXbzg">Super Bowl commercial for AI.com</a>.</p>
<p>Quoting AI.com founder <a href="https://twitter.com/kris/status/2020663711015514399">Kris Marszalek</a>, purchaser of the <a href="https://www.theregister.com/2026/02/09/70m_aicom_domain_sale/">most expensive domain in history</a> for $70m:</p>
<blockquote>
<p>ai.com is the world‚Äôs first easy-to-use and secure implementation of OpenClaw, the open source agent framework that went viral two weeks ago; we made it easy to use without any technical skills, while hardening security to keep your data safe.</p>
</blockquote>
<p>Looks like vaporware to me - all you can do right now is reserve a handle - but it's still remarkable to see an open source project get to <em>that</em> level of hype in such a short space of time.</p>
<p><strong>Update</strong>: OpenClaw creator Peter Steinberger <a href="https://steipete.me/posts/2026/openclaw">just announced</a> that he's joining OpenAI and plans to transfer ownership of OpenClaw to a new independent foundation.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/ai-agents">ai-agents</a>, <a href="https://simonwillison.net/tags/openclaw">openclaw</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/open-source">open-source</a>, <a href="https://simonwillison.net/tags/domains">domains</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/peter-steinberger">peter-steinberger</a></p>]]></description><pubDate>Sun, 15 Feb 2026 17:23:28 +0000</pubDate></item><item><title>Quoting Eric Meyer</title><link>https://simonwillison.net/2026/Feb/15/eric-meyer/#atom-everything</link><description><![CDATA[<blockquote cite="https://mastodon.social/@Meyerweb/116065151451468199"><p>I saw yet another ‚ÄúCSS is a massively bloated mess‚Äù whine and I‚Äôm like.  My dude.  My brother in Chromium.  It is trying as hard as it can to express the totality of visual presentation and layout design and typography and animation and digital interactivity and a few other things in a human-readable text format.  It‚Äôs not bloated, it‚Äôs fantastically ambitious.  Its reach is greater than most of us can hope to grasp.  Put some <em>respect</em> on its <em>name</em>.</p></blockquote>
<p class="cite">&mdash; <a href="https://mastodon.social/@Meyerweb/116065151451468199">Eric Meyer</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/css">css</a>, <a href="https://simonwillison.net/tags/web-standards">web-standards</a>, <a href="https://simonwillison.net/tags/eric-meyer">eric-meyer</a></p>]]></description><pubDate>Sun, 15 Feb 2026 13:36:20 +0000</pubDate></item></channel></rss>